---
title: "Conditional Probability"
description: |
  A short description of the post.
author:
  - name: vgherard
    url: https://vgherard.github.io
date: 2023-11-03
output:
  distill::distill_article:
    self_contained: false
bibliography: biblio.bib
draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```




Let $(\Omega, \,\mathcal E,\,P)$ be a probability space, and let 
$X\colon \Omega \to \Omega _X$ be a random variable with target space 
$(\Omega _X, \mathcal X)$. We denote the corresponding push-forward measure on 
$\mathcal X$ by $X_*P$, so that:

$$
(X_*P)(B)= P(X^{-1}(B))
$$
for all $B\in \mathcal X$. A measurable function 
$f\colon \Omega _X \to \mathbb R$ is integrable with respect to $X_*P$ 
if and only if $f\circ X$ is integrable with respect to $P$, in which case^[
These claims can be proved by a standard argument using approximations by simple 
functions.]: 
$$
\intop _\mathcal X f\,\text d(X_*P) = \intop _\mathcal \Omega (f \circ X)\,\text dP.
$$
Now, given an arbitrary event $E\in \mathcal E$ define 
$(X_*P)_E(B)=P(E\cap X^{-1}(B))$. Then $(X_*P)_E$ is a measure on 
$\mathcal X$ which is clearly dominated by $X_*P$, and there exists a 
Radon-Nikodym derivative 
$\frac{\text d (X_*P)_E}{\text d (X_*P)} \in L_1(X_*P)$. We define the 
conditional probability of event $E$ with respect to the random variable $X$ as
the random variable:

$$
P(E\vert X)\equiv \frac{\text d (X_*P)_E}{\text d (X_*P)}.
$$

The intuition behind this definition comes from the tautology (given the 
definition in terms of Radon-Nikodyn derivative):

$$
P(E \cap (X\in B)) = \intop _{B} P(E\vert X)\,\text d(X_*P).
$$ 
On one hand, from elementary probability theory, one would expect any 
sensible definition of conditional probability to satisfy this theorem. 
On the other hand, the theorem univocally identifies $P(E\vert X)$ as the 
Radon-Nikodym derivative $\frac{\text d (X_*P)_E}{\text d (X_*P)}$, modulo a set 
of $X_*P$ measure zero.

It is fairly easy to verify the following properties of conditional probability:

* *Countable additivity*. For any finite or countable family $(E_i)_{i\in I}$ of
disjoint events, $E_i \cap E_j = \emptyset$, we have:
$$
P(\cup _{i\in I} E_i \vert X = x) = \sum _{i \in I}P(E_i \vert X = x) 
$$
for almost all $x\in \Omega_X$.

* *Positivity*. For any event $E$ we have $P(E \vert X=x) \geq 0$ for almost all
$x \in \Omega$.

* *Normalization*. $P(\Omega \vert X = x) = 1$ for almost all $x \in \Omega$. 

This, however, does not generally imply that $P(\cdot \vert X = x)$ is a 
probability measure for almost all $x\in \Omega_X$^[For instance, denoting by 
$N_E = \{x \in \Omega_X \vert P(E\vert X = x) \geq 0\}$, positivity implies that 
$(X_*P)(N_E)=0$. However, there's no guarantee that 
$\cup _{E\in \mathcal E} N_E$ is also a measure zero set (and in fact it does 
not need to be measurable, since the union is generally uncountable).]. 
Functions $\nu \colon \mathcal E \times \Omega _X \to \mathbb R^+$ such that 
$\nu(\cdot, x)$ is a measure for all $x\in \Omega _X$, and 
$\nu (E,\cdot)$ is $\mathcal X$-measurable for all $E\in \mathcal E$ are called
*random measures*. If $\nu$ satisfies 
$$
P(E \cap (X\in B)) = \intop _{B} \nu (E,\cdot)\,\text d(X_*P)
$$
(or, equivalently, if $\nu (E,\cdot)$ is a version of 
$\frac{\text d (X_*P)_E}{\text d (X_*P)}$) for all $E\in \mathcal E$, $\nu$ is 
called a *regular conditional probability* for the random variable $X$. If the 
space $(\Omega,\, \mathcal E)$ is regular enough (*e.g.* if it is a Borel space)
one can prove that a regular conditional probability exists for any random 
variable $X$, see *e.g.* [@kallenberg1997foundations].

In order to gain some intuition into this definition, we consider some simple 
examples. If $X = \chi _A \colon \Omega \to \{0,1\}$, where $A\in \mathcal E$ 
has positive probability $0<P(A)<1$, we immediately find:

$$
P(E\vert \chi _A)(t) = t\cdot \frac{P(E\cap A)}{P(A)} + (1-t)\cdot \frac{P(E\cap A^c)}{P(A^c)}
$$
In particular, $P(E\vert A) \equiv P(E\vert \chi _A)(1)$ agrees with the 
elementary definition of conditional probability.

If $X = \text {id} _\Omega$, where the target space is equipped with a
sub-$\sigma$-algebra $\mathcal F \subseteq \mathcal E$, we have:

$$
P(E\vert \mathcal F)\equiv \frac{\text d (P\vert _\mathcal F)_E}{\text d (P\vert _\mathcal F)},
$$
which agrees with the definition of conditional probability with respect to a
sub-$\sigma$-algebra. In particular, if $\mathcal F$ is the $\sigma$-algebra 
generated by a finite or countable partition $\mathcal A = (A_i)_{i\in I}$ of 
$\mathcal \Omega$ such that $P(A_i)>0$ for all $i\in I$, we have:

$$
P(E\vert \mathcal A)=\sum _{i\in I} \frac{P(E\cap A_i)}{P(A_i)}\chi _{A_i},
$$
again in agreement with elementary definitions.

Finally, if $X\colon \Omega \to \mathbb R$ is a regular random variable, where
$\mathbb R$ is equipped with the Borel $\sigma$-algebra, $X_*P$ 
coincides with the Stieltjes measure generated by the cumulative distribution
function $P_X$ of $X$. Denoting $P(E\vert X)(x) \equiv P(E\vert X = x)$, we may
write:

$$
P(E \cap (X \in B))=\intop _B P(E\vert X=x) \,\text dP_X(x).
$$
and, in particular:

$$
P(E)=\intop _\mathbb R P(E\vert X=x) \,\text dP_X(x).
$$
