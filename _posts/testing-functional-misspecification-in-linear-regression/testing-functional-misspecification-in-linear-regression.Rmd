---
title: "Testing functional specification in linear regression"
description: |
  Some options in R, using the `{lmtest}` package.
author:
  - name: vgherard
    url: https://vgherard.github.io
date: 2023-05-25
output:
  distill::distill_article:
    self_contained: false
categories: 
  - Statistics
  - Model Misspecification
  - Regression
  - Linear Models
  - R
draft: true
---

Another one from the series on "misspecified regression models" (started with [Model Misspecification and Linear Sandwiches](https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches/)). 

## Intro

Lately I've been messing around with the  [`{lmtest}`](https://cran.r-project.org/web/packages/lmtest/index.html) R package, a nice collection of hypothesis tests for classical linear model assumptions: *linearity* (of course) and *heteroskedasticity* ($X$-independence of the conditional variance). 

Just to clarify, here the relevant "linearity" assumption is that the conditional mean $\mathbb E (Y\vert X)$ is given by a linear combination of *known functions* $f_i$ of $X$:

$$
\mathbb E (Y\vert X) = \sum _{i = 1}^p \alpha_if_i(X),
$$
Testing "linearity" (or, as goes the title, "functional specification") refers to testing that the chosen set of functions $\{f_{i}\}_{i=1,\dots,p}$ provide a valid description of the data generating process.

## First attempt: residual autocorrelation

My initial intuition was that it should be possible to test functional specification through the following procedure:

- Perform linear regression with the specified functional form.
- Order the residuals according to the corresponding values of $X$^[Here I'm implicitly assuming that we have a single $X$, but a similar logic should also apply to multivariate regression.].
- Test for serial correlation (e.g. performing a Durbin-Watson test, `lmtest::dwtest`) on the series of ordered residuals.

The idea is quite simple: if residuals exhibit some systematic pattern when
plotted against $X$, then for close values of $X$, residuals should also tend to be close, leading to a positive correlation. For example:

```{r}
set.seed(840)
x <- rnorm(1e2)
y <- x^3 + rnorm(length(x))
plot(x, y)
abline(lm(y ~ x))
```
This, I suspect, is the reason why functions such as `lmtest::dwtest()` have an
`order.by` argument which precisely allows to sort residuals before performing the test.

Unfortunately, it turns out that such a method is not only sensitive to functional misspecification, but also to heteroskedasticity - as one can quickly verify by running a simulation using `lmtest::dwtest()`.

The overall idea is interesting, and works for homoskedastic noise, but the limitation to constant variance may be a bit too stringent. For this reason I turned to a second method, which also allows to take into account the 
possibility of heteroskedastic noise.

## Second attempt: RESET + Heteroskedastic Consistent variance estimates

The idea of RESET tests (see `?lmtest::resettest()`) is also quite simple: 
if the linear model is correct, there should be relatively little gain in adding additional non-linear functions of the original covariates to the fit's formula. 

The improvements from these model adjustments can be tested through a standard 
$Z$-test (or $F$-test, for multiple adjustments at once), with an important 
catch: the covariance matrix of regression coefficients used in these tests can 
be chosen to be robust to heteroskedasticity (see [Model Misspecification and Linear Sandwiches](https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches/)).

The code that follows illustrates this procedure with an example dataset. The following section contains a more in-depth simulation study of the property of this test.

```{r fit_cars}
fit_cars <- lm(dist ~ speed, data = cars)
with(data = cars, plot(speed, dist))
abline(fit_cars)
```
```{r reset_test_cars}
lmtest::resettest(fit_cars, 
									type = "regressor", 
									power = 2,
									vcov = sandwich::vcovHC
									)
```
Unfortunately, the output of `lmtest::resettest` does not include the results of the extended fit, which can be useful to understand the *impact* of the omitted covariates on the overall model picture (independently of the RESET $p$-value under the null hypothesis). ^[With enough data, the RESET test would likely test positive for a variety of misspecifications, but that doesn't mean that such misspecification are necessarily relevant from a modeling perspective. Here, for instance, a large coefficient for $\text{(speed)}^2$ with a $Z$-score of two $\sigma$s could be more worrying than a minuscule coefficient with a $Z$-score of five $\sigma$s.]

In order to get some insight on the effect of misspecification, we need to manually perform the RESET fit and make the relevant comparisons. For instance,
here I perform the extended fit with the $\text{(speed)}^2$ covariate and make a visual comparison of the two fits: 

```{r fit_cars_sq}
fit_cars_sq <- lm(dist ~ speed + I(speed*speed), data = cars)
with(data = cars, plot(speed, dist))
abline(fit_cars)
lines(x = cars$speed, y = fitted(fit_cars_sq), col = "blue")
```

In the quadratic fit, the expected change in $\text{dist}$ for a unit change in $\text{speed}$ - the so called "marginal effect" of $\text{speed}$ - depends on $\text{speed}$:

```{r cars_marginal_effects, code_folding=TRUE}
with(
	data = data.frame(
		speed = cars$speed,
		slope_quadratic = 
			coef(fit_cars_sq)[[2]] + 2 * coef(fit_cars_sq)[[3]] * cars$speed  
	), {
	plot(
		x = speed, y = slope_quadratic, 
		type = "l", col = "blue", 
		xlab = "speed", ylab = "Slope", 
		main = "'Speed' vs. 'Dist': marginal effect")
	abline(h = coef(fit_cars)[[2]])
})
```


## RESET + HC vcov: a simulation study

We consider a univariate regression problem, with a regressor $X \sim \mathcal N (0,1)$, a noise term $\varepsilon \sim \mathcal N (0,\sigma= 3)$ and a response $Y$. We will consider three ground truth distributions for $Y$ given $X$:

$$
\begin{split}
\text{T1}:& \qquad Y=X+\varepsilon\\
\text{T2}:& \qquad Y=X^3 + \varepsilon\\
\text{T3}:& \qquad Y=X + X^2\varepsilon
\end{split}
$$
We will study, through simulation, the $p$-value distribution of the RESET test for linear regression based on the model $Y = q+m X + \epsilon$, where $q$ and $m$ are unknown coefficients, and $\epsilon$ is Gaussian noise with unknown variance. It follows that the model is correctly specified with respect to $\text{T1}$, has functional misspecification with respect to  $\text{T2}$, and potentially noise misspecification^[Sometimes also referred to as "second order misspecification".] with respect to $\text{T3}$, if we model variance as being independent of $X$.

Data will consist of independent samples $(X_i, Y_i)$ from the joint distribution of $X$ and $Y$. To facilitate simulation, we define some helpers in the code chunk below.

```{r simulation_helpers, code_folding=TRUE}
#' Helper to generate data with prescribed: 
#' * Regressor distribution: `x`
#' * Response conditional mean: `f`
#' * Response conditional noise: `eps` 
dgp_fun <- function(x, f, eps) {
	function(n) {
		.x <- x(n)
		data.frame(x = .x, y = f(.x) + eps(.x))
	}
}

#' Helper to simulate results of linear regression, with prescribed:
#' * Data generating process: `dgp`
#' * Sample size of simulated datasets: `n`
#' * Summary function (e.g. p-value of RESET test): `summarize_fun`
lm_simulate <- function(dgp, n, summarize_fun, nsim, simplify) {
	replicate(nsim, {
		data <- dgp(n)
		fit <- lm(y ~ x, data)
		summarize_fun(fit)
	}, simplify = simplify)
} 

#' Helper to plot the PDF and CDF of a vector of simulated p-values
plot_pvalue <- function(
		p, xlim, main = "Distribution of p-values", sub = NULL
		) 
{
	oldpar <- par(mfrow = c(1, 2))
	hist(p, freq = FALSE, main = NULL)
	plot(
		ecdf(p),
		do.points = FALSE,
		xlim = xlim, 
		xlab = "p",
		ylab = "Empirical Quantile",
		main = NULL
		)
	abline(0, 1, lty = "dotted")
	#mtext(main, line = -2, outer = TRUE)
	title(main = main, outer = TRUE, line = -1)
	if(!is.null(sub))
		mtext(sub, line = -3, outer = TRUE)
	# title(main = main, sub = sub, outer = TRUE, line = -1)
	par(oldpar) 
}

#' Helper to perform RESET test on a `lm` fit object, and plot the p-value
#' distribution. The estimator for regression coefficients variance-covariance
#' matrix can be set through the `vcov` argument.
reset_pvalue <- function(
		dgp, n,  # Data generating process params
		power = 2:3, type = "regressor", vcov = sandwich::vcovHC,  # RESET params
		plot = TRUE, xlim = c(0, 0.1),  # Plotting params
		nsim = 1e4  # Simulation params
		) 
{
	summarize_fun <- function(fit)
		lmtest::resettest(fit, power = power, type = type, vcov = vcov)$p.value
	
	p <- lm_simulate(
		dgp = dgp, 
		n = n, 
		summarize_fun = summarize_fun, 
		nsim = nsim,
		simplify = TRUE
		)
	
	if (plot) {
		dgp_name = deparse(substitute(dgp))
		sub <- paste0("DGP: ", dgp_name, 
									" | n = ", n, 
									" | nsim = ", nsim)
		plot_pvalue(p, xlim = xlim, sub = sub)
	}
	
	return(invisible(p))
	
}
```

### T1: Correct specification

```{r dgp_t1}
dgp_t1 <- dgp_fun(
	x = rnorm,
	f = \(x) x,
	eps = \(x) rnorm(length(x), sd = 3)
)

set.seed(840)
with(data = dgp_m1(100), plot(x = x, y = y))
```

```{r reset_t1}
p <- reset_pvalue(dgp = dgp_t1, n = 100, nsim = 1e3)
```
```{r}
p <- reset_pvalue(dgp = dgp_t1, n = 10, nsim = 1e3, vcov = stats::vcov)
```


```{r t1_pquantiles}
quantile(p, c(0.001, 0.01, 0.05, 0.1))
```



### T2: Functional misspecification
```{r dgp_t2}
dgp_t2 <- dgp_fun(
	x = rnorm,
	f = \(x) x^3,
	eps = \(x) rnorm(length(x), sd = 3)
)

set.seed(840)
with(data = dgp_t2(100), plot(x = x, y = y))
```

```{r}
reset_pvalue(
	dgp = dgp_t2, n = 100, nsim = 1e3
)
```
### M3: Heteroskedastic noise

```{r}

```

