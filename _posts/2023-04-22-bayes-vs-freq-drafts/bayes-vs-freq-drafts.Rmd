---
title: "Bayes Game"
description: |
  Illustrating differences between confidence and credible intervals through 
  a betting game.
author:
  - name: vgherard
    url: https://vgherard.github.io
date: 2023-03-07
output:
  distill::distill_article:
    self_contained: false
header-includes:
  - \usepackage{blkarray}
  - \usepackage{amsmath}
draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Intro

I have always felt a bit uncomfortable with the way I was taught about the 
differences between Frequentist and Bayesian methods. 
Going back ten years, I remember a lot of emphasis on how "Frequentists" and
"Bayesians" diverged in their interpretations of probabilities, which 
*naturally* led to approaching inferential problems in different ways.
Probably with some distortion from my part, this led me to a strange and, from 
several points of view, wrong idea: 
that whether you used a given formalism was a matter of committing to one 
or the other particular school of thought, and that there was no point in trying 
to compare the constructs of the two.

Now, while the philosophical debate is one I honestly wish I could speak about 
with knowledge, in this post I want to tackle a way more down-to-earth topic: 
Bayesian *vs.* Frequentist methods for gambling^[The introduction of 
bets as an expedient to operationally define subjective probabilities is 
historically due to the Italian mathematician
[Bruno de Finetti](https://en.wikipedia.org/wiki/Bruno_de_Finetti).]. In 
particular, we will contrast the frequentist's and bayesian's way of 
reporting measurements in the form of bets 
(*« Let's bet that our number of interest lies in this interval. »*). Two 
general features emerge from this comparison:

1. From the **purely operational** point of view, the question 
*"Bayesian or Frequentist?"* always has a well defined answer.
2. The answer is neither "Frequentist" nor "Bayesian", but rather "It depends".

# Intro

I have always felt a bit uneasy with the way I was taught about the 
differences between frequentist and bayesian methods. Back in my freshman years,
I recall that the explanations were mostly focused on how "frequentists" and
"bayesians" had diverging opinions on what kind of objects you may attach a 
probability to, which *naturally* led to approaching inferential problems in a 
different way. This led me to a strange idea, a sort of naive extrapolation: 
that whether you used a given formalism was a matter of committing to 
one particular philosophy or the other, and that there was no point in trying 
to compare the constructs of the two.

Now, I am deeply convinced that injecting too many opinions and too few facts 
into young scientists' minds is a superb strategy to grow orthodox imbeciles 
^[Here's a list of topics where I believe education focuses too much on opinions 
and too little on facts: Frequentist vs. Bayesian statistical inference; 
Interpretations of Quantum Mechanics; The derivation of Statistical Mechanics 
from first principles; The naturalness problem of the Standard Model...]. 
In hindsight, I can see why young me's reasoning was fallacious: it simply 
failed to distinguish mathematical methods from epistemics. 
On the mathematical grounding, both methods are born out of the 
one and only theory of probability (***cit Kolmogorov***), and one can obviously 
make meaningful comparisons and choices between the two, on a purely rational 
basis.

To demonstrate this, I'm going to describe two games of chance, that represent 
the related scientific tasks of hypothesis testing and reporting measurements.
We will see that, although the games are readily interpreted in the frequentist
framework, depending on a small but critical variation of the games' rules, 
the optimal strategies for the players involved can be either frequentist" or 
"bayesian".

# Hypothesis Testing

The game involves a Referee and two players, say Elis and Tom, 
that compete against each other in an iterative game. 
During each iteration, Tom tries to guess the outcome of a biased coin, 
with a established absolute rewards for a guess and penalizations for a mistake 
(fixed by the Referee at the beginning of the game):

|                | $S_1 = T$         | $S_1 = H$         |
|---------------:|------------------:|------------------:|
|$\hat S _1 = T$ | $G _{\text{TP}}$  | $-L _{\text{FP}}$ |
|$\hat S _1 = H$ | $-L _{\text{FN}}$ | $G _{\text{TN}}$  |

However, **Elis** can choose to flip the sign of this reward matrix 
(thereby penalizing a guess), based on some information provided by the Referee. 
The goals for Tom and Elis are to maximize and minimize the total reward, 
respectively.

### Rules of the game

Specifically, a single iteration proceeds as follows:

1. The **Referee** tosses a biased coin and annotates its outcome 
$S_1=0,\,1$ (heads or tails). She then flips the coin with a certain
(fixed throughout the game) probability, and communicates the resulting value 
$S_2=0,\,1$ to **Tom**.
2. **Tom** makes a guess $\hat S_1$ for the outcome of the first toss $S_1$, and
communicates his guess to the **Referee**.
3. The **Referee** provides *some information* to **Elis**.
4. **Elis** chooses whether or not to multiply the reward matrix by an overall
sign $\sigma = - 1$ thereby penalizing a guess and rewarding a mistake.
5. The **Referee** communicates the outcome of the iteration to the players.

As to the third step, we will consider three variants of the game's rules: 

A) **Elis** is provided with the value $S_2$.
B) **Elis** is provided with the value $S_1$.
C) **Elis** is provided no additional information.

### Analysis

We can write the reward in matrix form as follows:

$$
\begin{split}
R   & = \sigma \cdot R_0 \\
R_0 & = \begin{pmatrix}
\hat S_1 &
1-\hat S_1
\end{pmatrix} \begin{pmatrix}
G_{\text{TP}} & -L_{\text{FP}} \\
-L_{\text{FN}} & G_{\text{TN}}
\end{pmatrix}
\begin{pmatrix}
S_1 \\
1-S_1
\end{pmatrix}
\end{split}
$$
The expected reward is:

$$
\mathbb E(R)=\intop\text{d}P(S_1,S_2,\hat S_1)\,R
$$

where we have assumed that Elis' choice for $\sigma$ will be, in general, 
completely determined by the information available to her^[So that it does not 
appear as an additional variable in the integration measure $\text{d}P(\cdots)$.
On the other hand, Tom may still want to introduce some randomness in the 
$S_2 \mapsto \hat S_1$ relation.].

### Variant A

We rewrite .. as follows:

$$
\mathbb E (R) = \intop \text d P(S_2)\intop \text{d}P(S_1,\hat S_1\vert S_2)\,\sigma \cdot R_0
$$

If Elis knows the value of $S_2$, she can choose:

$$
\sigma = -\text{sgn}\left(\mathbb E(R_0 \vert S_2)\right)
$$
where:
$$
\mathbb E(R_0 \vert S_2)=\intop \text{d}P(S_1,\hat S_1\vert S_2)\,\cdot R_0
$$
in which case, the expected reward becomes:

$$
\mathbb E (R) = -\intop \text d P(S_2) \,\left|\mathbb E(R_0 \vert S_2)\right|
$$

This is never positive, and the optimal strategy for Tom is clearly to make 
$\mathbb E (R_0 \vert S_2)$ as small as possible in absolute value. This is 
attained with the (deterministic) choice:

$$
\hat{S}_1(S_2) = \begin{cases}
1 & \dfrac{\text{Pr}(S_1=1\vert S_2)}{\text{Pr}(S_1=0\vert S_2)} \leq \dfrac{L_\text{FP}+G_\text{TN}}{G_{\text{TP}}+L_\text{FN}} \\
0 & \text{otherwise}
\end{cases}
$$

### Quantum version

We start with a couple game :) Consider two entangled spins (of two 
distinguishable particles) ***Perhaps better entangled spin and momentum? to have a continuous likelihood ratio...***:

$$
\vert \alpha \rangle = \sqrt \theta\vert +_z\rangle _1 \otimes \vert +_{z'}\rangle _2+ \sqrt {1-\theta}\vert -_z\rangle _1 \otimes \vert -_{z'}\rangle _2,
$$
where $z$ and $z'$ are two different, slightly tilted cartesian axes (
see Fig). 1. There are two quantum scientists, say Tom and Elis, that can only 
measure spins along the $z$ axis. The game proceeds as follows:

- **Tom** measures the first spin and annotates its value $S_1 ^T $.
- **Elis** measures the second spin *along the same axis $z$* 
($S_2= \pm \frac{1}{2}$) and makes a guess $S_1 ^E$ on the value $S_1 ^T$ 
measured by **Tom**.
- The couple will get rewarded (or penalised) according to the following table:

|                        | $S_1 ^T = +\frac{1}{2}$  | $S_1^T = -\frac{1}{2}$ |
|-----------------------:|-------------------------:|-----------------------:|
|$S_1 ^E = +\frac{1}{2}$ | $G _{\text{TP}}$         |  $-L _{\text{FP}}$     |
|$S_1 ^E = -\frac{1}{2}$ | $-L _{\text{FN}}$        | $G _{\text{TN}}$       |

### Analysis

From a physical point of view, the first measurement of $S_1$ performed by Tom 
makes the system collapse into one of the two states 
$\theta\vert \pm_z\rangle \otimes \vert \pm_{z'}\rangle$ with probability 
$\theta$ and $1-\theta$, respectively. In these two different states, the 
distribution of $S_2$ is different, so that the outcome of Elis' measurement 
hints...

The expected reward is given by:

$$
\begin{split}
\mathbb E(R) &= \text{Pr}(
	S_1^T=+) \times \left[ G_{\text{TP}}\cdot\text{Pr}(S_1^E=+\vert S_1^T=+)-L_{\text{FN}}\cdot\text{Pr}(S_1^E=-\vert S_1^T=+) \right] +\\
	& + \text{Pr}(S_1^T=-) \times \left[ -L_{\text{FP}}\cdot\text{Pr}(S_1^E=+\vert S_1^T=-)+G_{\text{TN}}\cdot\text{Pr}(S_1^E=-\vert S_1^T=-) \right]
	\end{split}
$$
This can be rewritten in terms of the positive, false positive, negative and 
false negative rates:

$$
\mathbb E(R)=-\text{PR}\cdot(G_\text{TP}+L_\text{FN})\cdot \text{FNR}
							-\text{NR}\cdot(L_\text{FP}+G_\text{TN})\cdot \text{FPR} +\text{const.}
$$
where the constant is independent of $\text{FNR}$ and $\text{FPR}$, so that it 
can be omitted. In a frequentist framework, the optimal strategy to get the 
maximum possible expected reward can be described as follows: from the Neyman 
Pearson lemma, we know that the optimal test will be the likelihood-ratio test 
for the hypotheses $S_1 ^T = \pm \frac{1}{2}$. We can then tune the $\text{FPR}$
so that: 

$$
\frac{\text{d} (\text{FNR})}{\text{d} (\text{FPR})} = 
-\dfrac{\text{NR}}{\text{PR}}\cdot \dfrac{L_\text{FP}+G_\text{TN}}{G_\text{TP}+L_\text{FN}}
$$
where the expected reward has a maximum (assuming $\text{FNR}$ to be a convex 
function of $\text{FPR}$).

Elis uses the Neyman Pearson lemma, so that here inference is based on:

!!! In order to apply Neyman Pearson, one would have to randomize the test to 
obtain the exact significance... can the reasoning be saved somehow, though??

$$
\Lambda \equiv \dfrac{\text{Pr}(S_2 ^ E \vert S_1 ^T = +\frac{1}{2})}
{\text{Pr}(S_2 ^ E \vert S_1 ^T = -\frac{1}{2})} \leq k
$$
We can see that:

$$
\frac{\text{d} (\text{FNR})}{\text{d} (\text{FPR})} = 
-\frac{1}{k},
$$
a condition that is ensured by the relations:

$$
\text{FPR} = \text{Pr}(\Lambda \leq k \vert H_0)\\
1 - \text{FNR} = \text{Pr}(\Lambda \leq k \vert H_1) = \dfrac{1}{k}\text{FPR}
$$
since $\text{Pr}(\Lambda \leq k \vert H_1) = k \cdot \text{Pr}(\Lambda \leq k \vert H_0) $
That is to say:

$$
\dfrac{\text{Pr}(S_1 ^T = +\frac{1}{2}\vert S_2 ^E)}
{\text{Pr}(S_1 ^T = -\frac{1}{2}\vert S_2 ^E)} \leq k \dfrac{\text{Pr}(S_1 ^T = +\frac{1}{2})}
{\text{Pr}(S_1 ^T = -\frac{1}{2}
)} 
$$

A bayesian Elis would immediately answers that we should bet on 
$S_1 ^T = -\frac{1}{2}$ if:

$$
\text{Pr}(S_1 ^T = +\frac{1}{2}\vert S_2 ^E)\cdot G_\text{TP} - \text{Pr}(S_1 ^T = -\frac{1}{2}\vert S_2 ^E)\cdot L_\text{FP} \leq - \text{Pr}(S_1 ^T = +\frac{1}{2}\vert S_2 ^E)\cdot L_\text{FN}+\text{Pr}(S_1 ^T = -\frac{1}{2}\vert S_2 ^E)\cdot G_\text{TN}
$$
because the LHS and RHS are respectively the posterior expected rewards for the 
guesses $S_1 ^E = \pm \frac{1}{2}$. This provides the constant that one should
use in the Neyman Pearson lemma:

$$
k = \frac{G_{\text{TN}}+L_{\text{FP}}}{G_{\text{TP}}+L_{\text{FN}}}\cdot 
\dfrac{\text{PR}}{\text{NR}}
$$
