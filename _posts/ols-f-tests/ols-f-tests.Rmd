---
title: "OLS F-tests"
description: |
  A short description of the post.
author:
  - name: vgherard
    url: https://vgherard.github.io
date: 2023-10-06
output:
  distill::distill_article:
    self_contained: false
draft: true
---


Let $Y\in\mathbb{R}^{n\times1}$ and $X\in\mathbb{R}^{n\times p}$ be random 
variables and denote $H=X(X^{T}X)^{-1}X^{T}$. The usual OLS estimator is:

$$
\hat{Y}=HY=X\hat{\beta}
\\
\hat{\beta}=(X^{T}X)^{-1}X^{T}Y.
$$
We assume that data are i.i.d. draws from a joint distribution 
$\mathcal{F}(X,Y)$, and we denote:

$$
\beta	=\arg\min_{\beta^{\prime}}\mathbb{E}((Y-X\beta^{\prime})^{2}),\\
\epsilon	=Y-X\beta.
$$
The OLS residuals are:

$$
\hat r = Y-\hat Y=(1-H)Y = (1-H)\epsilon
$$
and the Residuals Squared of Sum is:

$$
\text{RSS}=\hat r^T\hat r=\epsilon^T(1-H)\epsilon
$$

Furthermore, we have:

$$
\hat \beta =\beta + (X^TX)^{-1}X^T\epsilon 
$$
For the moment, we have not assumed anything on the “error” term $\epsilon$ 
(in particular, we may have $\mathbb{E}(\epsilon\vert X)\neq0$ and 
$\mathbb{V}(\epsilon\vert X)=v(X)$ for some non-constant positive function 
$v(\cdot)$).
If the error term is normal with constant variance, these equations imply that 
$\hat \beta$ and $\hat r$ are conditionally independent. 
Furthermore both $\hat r$ and  $\hat \beta$ have Gaussian $X$-conditional 
distribution, and:

$$
\mathbb E(\hat r ^T\hat r\vert X) = (n-p)\sigma ^2,\\
\mathbb V(\hat \beta\vert X)=(X^T X) ^{-1} \sigma^2
$$
These information can be used to construct confidence sets for the parameter 
vector $\beta$.

—

Let us now decompose $X$ as follows:

$$
X=\begin{pmatrix}X_{0} & X_{1}\end{pmatrix},
$$ 

where $X_{0}\in\mathbb{R}^{n\times k}$ and $X_{1}\in\mathbb{R}^{n\times(p-k)}$,
and denote by $H_0$, $\hat \beta _0$, etc. the corresponding quantities for the
OLS with this reduced number of regressors.


*Theorem.* $H H_0 = H_0$.

The proof is immediate if we recall the interpretation of $H$ as the orthogonal
projection on the subspace of $\mathbb R ^n$ generated by the regressors 
vectors. The second projection clearly has no effect.

As a consequence the projectors $1-H$ and $H-H_0$ are orthogonal.

For a similar reason, we have the following independence relations (denote $\perp$):

$$
\hat \beta _0 \perp \hat r_0,\\
\hat \beta _0 \perp \hat r,\\
\hat \beta \perp \hat \beta _0, 
$$
Since $\hat r = (1-H) \epsilon$, this implies that:

$$
\hat r_0 ^T \hat r_0 = \hat r_1^T\hat r_1+(\hat r_0^T\hat r_0-\hat r_1^T\hat r_1)
$$
where the two terms are *independent* $\chi ^2$ variables. Since the sum and
ratios of these are still independent, we find independence of:

$$

$$
