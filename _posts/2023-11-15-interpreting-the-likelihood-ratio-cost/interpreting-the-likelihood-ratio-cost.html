<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { color: #00769e; background-color: #f1f3f5; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #00769e; } /* Normal */
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #657422; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #00769e; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #00769e; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #00769e; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #5e5e5e; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

<style>
  div.csl-bib-body { }
  div.csl-entry {
    clear: both;
    }
  .hanging div.csl-entry {
    margin-left:2em;
    text-indent:-2em;
  }
  div.csl-left-margin {
    min-width:2em;
    float:left;
  }
  div.csl-right-inline {
    margin-left:2em;
    padding-left:1em;
  }
  div.csl-indent {
    margin-left: 2em;
  }
</style>

  <!--radix_placeholder_meta_tags-->
  <title>Interpreting the Likelihood Ratio cost</title>

  <meta property="description" itemprop="description" content="Analysis of infinite sample properties and comparison with cross-entropy loss."/>


  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2023-11-15"/>
  <meta property="article:created" itemprop="dateCreated" content="2023-11-15"/>
  <meta name="article:author" content="vgherard"/>

  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Interpreting the Likelihood Ratio cost"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="Analysis of infinite sample properties and comparison with cross-entropy loss."/>
  <meta property="og:locale" content="en_US"/>

  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Interpreting the Likelihood Ratio cost"/>
  <meta property="twitter:description" content="Analysis of infinite sample properties and comparison with cross-entropy loss."/>

  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Application-independent evaluation of speaker detection;citation_volume=20;citation_doi=https://doi.org/10.1016/j.csl.2005.08.001;citation_issn=0885-2308;citation_author=Niko Brümmer;citation_author=Johan Preez"/>
  <meta name="citation_reference" content="citation_title=Elements of information theory 2nd edition (wiley series in telecommunications and signal processing);citation_publisher=Hardcover; Wiley-Interscience;citation_author=Thomas M. Cover;citation_author=Joy A. Thomas"/>
  <!--radix_placeholder_rmarkdown_metadata-->

  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","output","bibliography","categories","draft"]}},"value":[{"type":"character","attributes":{},"value":["Interpreting the Likelihood Ratio cost"]},{"type":"character","attributes":{},"value":["Analysis of infinite sample properties and comparison with cross-entropy loss.\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["vgherard"]},{"type":"character","attributes":{},"value":["https://vgherard.github.io"]}]}]},{"type":"character","attributes":{},"value":["2023-11-15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["biblio.bib"]},{"type":"character","attributes":{},"value":["Forensic Science","Bayesian Methods","Information Theory","Probability Theory","R"]},{"type":"logical","attributes":{},"value":[false]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["biblio.bib","interpreting-the-likelihood-ratio-cost_files/anchor-4.2.2/anchor.min.js","interpreting-the-likelihood-ratio-cost_files/bowser-1.9.3/bowser.min.js","interpreting-the-likelihood-ratio-cost_files/distill-2.2.21/template.v2.js","interpreting-the-likelihood-ratio-cost_files/figure-html5/unnamed-chunk-3-1.png","interpreting-the-likelihood-ratio-cost_files/figure-html5/unnamed-chunk-4-1.png","interpreting-the-likelihood-ratio-cost_files/header-attrs-2.24/header-attrs.js","interpreting-the-likelihood-ratio-cost_files/jquery-3.6.0/jquery-3.6.0.js","interpreting-the-likelihood-ratio-cost_files/jquery-3.6.0/jquery-3.6.0.min.js","interpreting-the-likelihood-ratio-cost_files/jquery-3.6.0/jquery-3.6.0.min.map","interpreting-the-likelihood-ratio-cost_files/popper-2.6.0/popper.min.js","interpreting-the-likelihood-ratio-cost_files/tippy-6.2.7/tippy-bundle.umd.min.js","interpreting-the-likelihood-ratio-cost_files/tippy-6.2.7/tippy-light-border.css","interpreting-the-likelihood-ratio-cost_files/tippy-6.2.7/tippy.css","interpreting-the-likelihood-ratio-cost_files/tippy-6.2.7/tippy.umd.min.js","interpreting-the-likelihood-ratio-cost_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

  <style type="text/css">

  body {
    background-color: white;
  }

  .pandoc-table {
    width: 100%;
  }

  .pandoc-table>caption {
    margin-bottom: 10px;
  }

  .pandoc-table th:not([align]) {
    text-align: left;
  }

  .pagedtable-footer {
    font-size: 15px;
  }

  d-byline .byline {
    grid-template-columns: 2fr 2fr;
  }

  d-byline .byline h3 {
    margin-block-start: 1.5em;
  }

  d-byline .byline .authors-affiliations h3 {
    margin-block-start: 0.5em;
  }

  .authors-affiliations .orcid-id {
    width: 16px;
    height:16px;
    margin-left: 4px;
    margin-right: 4px;
    vertical-align: middle;
    padding-bottom: 2px;
  }

  d-title .dt-tags {
    margin-top: 1em;
    grid-column: text;
  }

  .dt-tags .dt-tag {
    text-decoration: none;
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0em 0.4em;
    margin-right: 0.5em;
    margin-bottom: 0.4em;
    font-size: 70%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  d-article table.gt_table td,
  d-article table.gt_table th {
    border-bottom: none;
    font-size: 100%;
  }

  .html-widget {
    margin-bottom: 2.0em;
  }

  .l-screen-inset {
    padding-right: 16px;
  }

  .l-screen .caption {
    margin-left: 10px;
  }

  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .shaded-content {
    background: white;
  }

  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }

  .hidden {
    display: none !important;
  }

  d-article {
    padding-top: 2.5rem;
    padding-bottom: 30px;
  }

  d-appendix {
    padding-top: 30px;
  }

  d-article>p>img {
    width: 100%;
  }

  d-article h2 {
    margin: 1rem 0 1.5rem 0;
  }

  d-article h3 {
    margin-top: 1.5rem;
  }

  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }

  /* Tweak code blocks */

  d-article div.sourceCode code,
  d-article pre code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: auto;
  }

  d-article div.sourceCode {
    background-color: white;
  }

  d-article div.sourceCode pre {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }

  d-article pre {
    font-size: 12px;
    color: black;
    background: none;
    margin-top: 0;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  d-article pre a {
    border-bottom: none;
  }

  d-article pre a:hover {
    border-bottom: none;
    text-decoration: underline;
  }

  d-article details {
    grid-column: text;
    margin-bottom: 0.8em;
  }

  @media(min-width: 768px) {

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: visible !important;
  }

  d-article div.sourceCode pre {
    padding-left: 18px;
    font-size: 14px;
  }

  d-article pre {
    font-size: 14px;
  }

  }

  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  /* CSS for d-contents */

  .d-contents {
    grid-column: text;
    color: rgba(0,0,0,0.8);
    font-size: 0.9em;
    padding-bottom: 1em;
    margin-bottom: 1em;
    padding-bottom: 0.5em;
    margin-bottom: 1em;
    padding-left: 0.25em;
    justify-self: start;
  }

  @media(min-width: 1000px) {
    .d-contents.d-contents-float {
      height: 0;
      grid-column-start: 1;
      grid-column-end: 4;
      justify-self: center;
      padding-right: 3em;
      padding-left: 2em;
    }
  }

  .d-contents nav h3 {
    font-size: 18px;
    margin-top: 0;
    margin-bottom: 1em;
  }

  .d-contents li {
    list-style-type: none
  }

  .d-contents nav > ul {
    padding-left: 0;
  }

  .d-contents ul {
    padding-left: 1em
  }

  .d-contents nav ul li {
    margin-top: 0.6em;
    margin-bottom: 0.2em;
  }

  .d-contents nav a {
    font-size: 13px;
    border-bottom: none;
    text-decoration: none
    color: rgba(0, 0, 0, 0.8);
  }

  .d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6)
  }

  .d-contents nav > ul > li > a {
    font-weight: 600;
  }

  .d-contents nav > ul > li > ul {
    font-weight: inherit;
  }

  .d-contents nav > ul > li > ul > li {
    margin-top: 0.2em;
  }


  .d-contents nav ul {
    margin-top: 0;
    margin-bottom: 0.25em;
  }

  .d-article-with-toc h2:nth-child(2) {
    margin-top: 0;
  }


  /* Figure */

  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }

  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }

  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }

  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }

  /* Citations */

  d-article .citation {
    color: inherit;
    cursor: inherit;
  }

  div.hanging-indent{
    margin-left: 1em; text-indent: -1em;
  }

  /* Citation hover box */

  .tippy-box[data-theme~=light-border] {
    background-color: rgba(250, 250, 250, 0.95);
  }

  .tippy-content > p {
    margin-bottom: 0;
    padding: 2px;
  }


  /* Tweak 1000px media break to show more text */

  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }

    .grid {
      grid-column-gap: 16px;
    }

    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }

  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }

    .grid {
      grid-column-gap: 32px;
    }
  }


  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */

  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  /* Include appendix styles here so they can be overridden */

  d-appendix {
    contain: layout style;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-top: 60px;
    margin-bottom: 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    color: rgba(0,0,0,0.5);
    padding-top: 60px;
    padding-bottom: 48px;
  }

  d-appendix h3 {
    grid-column: page-start / text-start;
    font-size: 15px;
    font-weight: 500;
    margin-top: 1em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.65);
  }

  d-appendix h3 + * {
    margin-top: 1em;
  }

  d-appendix ol {
    padding: 0 0 0 15px;
  }

  @media (min-width: 768px) {
    d-appendix ol {
      padding: 0 0 0 30px;
      margin-left: -30px;
    }
  }

  d-appendix li {
    margin-bottom: 1em;
  }

  d-appendix a {
    color: rgba(0, 0, 0, 0.6);
  }

  d-appendix > * {
    grid-column: text;
  }

  d-appendix > d-footnote-list,
  d-appendix > d-citation-list,
  d-appendix > distill-appendix {
    grid-column: screen;
  }

  /* Include footnote styles here so they can be overridden */

  d-footnote-list {
    contain: layout style;
  }

  d-footnote-list > * {
    grid-column: text;
  }

  d-footnote-list a.footnote-backlink {
    color: rgba(0,0,0,0.3);
    padding-left: 0.5em;
  }



  /* Anchor.js */

  .anchorjs-link {
    /*transition: all .25s linear; */
    text-decoration: none;
    border-bottom: none;
  }
  *:hover > .anchorjs-link {
    margin-left: -1.125em !important;
    text-decoration: none;
    border-bottom: none;
  }

  /* Social footer */

  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }

  .disqus-comments {
    margin-right: 30px;
  }

  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }

  #disqus_thread {
    margin-top: 30px;
  }

  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }

  .article-sharing a:hover {
    border-bottom: none;
  }

  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }

  .subscribe p {
    margin-bottom: 0.5em;
  }


  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }


  .sidebar-section.custom {
    font-size: 12px;
    line-height: 1.6em;
  }

  .custom p {
    margin-bottom: 0.5em;
  }

  /* Styles for listing layout (hide title) */
  .layout-listing d-title, .layout-listing .d-title {
    display: none;
  }

  /* Styles for posts lists (not auto-injected) */


  .posts-with-sidebar {
    padding-left: 45px;
    padding-right: 45px;
  }

  .posts-list .description h2,
  .posts-list .description p {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  }

  .posts-list .description h2 {
    font-weight: 700;
    border-bottom: none;
    padding-bottom: 0;
  }

  .posts-list h2.post-tag {
    border-bottom: 1px solid rgba(0, 0, 0, 0.2);
    padding-bottom: 12px;
  }
  .posts-list {
    margin-top: 60px;
    margin-bottom: 24px;
  }

  .posts-list .post-preview {
    text-decoration: none;
    overflow: hidden;
    display: block;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    padding: 24px 0;
  }

  .post-preview-last {
    border-bottom: none !important;
  }

  .posts-list .posts-list-caption {
    grid-column: screen;
    font-weight: 400;
  }

  .posts-list .post-preview h2 {
    margin: 0 0 6px 0;
    line-height: 1.2em;
    font-style: normal;
    font-size: 24px;
  }

  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.4em;
    font-size: 16px;
  }

  .posts-list .post-preview .thumbnail {
    box-sizing: border-box;
    margin-bottom: 24px;
    position: relative;
    max-width: 500px;
  }
  .posts-list .post-preview img {
    width: 100%;
    display: block;
  }

  .posts-list .metadata {
    font-size: 12px;
    line-height: 1.4em;
    margin-bottom: 18px;
  }

  .posts-list .metadata > * {
    display: inline-block;
  }

  .posts-list .metadata .publishedDate {
    margin-right: 2em;
  }

  .posts-list .metadata .dt-authors {
    display: block;
    margin-top: 0.3em;
    margin-right: 2em;
  }

  .posts-list .dt-tags {
    display: block;
    line-height: 1em;
  }

  .posts-list .dt-tags .dt-tag {
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0.3em 0.4em;
    margin-right: 0.2em;
    margin-bottom: 0.4em;
    font-size: 60%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  .posts-list img {
    opacity: 1;
  }

  .posts-list img[data-src] {
    opacity: 0;
  }

  .posts-more {
    clear: both;
  }


  .posts-sidebar {
    font-size: 16px;
  }

  .posts-sidebar h3 {
    font-size: 16px;
    margin-top: 0;
    margin-bottom: 0.5em;
    font-weight: 400;
    text-transform: uppercase;
  }

  .sidebar-section {
    margin-bottom: 30px;
  }

  .categories ul {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }

  .categories li {
    color: rgba(0, 0, 0, 0.8);
    margin-bottom: 0;
  }

  .categories li>a {
    border-bottom: none;
  }

  .categories li>a:hover {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  }

  .categories .active {
    font-weight: 600;
  }

  .categories .category-count {
    color: rgba(0, 0, 0, 0.4);
  }


  @media(min-width: 768px) {
    .posts-list .post-preview h2 {
      font-size: 26px;
    }
    .posts-list .post-preview .thumbnail {
      float: right;
      width: 30%;
      margin-bottom: 0;
    }
    .posts-list .post-preview .description {
      float: left;
      width: 45%;
    }
    .posts-list .post-preview .metadata {
      float: left;
      width: 20%;
      margin-top: 8px;
    }
    .posts-list .post-preview p {
      margin: 0 0 12px 0;
      line-height: 1.5em;
      font-size: 16px;
    }
    .posts-with-sidebar .posts-list {
      float: left;
      width: 75%;
    }
    .posts-with-sidebar .posts-sidebar {
      float: right;
      width: 20%;
      margin-top: 60px;
      padding-top: 24px;
      padding-bottom: 24px;
    }
  }


  /* Improve display for browsers without grid (IE/Edge <= 15) */

  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }

  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }

  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }

  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }

  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }

  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }

  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }


  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }

  .downlevel .footnotes ol {
    padding-left: 13px;
  }

  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }

  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }

  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }

  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  .downlevel .posts-list .post-preview {
    color: inherit;
  }



  </style>

  <script type="application/javascript">

  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }

  // show body when load is complete
  function on_load_complete() {

    // add anchors
    if (window.anchors) {
      window.anchors.options.placement = 'left';
      window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
    }


    // set body to visible
    document.body.style.visibility = 'visible';

    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }

    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }

  function init_distill() {

    init_common();

    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);

    // create d-title
    $('.d-title').changeElementType('d-title');

    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);

    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();

    // move posts container into article
    $('.posts-container').appendTo($('d-article'));

    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');

    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;

    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();

    // move refs into #references-listing
    $('#references-listing').replaceWith($('#refs'));

    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-contents a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });

    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');

    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {

      // capture layout
      var layout = $(this).attr('data-layout');

      // apply layout to markdown level block elements
      var elements = $(this).children().not('details, div.sourceCode, pre, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });


      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });

    // remove code block used to force  highlighting css
    $('.distill-force-highlighting-css').parent().remove();

    // remove empty line numbers inserted by pandoc when using a
    // custom syntax highlighting theme
    $('code.sourceCode a:empty').remove();

    // load distill framework
    load_distill_framework();

    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {

      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;

      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');

      // article with toc class
      $('.d-contents').parent().addClass('d-article-with-toc');

      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

      // add orcid ids
      $('.authors-affiliations').find('.author').each(function(i, el) {
        var orcid_id = front_matter.authors[i].orcidID;
        if (orcid_id) {
          var a = $('<a></a>');
          a.attr('href', 'https://orcid.org/' + orcid_id);
          var img = $('<img></img>');
          img.addClass('orcid-id');
          img.attr('alt', 'ORCID ID');
          img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
          a.append(img);
          $(this).append(a);
        }
      });

      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }

      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");

      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }

       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }

      // remove d-appendix and d-footnote-list local styles
      $('d-appendix > style:first-child').remove();
      $('d-footnote-list > style:first-child').remove();

      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();

      // hoverable references
      $('span.citation[data-cites]').each(function() {
        const citeChild = $(this).children()[0]
        // Do not process if @xyz has been used without escaping and without bibliography activated
        // https://github.com/rstudio/distill/issues/466
        if (citeChild === undefined) return true

        if (citeChild.nodeName == "D-FOOTNOTE") {
          var fn = citeChild
          $(this).html(fn.shadowRoot.querySelector("sup"))
          $(this).id = fn.id
          fn.remove()
        }
        var refs = $(this).attr('data-cites').split(" ");
        var refHtml = refs.map(function(ref) {
          // Could use CSS.escape too here, we insure backward compatibility in navigator
          return "<p>" + $('div[id="ref-' + ref + '"]').html() + "</p>";
        }).join("\n");
        window.tippy(this, {
          allowHTML: true,
          content: refHtml,
          maxWidth: 500,
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        });
      });

      // fix footnotes in tables (#411)
      // replacing broken distill.pub feature
      $('table d-footnote').each(function() {
        // we replace internal showAtNode methode which is triggered when hovering a footnote
        this.hoverBox.showAtNode = function(node) {
          // ported from https://github.com/distillpub/template/pull/105/files
          calcOffset = function(elem) {
              let x = elem.offsetLeft;
              let y = elem.offsetTop;
              // Traverse upwards until an `absolute` element is found or `elem`
              // becomes null.
              while (elem = elem.offsetParent && elem.style.position != 'absolute') {
                  x += elem.offsetLeft;
                  y += elem.offsetTop;
              }

              return { left: x, top: y };
          }
          // https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetTop
          const bbox = node.getBoundingClientRect();
          const offset = calcOffset(node);
          this.show([offset.left + bbox.width, offset.top + bbox.height]);
        }
      })

      // clear polling timer
      clearInterval(tid);

      // show body now that everything is ready
      on_load_complete();
    }

    var tid = setInterval(distill_post_process, 50);
    distill_post_process();

  }

  function init_downlevel() {

    init_common();

     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));

    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;

    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();

    // remove toc
    $('.d-contents').remove();

    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });


    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);

    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();

    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });

    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));

    $('body').addClass('downlevel');

    on_load_complete();
  }


  function init_common() {

    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};

        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });

        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);

    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});

    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      // ignore leaflet img layers (#106)
      figures = figures.filter(':not(img[class*="leaflet"])')
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });

      }
    });

    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });

    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace(/^index[.]html/, "./"));
      });
    }

    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');

    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");

    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();

    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }

  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });

  </script>

  <!--/radix_placeholder_distill-->
  <script src="interpreting-the-likelihood-ratio-cost_files/header-attrs-2.24/header-attrs.js"></script>
  <script src="interpreting-the-likelihood-ratio-cost_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="interpreting-the-likelihood-ratio-cost_files/popper-2.6.0/popper.min.js"></script>
  <link href="interpreting-the-likelihood-ratio-cost_files/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="interpreting-the-likelihood-ratio-cost_files/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="interpreting-the-likelihood-ratio-cost_files/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="interpreting-the-likelihood-ratio-cost_files/anchor-4.2.2/anchor.min.js"></script>
  <script src="interpreting-the-likelihood-ratio-cost_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="interpreting-the-likelihood-ratio-cost_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="interpreting-the-likelihood-ratio-cost_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Interpreting the Likelihood Ratio cost","description":"Analysis of infinite sample properties and comparison with cross-entropy loss.","authors":[{"author":"vgherard","authorURL":"https://vgherard.github.io","affiliation":"&nbsp;","affiliationURL":"#","orcidID":""}],"publishedDate":"2023-11-15T00:00:00.000+01:00","citationText":"vgherard, 2023"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Interpreting the Likelihood Ratio cost</h1>
<!--radix_placeholder_categories-->
<div class="dt-tags">
<div class="dt-tag">Forensic Science</div>
<div class="dt-tag">Bayesian Methods</div>
<div class="dt-tag">Information Theory</div>
<div class="dt-tag">Probability Theory</div>
<div class="dt-tag">R</div>
</div>
<!--/radix_placeholder_categories-->
<p><p>Analysis of infinite sample properties and comparison with cross-entropy loss.</p></p>
</div>

<div class="d-byline">
  vgherard <a href="https://vgherard.github.io" class="uri">https://vgherard.github.io</a> 
  
<br/>2023-11-15
</div>

<div class="d-article">
<h2 id="intro">Intro</h2>
<p>During the last few months, I’ve been working on a machine learning algorithm with applications in <a href="https://en.wikipedia.org/wiki/Forensic_science">Forensic Science</a>, a.k.a. Criminalistics.
In this field, one common task for the data analyst is to present the <em>trier-of-fact</em> (the person or people who determine the facts in a legal proceeding) with a numerical assessment of the strength of the evidence provided by available data towards different hypotheses. In more familiar terms, the forensic expert is responsible of computing the likelihoods (or likelihood ratios) of data under competing hypotheses, which are then used by the trier-of-fact to produce Bayesian posterior probabilities for the hypotheses in question<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>In relation to this, forensic scientists have developed a bunch of techniques to evaluate the performance of a likelihood ratio model in discriminating between two alternative hypothesis. In particular, I have come across the so called <em>Likelihood Ratio Cost</em>, usually defined as:</p>
<p><span class="math display" id="eq:CLLR">\[
C_{\text{LLR}} = \frac{1}{2N_1} \sum _{Y_i=1} \log(1+r(X_i) ^{-1})+\frac{1}{2N_0} \sum _{Y_i=0} \log(1+r(X_i)), \tag{1}
\]</span>
where we assume we have data consisting of <span class="math inline">\(N_1+N_0\)</span> independent identically distributed observations <span class="math inline">\((X_i,\,Y_i)\)</span>, with binary <span class="math inline">\(Y\)</span>; <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_0\)</span> stand for the number of positive (<span class="math inline">\(Y=1\)</span>) and negative (<span class="math inline">\(Y=0\)</span>) cases; and <span class="math inline">\(r(X)\)</span> is a model for the likelihood ratio <span class="math inline">\(\Lambda(X) \equiv \frac{\text{Pr}(X\vert Y = 1)}{\text{Pr}(X\vert Y = 0)}\)</span>.</p>
<p>The main reason for writing this note was to understand a bit better what it means to optimize Eq. <a href="#eq:CLLR">(1)</a>, which does not look immediately obvious to me from its definition<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. In particular: is the population minimizer of Eq. <a href="#eq:CLLR">(1)</a> the actual likelihood ratio? And in what sense is a model with lower <span class="math inline">\(C_\text{LLR}\)</span> better than one with a correspondingly higher value?</p>
<p>The short answers to these questions are: yes; and: <span class="math inline">\(C_\text{LLR}\)</span> optimization seeks for the model with the best predictive performance in a Bayesian inference setting with uninformative prior on <span class="math inline">\(Y\)</span>, assuming that this prior actually reflects reality (<em>i.e.</em> <span class="math inline">\(\text{Pr}(Y=1) = \text{Pr}(Y=0) = \frac{1}{2}\)</span>). The mathematical details are given in the rest of the post.</p>
<!-- Strictly speaking, there are several aspects to what I have simply referred to as the "performance of a likelihood ratio model". First of all, there is the -->
<!-- left-over uncertainty on $Y$ after measuring $X$, which is an intrinsic property of the data and is independent of modeling. Second, $X$ may not correspond to raw data, but rather be the result of some data-processing/summary, which will in general reduce the amount of available information on $Y$. Finally, in the general case, the likelihood ratio $r(X)$ will not be an exact model, but only an approximation estimated from data. All this aspects get captured and mixed by Eq. \@ref(eq:CLLR), luckily in a way that can be actually decomposed (see below). -->
<h2 id="cross-entropy-with-random-weights">Cross-entropy with random weights</h2>
<p>We start with a mathematical digression, which will turn out useful for further developments. Let <span class="math inline">\(\{(X_i,\,Y_i)\}_{i=1,\,2,\,\dots,N}\)</span> be independent draws from a joint distribution, with binary <span class="math inline">\(Y_i \in \{0,\,1\}\)</span>. Given a function
<span class="math inline">\(w=w(\boldsymbol Y)\)</span> that is symmetric in its arguments<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, we define the random functional:</p>
<p><span class="math display" id="eq:WeightedLoss">\[
\mathcal L_N^w[f] = -\frac{1}{N}\sum_{i=1} ^N \left[w(\boldsymbol Y)Y_i \log(f(X_i))+ w({\boldsymbol Y}^c)( Y_i^c) \log(f(X_i)^c)\right],\tag{2}
\]</span>
where <span class="math inline">\(f=f(X)\)</span> is any function satisfying <span class="math inline">\(f(X)\in [0,\,1]\)</span> for all <span class="math inline">\(X\)</span>, and we let <span class="math inline">\(q^c = 1-q\)</span> for any number <span class="math inline">\(q \in [0,\,1]\)</span>. Notice that for <span class="math inline">\(w(\boldsymbol{Y}) \equiv 1\)</span>, this is just the usual cross-entropy loss.</p>
<p>We now look for the population minimizer of <a href="#eq:WeightedLoss">(2)</a>, <em>i.e.</em> the function <span class="math inline">\(f_*\)</span> that minimizes the functional <span class="math inline">\(f \mapsto \mathbb E(\mathcal L _N ^w [f])\)</span><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. Writing the expectation as:</p>
<p><span class="math display">\[
\mathbb E(\mathcal L _N ^w [f]) = -\frac{1}{N}\sum _{i=1} ^N \mathbb E\left[ \mathbb E(Y_i\cdot w(\boldsymbol Y)\vert X_i)\cdot \log(f(X_i))+E(Y_i^c\cdot w(\boldsymbol Y ^c)\vert X_i)\cdot \log(f^c(X_i))\right],
\]</span>
we can easily see that <span class="math inline">\(\mathbb E(\mathcal L _N ^w [f])\)</span> is a convex functional with a unique minimum given by:</p>
<p><span class="math display" id="eq:PopMinimizer">\[
f_*(X_i) = \frac{1}{1+r(X_i)^{-1}},\quad r_*(X_i) = \dfrac{E(Y_i\cdot w(\boldsymbol Y)\vert X_i)}{E(Y_i^c\cdot w(\boldsymbol Y^c)\vert X_i)}.\tag{3}
\]</span>
The corresponding expected loss is:</p>
<p><span class="math display">\[
\mathbb E(\mathcal L _N ^w [f_*]) = \mathbb E\left[ \mathbb E(Y_i\cdot w(\boldsymbol Y) + Y_i^c\cdot w(\boldsymbol Y^c)\vert X_i)\cdot \mathcal H(f_*(X_i))\right],
\]</span>
where <span class="math inline">\(\mathcal H(p) = -p \log (p) -(1-p) \log(1-p)\)</span> is the entropy of a binary random variable <span class="math inline">\(Z\)</span> with probability <span class="math inline">\(p = \text{Pr}(Z=1)\)</span> (the index <span class="math inline">\(i\)</span> in the previous expression can be any index, since data points are assumed to be identically distributed).</p>
<p>Before looking at values of <span class="math inline">\(f\)</span> other than <span class="math inline">\(f_*\)</span>, we observe that the previous expectation can be succintly expressed as:</p>
<p><span class="math display">\[
\mathbb E(\mathcal L _N ^w [f_*]) = k \cdot H^\prime(Y\vert X),
\]</span>
where
<span class="math display" id="eq:DefKappa">\[
k = \mathbb E(Y_i\cdot w(\boldsymbol Y) + Y_i^c\cdot w(\boldsymbol Y^c))\tag{4}
\]</span>
and <span class="math inline">\(H&#39;(Y\vert X)\)</span> is the conditional entropy of <span class="math inline">\(Y\vert X\)</span> with respect to a <em>different</em> probability measure <span class="math inline">\(\text{Pr}^\prime\)</span>, defined by:</p>
<p><span class="math display" id="eq:DefPrPrime">\[
\text{Pr}^\prime(E) = t \cdot \text {Pr}(E \vert Y = 1) + (1-t)\cdot \text {Pr}(E \vert Y = 0), \tag{5}
\]</span>
where <span class="math inline">\(t=\text{Pr}^\prime(Y=1)\in [0,\,1]\)</span> is fixed by the requirement<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>:</p>
<p><span class="math display" id="eq:DefPrPrime2">\[
\dfrac{\text {Pr}^\prime (Y=1)}{\text{Pr}^\prime (Y=0)}=\dfrac{\text {Pr} (Y=1)}{\text{Pr} (Y=0)}\cdot\dfrac{\mathbb E(w(\boldsymbol Y)\vert \sum _i Y_i &gt;0)}{\mathbb E(w(\boldsymbol Y^c)\vert \sum _i Y_i^c &gt;0)}. \tag{6}
\]</span>
In terms of <span class="math inline">\(\text{Pr}^\prime\)</span>, the population minimizers <span class="math inline">\(f_*\)</span> and <span class="math inline">\(r_*\)</span> in Eq. <a href="#eq:PopMinimizer">(3)</a> can be simply expressed as:</p>
<p><span class="math display" id="eq:PopMinimizer2">\[
r_*(X)=\dfrac{\text {Pr}^\prime(Y=1\vert X)}{\text {Pr}^\prime(Y=0\vert X)},\qquad f_*(X)=\text {Pr}^\prime(Y=1\vert X). \tag{7}
\]</span>
If now <span class="math inline">\(f\)</span> is an arbitrary function, we have:</p>
<p><span class="math display">\[
\begin{split}
\mathbb E(\mathcal L _N ^w [f]) - \mathbb E(\mathcal L _N ^w [f_*]) &amp;= \mathbb E\left[ \mathbb E(Y_i\cdot w(\boldsymbol Y) + Y_i^c\cdot w(\boldsymbol Y^c)\vert X_i)\cdot \mathcal D(f_*(X_i)\vert \vert f(X_i))\right]
&amp;= k\cdot D(\text{Pr}^\prime\vert \vert \text{Pr}^\prime _f)
\end{split}
\]</span>
where <span class="math inline">\(\mathcal D(p\vert \vert q) = p \log (\frac{p}{q}) + (1-p) \log (\frac{1-p}{1-q})\)</span>, and <span class="math inline">\(D(\text{Pr}^\prime\vert \vert \text{Pr}^\prime _f)\)</span> is the Kullback-Liebler divergence between the measure <span class="math inline">\(\text{Pr}^\prime\)</span> and the measure <span class="math inline">\(\text{Pr}^\prime _f\)</span> defined by:</p>
<p><span class="math display">\[
\text{Pr}^\prime _f(Y = 1\vert X)=f(X),\qquad \text{Pr}^\prime _f(X)=\text{Pr}^\prime(X)
\]</span>
(notice that <span class="math inline">\(\text {Pr} ^{\prime} _{f_*} \equiv \text{Pr} ^{\prime}\)</span> by definition).
Finally, suppose that <span class="math inline">\(X = g(\widetilde X)\)</span> for some random variable <span class="math inline">\(\widetilde X\)</span>, and define the corresponding functional:</p>
<p><span class="math display">\[
\widetilde{\mathcal L} _N^w[\widetilde f]  = -\frac{1}{N}\sum_{i=1} ^N \left[w(\boldsymbol Y)Y_i \log(\widetilde f(\widetilde X))+ w({\boldsymbol Y}^c)( Y_i^c) \log(\widetilde f(\widetilde X)^c)\right].
\]</span>
Then <span class="math inline">\(\mathcal L _N ^w [f] = \widetilde{\mathcal L} _N^w[f \circ g]\)</span>. If <span class="math inline">\(\widetilde f _* =\)</span> is the population minimizer of <span class="math inline">\(\widetilde{\mathcal L} _N^w\)</span>, it follows that <span class="math inline">\(\mathbb E (\widetilde{\mathcal L} _N^w[\widetilde f _*]) \leq \mathbb E(\mathcal L _N ^w [f_*])\)</span>.</p>
<p>Putting everything together, we can decompose the expected loss for a function <span class="math inline">\(f=f(X)\)</span>, where <span class="math inline">\(X= g(\widetilde X)\)</span>, in the following suggestive way:</p>
<p><span class="math display" id="eq:DecompositionWeightedLoss">\[
\begin{split}
\mathbb E(\mathcal L _N ^w [f]) &amp;= (L_N ^w)_\text{min}+(L_N ^w)_\text{proc} +(L_N ^w)_\text{missp},\\
(L_N ^w)_\text{min}&amp;\equiv\mathbb E(\widetilde{\mathcal L} _N^w[{\widetilde f} _*])  \\ &amp;=
\mathbb E\left[ \mathbb E(Y_i\cdot w(\boldsymbol Y) + Y_i^c\cdot w(\boldsymbol Y^c)\vert \widetilde X _i)\cdot \mathcal H({\widetilde f} _*(\widetilde X _i))\right]\\
&amp;=k\cdot H^\prime(Y\vert \widetilde X),\\
(L_N ^w)_\text{proc}&amp;\equiv\mathbb E(\mathcal L _N ^w [f_*]-\widetilde{\mathcal L} _N^w[\phi_*])  \\&amp; =
\mathbb E\left[ \mathbb E(Y_i\cdot w(\boldsymbol Y) + Y_i^c\cdot w(\boldsymbol Y^c)\vert X_i)\cdot  \mathcal H(f_*(X_i))
\right]- (L_N ^w)_\text{min}\\
&amp; = k\cdot I^\prime(Y; \widetilde X\vert X),\\
(L_N ^w)_\text{missp} &amp; \equiv \mathbb E(\mathcal L _N ^w [f]) - \mathbb E(\mathcal L _N ^w [f_*]) \\&amp;= \mathbb E\left[ \mathbb E(Y_i\cdot w(\boldsymbol Y) + Y_i^c\cdot w(\boldsymbol Y^c)\vert X_i)\cdot  \mathcal  D(f_*(X_i)\vert \vert f(X_i))\right]\\ &amp;=k\cdot D(\text {Pr}^\prime\vert \vert \text {Pr}^\prime _f),
\end{split} \tag{8}
\]</span>
where <span class="math inline">\(k\)</span> is defined in Eq. <a href="#eq:DefKappa">(4)</a>. In the equation for <span class="math inline">\((L^w _N)_\text{proc}\)</span> we introduced the conditional mutual information (with respect to the measure <span class="math inline">\(\text{Pr}^\prime\)</span>), that satisfies <span class="citation" data-cites="Cover2006">(<a href="#ref-Cover2006" role="doc-biblioref">Cover and Thomas 2006</a>)</span>:</p>
<p><span class="math display">\[
I(\widetilde X;Y\vert X) = I(\widetilde X,Y)-I(X,Y) = H(Y\vert X)-H(Y\vert \widetilde X).
\]</span></p>
<p>The three components in Eq. <a href="#eq:DecompositionWeightedLoss">(8)</a> can be interpreted as follows: <span class="math inline">\((L_N ^w)_\text{min}\)</span> represents the minimum expected loss achievable, given the data available <span class="math inline">\(\widetilde X\)</span>; <span class="math inline">\((L_N ^w)_\text{proc}\)</span> accounts for the information lost in the processing transformation <span class="math inline">\(X=g(\widetilde X)\)</span>; finally <span class="math inline">\((L_N ^w)_\text{missp}\)</span> is due to misspecification, <em>i.e.</em> the fact that the model <span class="math inline">\(f(X)\)</span> for the true posterior probability <span class="math inline">\(f_*(X)\)</span> is an approximation.</p>
<p>All the information-theoretic quantities (and their corresponding operative interpretations hinted in the previous paragraph) make reference to the measure <span class="math inline">\(\text{Pr}^\prime\)</span> defined by Eqs. <a href="#eq:DefPrPrime">(5)</a> and <a href="#eq:DefPrPrime2">(6)</a>. This is merely the result of altering the proportion of positive (<span class="math inline">\(Y=1\)</span>) and negative (<span class="math inline">\(Y=0\)</span>) examples in the <span class="math inline">\(X\)</span>-<span class="math inline">\(Y\)</span> joint distribution by a factor dictated by the weight function <span class="math inline">\(w\)</span> - while keeping conditional distributions such as <span class="math inline">\(X\vert Y\)</span> unchanged.</p>
<h2 id="a-familiar-case-cross-entropy-loss">A familiar case: cross-entropy loss</h2>
<p>For <span class="math inline">\(w(\boldsymbol {Y}) = 1\)</span>, the functional <span class="math inline">\(\mathcal {L} _{N} ^{w}[f]\)</span>
coincides with the usual cross-entropy loss<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>:</p>
<p><span class="math display" id="eq:CrossEntropyLoss">\[
\text{CE}[f] = -\frac{1}{N}\sum_{i=1} ^N \left[Y_i \log(f(X_i))+ (1-Y_i) \log(1-f(X_i))\right].\tag{9}
\]</span></p>
<p>From Eq. <a href="#eq:DefPrPrime2">(6)</a> we see that the measure <span class="math inline">\(\text{Pr}^{\prime}\)</span>
coincides with the original <span class="math inline">\(\text{Pr}\)</span>, so that by Eq. <a href="#eq:PopMinimizer">(3)</a>
the population minimizer of <a href="#eq:CrossEntropyLoss">(9)</a> is
<span class="math inline">\(f_{*}(X) = \text{Pr}(Y=1\vert X)\)</span> (independently of sample size). Since <span class="math inline">\(k = 1\)</span> (<em>cf.</em> Eq. <a href="#eq:DefKappa">(4)</a>), the decomposition <a href="#eq:DecompositionWeightedLoss">(8)</a> reads:</p>
<p><span class="math display" id="eq:DecompositionCE">\[
\begin{split}
\mathbb E(\text{CE} [f]) &amp;= (\text{CE})_\text{min}+(\text{CE})_\text{proc} +(\text{CE})_\text{missp},\\
(\text{CE})_\text{min}&amp;=H(Y\vert \widetilde X),\\
(\text{CE})_\text{proc}&amp;= I(Y; \widetilde X\vert X),\\
(\text{CE})_\text{missp} &amp;=D(\text {Pr}\vert \vert \text {Pr} _{f}),
\end{split} \tag{10}
\]</span></p>
<p>where conditional entropy <span class="math inline">\(H\)</span>, mutual information <span class="math inline">\(I\)</span> and relative entropy <span class="math inline">\(D\)</span> now simply refer to the original measure <span class="math inline">\(\text{Pr}\)</span>.</p>
<h2 id="the-likelihood-ratio-cost">The Likelihood Ratio Cost</h2>
<p>The quantity <span class="math inline">\(C_{\text{LLR}}\)</span> defined in Eq. <a href="#eq:CLLR">(1)</a> can be put in the general form <a href="#eq:WeightedLoss">(2)</a>, if we let <span class="math inline">\(f(X) = (1+r(X)^{-1})^{-1}\)</span> and<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>:</p>
<p><span class="math display">\[
w(\boldsymbol Y) = \left(\dfrac{2}{N}\sum _{i = 1}^{N}Y_j \right)^{-1}
\]</span>
In what follows, I will consider a slight modification of the usual
<span class="math inline">\(C_\text{LLR}\)</span>, defined by the weight function:</p>
<p><span class="math display">\[
w(\boldsymbol Y) = \dfrac{1}{2(N-1)}\sum _{i = 1}^{N}(1-Y_j).
\]</span>
This yields Eq. <a href="#eq:CLLR">(1)</a> multiplied by <span class="math inline">\(\dfrac{N_1N_0}{N(N-1)}\)</span>, which I will keep denoting as <span class="math inline">\(C_\text{LLR}\)</span>, with a slight abuse of notation.</p>
<p>We can easily compute<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>:</p>
<p><span class="math display" id="eq:PriorCLLR">\[
\dfrac{\text {Pr}^\prime (Y=1)}{\text{Pr}^\prime (Y=0)}=1, \tag{11}
\]</span>
so that, by Eq. <a href="#eq:PopMinimizer">(3)</a>, the population minimizer of
<span class="math inline">\(C_\text{LLR}\)</span> is:</p>
<p><span class="math display">\[
r_*(X) = \Lambda (X),\quad f_*(X)=\dfrac{1}{1+\Lambda(X)^{-1}},
\]</span></p>
<p>where <span class="math inline">\(\Lambda(X)\)</span> denotes the <em>likelihood-ratio</em> of <span class="math inline">\(X\)</span>, schematically:</p>
<p><span class="math display">\[
\Lambda(X)\equiv \dfrac{\text{Pr}(X\vert Y = 1)}{\text{Pr}(X\vert Y = 0)}.
\]</span></p>
<p>The constant <span class="math inline">\(k\)</span> in Eq. <a href="#eq:DefKappa">(4)</a> is:</p>
<p><span class="math display">\[
k = \text{Pr}(Y = 1)\text{Pr}(Y = 0)=\text{Var}(Y)
\]</span></p>
<p>The general decomposition <a href="#eq:DecompositionWeightedLoss">(8)</a> becomes:
<span class="math display" id="eq:DecompositionCE">\[
\begin{split}
\mathbb E(C_\text{LLR} [f]) &amp;= (C_\text{LLR})_\text{min}+(C_\text{LLR})_\text{proc} +(C_\text{LLR})_\text{missp},\\
(C_\text{LLR})_\text{min}&amp;=\text{Var}(Y)\cdot H^{\prime}(Y\vert \widetilde X),\\
(C_\text{LLR})_\text{proc}&amp;= \text{Var}(Y)\cdot I^{\prime}(Y; \widetilde X\vert X),\\
(C_\text{LLR})_\text{missp} &amp;=\text{Var}(Y)\cdot D^{\prime}(\text {Pr}\vert \vert \text {Pr} _{f}),
\end{split} \tag{10}
\]</span></p>
<p>where <span class="math inline">\(\text{Pr}^\prime\)</span> is now given by <a href="#eq:PriorCLLR">(11)</a>.</p>
<h2 id="discussion">Discussion</h2>
<p>The table below provides a comparison between cross-entropy and likelihood-ratio cost,
summarizing the results from previous sections.</p>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 38%" />
<col style="width: 46%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Cross-entropy</th>
<th>Likelihood Ratio Cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(f_*(X)\)</span></td>
<td><span class="math inline">\(\text{Pr}(Y = 1\vert X)\)</span></td>
<td><span class="math inline">\((1+\Lambda(X)^{-1})^{-1}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(r_*(X)\)</span>`</td>
<td>Posterior odds ratio</td>
<td>Likelihood ratio</td>
</tr>
<tr class="odd">
<td>Minimum Loss</td>
<td><span class="math inline">\(H(Y\vert \widetilde X)\)</span></td>
<td><span class="math inline">\(\text{Var}(Y) \cdot H^\prime(Y\vert \widetilde X)\)</span></td>
</tr>
<tr class="even">
<td>Processing Loss</td>
<td><span class="math inline">\(I(Y; \widetilde X\vert X)\)</span></td>
<td><span class="math inline">\(\text{Var}(Y) \cdot I^\prime(Y; \widetilde X\vert X)\)</span></td>
</tr>
<tr class="odd">
<td>Misspecification Loss</td>
<td><span class="math inline">\(D(f_*\vert\vert f)\)</span></td>
<td><span class="math inline">\(\text{Var}(Y) \cdot D^\prime(f_*\vert\vert f)\)</span></td>
</tr>
<tr class="even">
<td>Reference measure</td>
<td><span class="math inline">\(\text{Pr}\)</span></td>
<td><span class="math inline">\(\text{Pr}^{\prime} = \frac{\text{Pr}(\cdot \vert Y = 1)+\text{Pr}(\cdot \vert Y = 0)}{2}\)</span></td>
</tr>
</tbody>
</table>
<p>The objective of <span class="math inline">\(C_\text{LLR}\)</span> is found to be the likelihood ratio, as
terminology suggests. The interpretation of model selection according to
<span class="math inline">\(C_\text{LLR}\)</span> minimization turns out to be slightly more involved, compared to
cross-entropy, which we first review.</p>
<p>Suppose we are given a set of predictive models <span class="math inline">\(\{\mathcal M_i\}_{i\in I}\)</span>,
each of which consists of a processing transformation, <span class="math inline">\(\widetilde X \mapsto X\)</span>,
and an estimate of the posterior probability <span class="math inline">\(\text{Pr}(Y = 1\vert X)\)</span>.
When the sample size <span class="math inline">\(N \to \infty\)</span>, cross-entropy minimization will almost
certainly select the model that minimizes
<span class="math inline">\(I(Y; \widetilde X\vert X) + D(f_*\vert \vert f)\)</span>. Following standard
Information Theory arguments, we can interpret this model as the statistically
optimal compression algorithm for <span class="math inline">\(Y\)</span>, assuming <span class="math inline">\(X\)</span> to be available at both
the encoding and decoding ends.</p>
<p>The previous argument carries over <em>mutatis mutandi</em> to <span class="math inline">\(C_\text{LLR}\)</span>
minimization, with an important qualification: optimal average compression is
now achieved for data distributed according to a different probability measure
<span class="math inline">\(\text{Pr}&#39;(\cdot) = \frac{1}{2}\text {Pr}(\cdot\vert Y = 1) + \frac{1}{2}\text {Pr}(\cdot\vert Y = 0)\)</span>. In particular, according to <span class="math inline">\(\text{Pr}&#39;\)</span>,
the likelihood ratio coincides with the posterior odds ratio, and
<span class="math inline">\((1+\Lambda(X)^{-1})^{-1}\)</span> coincides with posterior probability, which clarifies
why we can measure differences from the true likelihood-ratio through the
Kullback-Liebler divergence.</p>
<p>The measure <span class="math inline">\(\text{Pr}&#39;\)</span> is not just an abstruse mathematical construct:
it is the result of balanced sampling from the original distribution, <em>i.e.</em>
taking an equal number of positive and negative cases<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>. If the <span class="math inline">\((X,\,Y)\)</span> distribution is already balanced,
either by design or because of some underlying symmetry in the data generating
process, our analysis implies that likelihood-ratio cost and cross-entropy
minimization are essentially equivalent for <span class="math inline">\(N\to \infty\)</span>. In general, with
<span class="math inline">\(\text{Pr} (Y=1) \neq \text{Pr} (Y=0)\)</span>, this is not the case<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>.</p>
<p>The fact that <span class="math inline">\(C_\text{LLR}\)</span> seeks for optimal predictors according to the balanced measure <span class="math inline">\(\text{Pr}&#39;\)</span> is, one could argue, not completely crazy from the point of view of forensic science, where “<span class="math inline">\(Y\in\{0,1\}\)</span>” often stands for a sort verdict (guilty <em>vs.</em> not guilty, say). Indeed, optimizing
with respect to <span class="math inline">\(\text{Pr}^\prime\)</span> means that our predictions are designed to be optimal in a world in which the verdict could be <em>a priori</em> <span class="math inline">\(Y=0\)</span> or <span class="math inline">\(Y=1\)</span> with equal probability - which is what an unbiased trier-of-fact should ideally assume. Minimizing <span class="math inline">\(C_\text{LLR}\)</span>, we guard ourselves against any bias
that may be implicit in the training dataset, extraneous to the <span class="math inline">\(X\)</span>-<span class="math inline">\(Y\)</span>
relation and not explicitly modeled, a feature that may be regarded as desirable from a legal standpoint.</p>
<h2 id="simulated-example">Simulated example</h2>
<p>In general, the posterior odd ratio and likelihood ratio differ only by a
constant, so it is reasonable to try to fit the same functional form to both of
them. Let us illustrate with a simulated example of this type the differences
between cross-entropy and <span class="math inline">\(C_{\text{LLR}}\)</span> optimization mentioned in the
previous Section.</p>
<p>Suppose that <span class="math inline">\(X \in \mathbb R\)</span> has conditional density:
<span class="math display">\[
\phi(X\vert Y) = (2\pi\sigma _Y^2)^{-\frac{1}{2}} \exp(-\frac{(X-\mu_Y)^2}{2\sigma _Y^2})
\]</span>
and <span class="math inline">\(Y\)</span> has marginal probability <span class="math inline">\(\text{Pr}(Y = 1) = \pi\)</span>. The true likelihood-ratio and posterior odds ratio are respectively given by:</p>
<p><span class="math display">\[
\begin{split}
\Lambda (X) &amp;
    \equiv \frac{\phi(X\vert Y=1)}{\phi(X\vert Y=0)}
    = e ^ {a X^2 + bX +c},\\
\rho (X) &amp;
    \equiv \frac{\text{Pr}(Y = 1\vert X)}{\text{Pr}(Y = 0\vert X)}
    = e ^ {a X ^ 2 + bX +c+d},
\end{split}
\]</span>
where we have defined:</p>
<p><span class="math display">\[
a  \equiv \dfrac{\sigma _1 ^2 -\sigma_0 ^2}{2\sigma _0 ^2\sigma_1 ^2},\quad
b  \equiv \mu _1 - \mu _0, \quad
c  \equiv \dfrac{\mu_0^2}{2\sigma_0^2} -\dfrac{\mu_1 ^2}{2\sigma _1^2}+\ln(\frac{\sigma _0 }{\sigma _1 }),\quad
d  \equiv \ln (\frac {\pi}{1-\pi}) .
\]</span></p>
<p>Suppose that we fit an exponential function <span class="math inline">\(r(X)=e^{mX +q}\)</span> to <span class="math inline">\(\Lambda(X)\)</span> by
likelihood-ratio cost minimization, and similarly <span class="math inline">\(r&#39;(X)=e^{m&#39;X+q&#39;}\)</span> to
<span class="math inline">\(\rho(X)\)</span> by cross-entropy minimization<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>. Due to the previous discussion, one could reasonably expect the results of the two procedure to differ in some way, which is demonstrated below by simulation.</p>
<p>The chunk of R code below defines the function and data used for the simulation.
In particular, I’m considering a heavily unbalanced case
(<span class="math inline">\(\text{Pr}(Y = 1) = 0.1\%\)</span>) in which negative cases give rise to a sharply
localized <span class="math inline">\(X\)</span> peak around <span class="math inline">\(X=0\)</span> (<span class="math inline">\(\mu _0 = 0\)</span>, <span class="math inline">\(\sigma_0 = .25\)</span>),
while the few positive cases give rise to a broader signal centered at <span class="math inline">\(X=1\)</span>
(<span class="math inline">\(\mu _1 = 1\)</span>, <span class="math inline">\(\sigma _1 = 1\)</span>).</p>
<div class="layout-chunk" data-layout="l-body">
<details>
<summary>
Show code
</summary>
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='co'># Tidyverse facilities for plotting</span></span>
<span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://dplyr.tidyverse.org'>dplyr</a></span><span class='op'>)</span></span>
<span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://ggplot2.tidyverse.org'>ggplot2</a></span><span class='op'>)</span> </span>
<span></span>
<span><span class='co'># Loss functions</span></span>
<span><span class='va'>weighted_loss</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>par</span>, <span class='va'>data</span>, <span class='va'>w</span><span class='op'>)</span> <span class='op'>{</span></span>
<span>  <span class='va'>m</span> <span class='op'>&lt;-</span> <span class='va'>par</span><span class='op'>[[</span><span class='fl'>1</span><span class='op'>]</span><span class='op'>]</span></span>
<span>  <span class='va'>q</span> <span class='op'>&lt;-</span> <span class='va'>par</span><span class='op'>[[</span><span class='fl'>2</span><span class='op'>]</span><span class='op'>]</span></span>
<span>  <span class='va'>x</span> <span class='op'>&lt;-</span> <span class='va'>data</span><span class='op'>$</span><span class='va'>x</span></span>
<span>  <span class='va'>y</span> <span class='op'>&lt;-</span> <span class='va'>data</span><span class='op'>$</span><span class='va'>y</span></span>
<span>  </span>
<span>  <span class='va'>z</span> <span class='op'>&lt;-</span> <span class='va'>m</span> <span class='op'>*</span> <span class='va'>x</span> <span class='op'>+</span> <span class='va'>q</span></span>
<span>  <span class='va'>p</span> <span class='op'>&lt;-</span> <span class='fl'>1</span> <span class='op'>/</span> <span class='op'>(</span><span class='fl'>1</span> <span class='op'>+</span> <span class='fu'><a href='https://rdrr.io/r/base/Log.html'>exp</a></span><span class='op'>(</span><span class='op'>-</span><span class='va'>z</span><span class='op'>)</span><span class='op'>)</span></span>
<span>  </span>
<span>  <span class='op'>-</span><span class='fu'><a href='https://rdrr.io/r/base/mean.html'>mean</a></span><span class='op'>(</span><span class='va'>y</span> <span class='op'>*</span> <span class='fu'>w</span><span class='op'>(</span><span class='va'>y</span><span class='op'>)</span> <span class='op'>*</span> <span class='fu'><a href='https://rdrr.io/r/base/Log.html'>log</a></span><span class='op'>(</span><span class='va'>p</span><span class='op'>)</span> <span class='op'>+</span> <span class='op'>(</span><span class='fl'>1</span><span class='op'>-</span><span class='va'>y</span><span class='op'>)</span> <span class='op'>*</span> <span class='fu'>w</span><span class='op'>(</span><span class='fl'>1</span><span class='op'>-</span><span class='va'>y</span><span class='op'>)</span> <span class='op'>*</span> <span class='fu'><a href='https://rdrr.io/r/base/Log.html'>log</a></span><span class='op'>(</span><span class='fl'>1</span><span class='op'>-</span><span class='va'>p</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='op'>}</span></span>
<span></span>
<span><span class='va'>cross_entropy</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>par</span>, <span class='va'>data</span><span class='op'>)</span> </span>
<span>  <span class='fu'>weighted_loss</span><span class='op'>(</span><span class='va'>par</span>, <span class='va'>data</span>, w <span class='op'>=</span> \<span class='op'>(</span><span class='va'>y</span><span class='op'>)</span> <span class='fl'>1</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>cllr</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>par</span>, <span class='va'>data</span><span class='op'>)</span> </span>
<span>  <span class='fu'>weighted_loss</span><span class='op'>(</span><span class='va'>par</span>, <span class='va'>data</span>, w <span class='op'>=</span> \<span class='op'>(</span><span class='va'>y</span><span class='op'>)</span> <span class='fu'><a href='https://rdrr.io/r/base/mean.html'>mean</a></span><span class='op'>(</span><span class='fl'>1</span><span class='op'>-</span><span class='va'>y</span><span class='op'>)</span><span class='op'>)</span></span>
<span></span>
<span></span>
<span><span class='co'># Data generating process</span></span>
<span><span class='va'>rxy</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>n</span>, <span class='va'>pi</span> <span class='op'>=</span> <span class='fl'>.001</span>, <span class='va'>mu1</span> <span class='op'>=</span> <span class='fl'>1</span>, <span class='va'>mu0</span> <span class='op'>=</span> <span class='fl'>0</span>, <span class='va'>sd1</span> <span class='op'>=</span> <span class='fl'>1</span>, <span class='va'>sd0</span> <span class='op'>=</span> <span class='fl'>0.25</span><span class='op'>)</span> <span class='op'>{</span> </span>
<span>  <span class='va'>y</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/stats/Uniform.html'>runif</a></span><span class='op'>(</span><span class='va'>n</span><span class='op'>)</span> <span class='op'>&lt;</span> <span class='va'>pi</span></span>
<span>  <span class='va'>x</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/stats/Normal.html'>rnorm</a></span><span class='op'>(</span><span class='va'>n</span>, mean <span class='op'>=</span> <span class='va'>y</span> <span class='op'>*</span> <span class='va'>mu1</span> <span class='op'>+</span> <span class='op'>(</span><span class='fl'>1</span><span class='op'>-</span><span class='va'>y</span><span class='op'>)</span> <span class='op'>*</span> <span class='va'>mu0</span>, sd <span class='op'>=</span> <span class='va'>y</span> <span class='op'>*</span> <span class='va'>sd1</span> <span class='op'>+</span> <span class='op'>(</span><span class='fl'>1</span><span class='op'>-</span><span class='va'>y</span><span class='op'>)</span> <span class='op'>*</span> <span class='va'>sd0</span><span class='op'>)</span></span>
<span>  <span class='fu'><a href='https://rdrr.io/r/base/data.frame.html'>data.frame</a></span><span class='op'>(</span>x <span class='op'>=</span> <span class='va'>x</span>, y <span class='op'>=</span> <span class='va'>y</span><span class='op'>)</span></span>
<span><span class='op'>}</span></span>
<span><span class='va'>pi</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/formals.html'>formals</a></span><span class='op'>(</span><span class='va'>rxy</span><span class='op'>)</span><span class='op'>$</span><span class='va'>pi</span></span>
<span></span>
<span></span>
<span><span class='co'># Simulation</span></span>
<span><span class='fu'><a href='https://rdrr.io/r/base/Random.html'>set.seed</a></span><span class='op'>(</span><span class='fl'>840</span><span class='op'>)</span></span>
<span><span class='va'>data</span> <span class='op'>&lt;-</span> <span class='fu'>rxy</span><span class='op'>(</span>n <span class='op'>=</span> <span class='fl'>1e6</span><span class='op'>)</span></span>
<span><span class='va'>par_cllr</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/stats/optim.html'>optim</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>,<span class='fl'>0</span><span class='op'>)</span>, <span class='va'>cllr</span>, data <span class='op'>=</span> <span class='va'>data</span><span class='op'>)</span><span class='op'>$</span><span class='va'>par</span></span>
<span><span class='va'>par_cross_entropy</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/stats/optim.html'>optim</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>,<span class='fl'>0</span><span class='op'>)</span>, <span class='va'>cross_entropy</span>, data <span class='op'>=</span> <span class='va'>data</span><span class='op'>)</span><span class='op'>$</span><span class='va'>par</span></span>
<span><span class='va'>par_cross_entropy</span><span class='op'>[</span><span class='fl'>2</span><span class='op'>]</span> <span class='op'>&lt;-</span> <span class='va'>par_cross_entropy</span><span class='op'>[</span><span class='fl'>2</span><span class='op'>]</span> <span class='op'>-</span> <span class='fu'><a href='https://rdrr.io/r/base/Log.html'>log</a></span><span class='op'>(</span><span class='va'>pi</span> <span class='op'>/</span> <span class='op'>(</span><span class='fl'>1</span><span class='op'>-</span><span class='va'>pi</span><span class='op'>)</span><span class='op'>)</span></span>
<span></span>
<span></span>
<span><span class='co'># Helpers to extract LLRs from models</span></span>
<span><span class='va'>llr</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>x</span>, <span class='va'>par</span><span class='op'>)</span></span>
<span>  <span class='va'>par</span><span class='op'>[</span><span class='fl'>1</span><span class='op'>]</span> <span class='op'>*</span> <span class='va'>x</span> <span class='op'>+</span> <span class='va'>par</span><span class='op'>[</span><span class='fl'>2</span><span class='op'>]</span> </span>
<span></span>
<span><span class='va'>llr_true</span> <span class='op'>&lt;-</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span> <span class='op'>{</span></span>
<span>  <span class='va'>mu1</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/formals.html'>formals</a></span><span class='op'>(</span><span class='va'>rxy</span><span class='op'>)</span><span class='op'>$</span><span class='va'>mu1</span> </span>
<span>  <span class='va'>mu0</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/formals.html'>formals</a></span><span class='op'>(</span><span class='va'>rxy</span><span class='op'>)</span><span class='op'>$</span><span class='va'>mu0</span> </span>
<span>  <span class='va'>sd1</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/formals.html'>formals</a></span><span class='op'>(</span><span class='va'>rxy</span><span class='op'>)</span><span class='op'>$</span><span class='va'>sd1</span></span>
<span>  <span class='va'>sd0</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/formals.html'>formals</a></span><span class='op'>(</span><span class='va'>rxy</span><span class='op'>)</span><span class='op'>$</span><span class='va'>sd0</span></span>
<span>    </span>
<span>  <span class='va'>a</span> <span class='op'>&lt;-</span> <span class='fl'>0.5</span> <span class='op'>*</span> <span class='op'>(</span><span class='va'>sd1</span> <span class='op'>^</span><span class='fl'>2</span> <span class='op'>-</span> <span class='va'>sd0</span> <span class='op'>^</span><span class='fl'>2</span><span class='op'>)</span> <span class='op'>/</span> <span class='op'>(</span><span class='va'>sd1</span> <span class='op'>^</span><span class='fl'>2</span> <span class='op'>*</span> <span class='va'>sd0</span> <span class='op'>^</span><span class='fl'>2</span><span class='op'>)</span></span>
<span>  <span class='va'>b</span> <span class='op'>&lt;-</span> <span class='va'>mu1</span> <span class='op'>/</span> <span class='op'>(</span><span class='va'>sd1</span><span class='op'>^</span><span class='fl'>2</span><span class='op'>)</span> <span class='op'>-</span> <span class='va'>mu0</span> <span class='op'>/</span> <span class='op'>(</span><span class='va'>sd0</span><span class='op'>^</span><span class='fl'>2</span><span class='op'>)</span></span>
<span>  <span class='va'>c</span> <span class='op'>&lt;-</span> <span class='fl'>0.5</span> <span class='op'>*</span> <span class='op'>(</span><span class='va'>mu0</span><span class='op'>^</span><span class='fl'>2</span> <span class='op'>/</span> <span class='op'>(</span><span class='va'>sd0</span><span class='op'>^</span><span class='fl'>2</span><span class='op'>)</span> <span class='op'>-</span> <span class='va'>mu1</span><span class='op'>^</span><span class='fl'>2</span> <span class='op'>/</span> <span class='op'>(</span><span class='va'>sd1</span><span class='op'>^</span><span class='fl'>2</span><span class='op'>)</span><span class='op'>)</span> <span class='op'>+</span> <span class='fu'><a href='https://rdrr.io/r/base/Log.html'>log</a></span><span class='op'>(</span><span class='va'>sd0</span> <span class='op'>/</span> <span class='va'>sd1</span><span class='op'>)</span></span>
<span>  <span class='va'>a</span> <span class='op'>*</span> <span class='va'>x</span> <span class='op'>*</span> <span class='va'>x</span> <span class='op'>+</span> <span class='va'>b</span> <span class='op'>*</span> <span class='va'>x</span> <span class='op'>+</span> <span class='va'>c</span></span>
<span><span class='op'>}</span></span></code></pre>
</div>
</details>
</div>
<p>So, what do our best estimates look like? The plot below shows the best fit
lines for the log-likelihood ratio from <span class="math inline">\(C_{\text{LLR}}\)</span> minimization (in solid red) and cross-entropy minimization (in solid blue). The true log-likelihood ratio parabola is the black line. Also shown are the <span class="math inline">\(\text{LLR}=0\)</span> line (in dashed red) and the <span class="math inline">\(\text{LLR}=\ln(\frac{1-\pi}{\pi})\)</span> (in
dashed blue), which are the appropriate Bayes thresholds for classifying a
data point as positive (<span class="math inline">\(Y=1\)</span>), assuming data comes from a balanced and unbalanced distribution, respectively.</p>
<div class="layout-chunk" data-layout="l-body">
<details>
<summary>
Show code
</summary>
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'><a href='https://ggplot2.tidyverse.org/reference/ggplot.html'>ggplot</a></span><span class='op'>(</span><span class='op'>)</span> <span class='op'>+</span> </span>
<span>  <span class='fu'><a href='https://ggplot2.tidyverse.org/reference/geom_function.html'>geom_function</a></span><span class='op'>(</span>fun <span class='op'>=</span> \<span class='op'>(</span><span class='va'>x</span><span class='op'>)</span> <span class='fu'>llr</span><span class='op'>(</span><span class='va'>x</span>, <span class='va'>par_cllr</span><span class='op'>)</span>, color <span class='op'>=</span> <span class='st'>"red"</span><span class='op'>)</span> <span class='op'>+</span> </span>
<span>  <span class='fu'><a href='https://ggplot2.tidyverse.org/reference/geom_function.html'>geom_function</a></span><span class='op'>(</span>fun <span class='op'>=</span> \<span class='op'>(</span><span class='va'>x</span><span class='op'>)</span> <span class='fu'>llr</span><span class='op'>(</span><span class='va'>x</span>, <span class='va'>par_cross_entropy</span><span class='op'>)</span>, color <span class='op'>=</span> <span class='st'>"blue"</span><span class='op'>)</span> <span class='op'>+</span></span>
<span>  <span class='fu'><a href='https://ggplot2.tidyverse.org/reference/geom_function.html'>geom_function</a></span><span class='op'>(</span>fun <span class='op'>=</span> \<span class='op'>(</span><span class='va'>x</span><span class='op'>)</span> <span class='fu'>llr_true</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span>, color <span class='op'>=</span> <span class='st'>"black"</span><span class='op'>)</span> <span class='op'>+</span></span>
<span>  <span class='fu'><a href='https://ggplot2.tidyverse.org/reference/geom_abline.html'>geom_hline</a></span><span class='op'>(</span><span class='fu'><a href='https://ggplot2.tidyverse.org/reference/aes.html'>aes</a></span><span class='op'>(</span>yintercept <span class='op'>=</span> <span class='fl'>0</span><span class='op'>)</span>, linetype <span class='op'>=</span> <span class='st'>"dashed"</span>, color <span class='op'>=</span> <span class='st'>"red"</span><span class='op'>)</span> <span class='op'>+</span></span>
<span>    <span class='fu'><a href='https://ggplot2.tidyverse.org/reference/geom_abline.html'>geom_hline</a></span><span class='op'>(</span><span class='fu'><a href='https://ggplot2.tidyverse.org/reference/aes.html'>aes</a></span><span class='op'>(</span>yintercept <span class='op'>=</span> <span class='op'>-</span><span class='fu'><a href='https://rdrr.io/r/base/Log.html'>log</a></span><span class='op'>(</span><span class='va'>pi</span> <span class='op'>/</span> <span class='op'>(</span><span class='fl'>1</span><span class='op'>-</span><span class='va'>pi</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span>, </span>
<span>               linetype <span class='op'>=</span> <span class='st'>"dashed"</span>, color <span class='op'>=</span> <span class='st'>"blue"</span><span class='op'>)</span> <span class='op'>+</span></span>
<span>    <span class='fu'><a href='https://ggplot2.tidyverse.org/reference/lims.html'>ylim</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='op'>-</span><span class='fl'>10</span>,<span class='fl'>10</span><span class='op'>)</span><span class='op'>)</span> <span class='op'>+</span> <span class='fu'><a href='https://ggplot2.tidyverse.org/reference/lims.html'>xlim</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='op'>-</span><span class='fl'>1</span>, <span class='fl'>2</span><span class='op'>)</span><span class='op'>)</span> <span class='op'>+</span></span>
<span>  <span class='fu'><a href='https://ggplot2.tidyverse.org/reference/labs.html'>xlab</a></span><span class='op'>(</span><span class='st'>"X"</span><span class='op'>)</span> <span class='op'>+</span> <span class='fu'><a href='https://ggplot2.tidyverse.org/reference/labs.html'>ylab</a></span><span class='op'>(</span><span class='st'>"Log-Likelihood Ratio"</span><span class='op'>)</span></span></code></pre>
</div>
</details>
<p><img src="interpreting-the-likelihood-ratio-cost_files/figure-html5/unnamed-chunk-3-1.png" width="624" /></p>
</div>
<p>The reason why the lines differ is that they are designed to solve a different predictive problem: as we’ve argued above, minimizing <span class="math inline">\(C_\text{LLR}\)</span> looks for the best <span class="math inline">\(Y\vert X\)</span> conditional probability estimate according to the balanced
measure <span class="math inline">\(\text{Pr}&#39;\)</span>, whereas cross-entropy minimization does the same for
the original measure <span class="math inline">\(\text{Pr}\)</span>. This is how data looks like under the two measures (the histograms are stacked - in the unbalanced case, positive examples are invisible on the linear scale of the plot):</p>
<div class="layout-chunk" data-layout="l-body">
<details>
<summary>
Show code
</summary>
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>test_data</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://dplyr.tidyverse.org/reference/bind_rows.html'>bind_rows</a></span><span class='op'>(</span></span>
<span>  <span class='fu'>rxy</span><span class='op'>(</span>n <span class='op'>=</span> <span class='fl'>1e6</span>, pi <span class='op'>=</span> <span class='fl'>0.5</span><span class='op'>)</span> <span class='op'>|&gt;</span> <span class='fu'><a href='https://dplyr.tidyverse.org/reference/mutate.html'>mutate</a></span><span class='op'>(</span>type <span class='op'>=</span> <span class='st'>"Balanced"</span>, llr_thresh <span class='op'>=</span> <span class='fl'>0</span><span class='op'>)</span>,</span>
<span>  <span class='fu'>rxy</span><span class='op'>(</span>n <span class='op'>=</span> <span class='fl'>1e6</span><span class='op'>)</span> <span class='op'>|&gt;</span> <span class='fu'><a href='https://dplyr.tidyverse.org/reference/mutate.html'>mutate</a></span><span class='op'>(</span>type <span class='op'>=</span> <span class='st'>"Unbalanced"</span>, llr_thresh <span class='op'>=</span> <span class='op'>-</span><span class='fu'><a href='https://rdrr.io/r/base/Log.html'>log</a></span><span class='op'>(</span><span class='va'>pi</span> <span class='op'>/</span> <span class='op'>(</span><span class='fl'>1</span><span class='op'>-</span><span class='va'>pi</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span></span>
<span>  <span class='op'>)</span></span>
<span></span>
<span><span class='va'>test_data</span> <span class='op'>|&gt;</span> </span>
<span>  <span class='fu'><a href='https://ggplot2.tidyverse.org/reference/ggplot.html'>ggplot</a></span><span class='op'>(</span><span class='fu'><a href='https://ggplot2.tidyverse.org/reference/aes.html'>aes</a></span><span class='op'>(</span>x <span class='op'>=</span> <span class='va'>x</span>, fill <span class='op'>=</span> <span class='va'>y</span><span class='op'>)</span><span class='op'>)</span> <span class='op'>+</span> </span>
<span>  <span class='fu'><a href='https://ggplot2.tidyverse.org/reference/geom_histogram.html'>geom_histogram</a></span><span class='op'>(</span>bins <span class='op'>=</span> <span class='fl'>100</span><span class='op'>)</span> <span class='op'>+</span></span>
<span>  <span class='fu'><a href='https://ggplot2.tidyverse.org/reference/facet_grid.html'>facet_grid</a></span><span class='op'>(</span><span class='va'>type</span> <span class='op'>~</span> <span class='va'>.</span>, scales <span class='op'>=</span> <span class='st'>"free_y"</span><span class='op'>)</span> <span class='op'>+</span></span>
<span>  <span class='fu'><a href='https://ggplot2.tidyverse.org/reference/lims.html'>xlim</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='op'>-</span><span class='fl'>2</span>, <span class='fl'>4</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
</details>
<p><img src="interpreting-the-likelihood-ratio-cost_files/figure-html5/unnamed-chunk-4-1.png" width="624" /></p>
</div>
<p>These differences are reflected in the misclassification rates of the resulting classifiers defined by <span class="math inline">\(\hat Y(X)=I(\text{LLR}(X)&gt;\text{threshold})\)</span>, where the appropriate threshold is zero in the balanced case, and <span class="math inline">\(\ln(\frac{1-\pi}{\pi})\)</span> in the unbalanced case. According to intuition, we see that the <span class="math inline">\(C_\text{LLR}\)</span> optimizer beats the cross-entropy optimizer on the balanced sample, while performing
significantly worse on the unbalanced one.</p>
<div class="layout-chunk" data-layout="l-body">
<details>
<summary>
Show code
</summary>
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>test_data</span> <span class='op'>|&gt;</span></span>
<span>  <span class='fu'><a href='https://dplyr.tidyverse.org/reference/mutate.html'>mutate</a></span><span class='op'>(</span></span>
<span>    llr_cllr <span class='op'>=</span> <span class='fu'>llr</span><span class='op'>(</span><span class='va'>x</span>, <span class='va'>par_cllr</span><span class='op'>)</span>,</span>
<span>    llr_cross_entropy <span class='op'>=</span> <span class='fu'>llr</span><span class='op'>(</span><span class='va'>x</span>, <span class='va'>par_cross_entropy</span><span class='op'>)</span>,</span>
<span>    llr_true <span class='op'>=</span> <span class='fu'>llr_true</span><span class='op'>(</span><span class='va'>x</span><span class='op'>)</span></span>
<span>    <span class='op'>)</span> <span class='op'>|&gt;</span></span>
<span>  <span class='fu'><a href='https://dplyr.tidyverse.org/reference/group_by.html'>group_by</a></span><span class='op'>(</span><span class='va'>type</span><span class='op'>)</span> <span class='op'>|&gt;</span></span>
<span>  <span class='fu'><a href='https://dplyr.tidyverse.org/reference/summarise.html'>summarise</a></span><span class='op'>(</span></span>
<span>    cllr <span class='op'>=</span> <span class='fl'>1</span> <span class='op'>-</span> <span class='fu'><a href='https://rdrr.io/r/base/mean.html'>mean</a></span><span class='op'>(</span><span class='op'>(</span><span class='va'>llr_cllr</span> <span class='op'>&gt;</span> <span class='va'>llr_thresh</span><span class='op'>)</span> <span class='op'>==</span> <span class='va'>y</span><span class='op'>)</span>,</span>
<span>    cross_entropy <span class='op'>=</span> <span class='fl'>1</span> <span class='op'>-</span> <span class='fu'><a href='https://rdrr.io/r/base/mean.html'>mean</a></span><span class='op'>(</span><span class='op'>(</span><span class='va'>llr_cross_entropy</span> <span class='op'>&gt;</span> <span class='va'>llr_thresh</span><span class='op'>)</span> <span class='op'>==</span> <span class='va'>y</span><span class='op'>)</span>,</span>
<span>    true_llr <span class='op'>=</span> <span class='fl'>1</span> <span class='op'>-</span> <span class='fu'><a href='https://rdrr.io/r/base/mean.html'>mean</a></span><span class='op'>(</span><span class='op'>(</span><span class='va'>llr_true</span> <span class='op'>&gt;</span> <span class='va'>llr_thresh</span><span class='op'>)</span> <span class='op'>==</span> <span class='va'>y</span><span class='op'>)</span></span>
<span>    <span class='op'>)</span></span></code></pre>
</div>
</details>
<pre><code># A tibble: 2 × 4
  type           cllr cross_entropy true_llr
  &lt;chr&gt;         &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;
1 Balanced   0.166         0.185    0.140   
2 Unbalanced 0.000994      0.000637 0.000518</code></pre>
</div>
<h2 id="final-remarks">Final remarks</h2>
<p>Our main conclusion in a nutshell is that <span class="math inline">\(C_\text{LLR}\)</span> minimization is
equivalent, <em>in the infinite sample limit</em>, to cross-entropy minimization on a
balanced version of the original distribution. We haven’t discussed what happens
for finite samples where variance starts to play a role, affecting the
<em>efficiency</em> of loss functions as model optimization and selection criteria.
For instance, for a well specified model of likelihood ratio, how do the
convergence properties of <span class="math inline">\(C_{\text{LLR}}\)</span> and cross-entropy estimators compare
to each other? I expect that answering questions like this would require a much
more in-depth study than the one performed here (likely, with simulation playing
a central role).</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-BRUMMER2006230" class="csl-entry" role="listitem">
Brümmer, Niko, and Johan du Preez. 2006. <span>“Application-Independent Evaluation of Speaker Detection.”</span> <em>Computer Speech &amp; Language</em> 20 (2): 230–75. https://doi.org/<a href="https://doi.org/10.1016/j.csl.2005.08.001">https://doi.org/10.1016/j.csl.2005.08.001</a>.
</div>
<div id="ref-Cover2006" class="csl-entry" role="listitem">
Cover, Thomas M., and Joy A. Thomas. 2006. <em>Elements of Information Theory 2nd Edition (Wiley Series in Telecommunications and Signal Processing)</em>. Hardcover; Wiley-Interscience.
</div>
</div>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>This is how I understood things should <em>theoretically</em> work, from discussions with friends who are actually working on this field. I have no idea on how much day-to-day practice comes close to this mathematical ideal, and whether there exist alternative frameworks to the one I have just described.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The Likelihood Ratio Cost was introduced in <span class="citation" data-cites="BRUMMER2006230">(<a href="#ref-BRUMMER2006230" role="doc-biblioref">Brümmer and du Preez 2006</a>)</span>. The reference looks very complete, but I find its notation and terminology so unfamiliar that I decided to do my own investigation and leave this reading for a second moment.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>That is to say, <span class="math inline">\(w(Y_{\sigma(1)},\,Y_{\sigma(2)},\dots,\,Y_{\sigma(N)})=w(Y_1,\,Y_2,\dots,\,Y_N)\)</span> for any permutation <span class="math inline">\(\sigma\)</span> of the set <span class="math inline">\(\{1,\,2,\,\dots,\,N\}\)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><em>Nota bene:</em> the function <span class="math inline">\(f\)</span> is here assumed to be fixed, whereas the randomness in the quantity <span class="math inline">\(L _N ^w [f]\)</span> only comes from the paired observations <span class="math inline">\(\{(X_i,\,Y_i)\}_{i=1,\,2,\,\dots,N}\)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Notice that, due to symmetry <span class="math inline">\(\mathbb E(w(\boldsymbol Y)\vert \sum _i Y_i &gt;0) = \mathbb E(w(\boldsymbol Y)\vert Y_1 = 1)\)</span>, which might be easier to compute.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Here and below I relax a bit
the notation, as most details should be clear from context.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>The quantity <span class="math inline">\(w(\boldsymbol Y)\)</span> is not defined when all <span class="math inline">\(Y_i\)</span>’s are zero, as the right-hand
side of Eq. <a href="#eq:CLLR">(1)</a> itself. In this case, we make the convention <span class="math inline">\(w(\boldsymbol Y) = 0\)</span>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>For the original loss in Eq. <a href="#eq:CLLR">(1)</a>, without the modification discussed above, the result would have been
<span class="math inline">\(\dfrac{\text {Pr}^\prime (Y=1)}{\text{Pr}^\prime (Y=0)}=\dfrac{1-\text {Pr}(Y=0)^N}{1-\text {Pr}(Y=1)^N}.\)</span><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Formally, given an i.i.d.
stochastic process <span class="math inline">\(Z_i = (X_i,\,Y_i)\)</span>, we can define a new stochastic process
<span class="math inline">\(Z_i ^\prime = (X_i^\prime,\,Y_i^\prime)\)</span> such that
<span class="math inline">\(Z_i ^\prime = Z_{2i - 1}\)</span> if <span class="math inline">\(Y_{2i-1}\neq Y_{2i}\)</span>, and <span class="math inline">\(Z_i ^\prime = \perp\)</span>
(not defined) otherwise. Discarding <span class="math inline">\(\perp\)</span> values, we obtain an i.i.d.
stochastic process whose individual observations are distributed according to
<span class="math inline">\(\text{Pr}^\prime\)</span>.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>There is another
case in which <span class="math inline">\(C_{\text{LLR}}\)</span> and cross-entropy minimization converge to the
same answer as <span class="math inline">\(N\to \infty\)</span>: when used for model selection among a class of
models for the likelihood or posterior odds ratio that contains their correct
functional form.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>This is just logistic regression. It could be a reasonable approximation if <span class="math inline">\(\sigma_0 ^2\approx \sigma_1 ^2\)</span>, which however I will assume below to be badly violated.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
<h3 id="references">References</h3>
<div id="references-listing"></div>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
