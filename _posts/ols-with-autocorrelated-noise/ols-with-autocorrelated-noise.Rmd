---
title: "OLS with autocorrelated noise"
description: |
  A short description of the post.
author:
  - name: vgherard
    url: https://vgherard.github.io
date: 2023-05-20
output:
  distill::distill_article:
    self_contained: false
draft: true
---


Consider two time series $Y_t$ and $X_t$ such that:

$$
Y_t =  X_t \cdot \beta+\eta_t								(\#eq:YvsX)
$$
where $\eta_t$ is $\text{AR}(1)$ noise:

$$
\eta_{t+1} = \alpha \eta_t + \epsilon_t, \qquad \epsilon _t \sim \mathcal N(0,\sigma^2_0)																(\#eq:AR1)
$$
By iteration of \@ref(eq:AR1), we see that $\eta_t$ has gaussian *unconditional distribution*:

$$
\eta_t \sim \mathcal N (0, \sigma ^2),\qquad \sigma^2 \equiv \frac{\sigma^2_0}{1-\alpha ^2}								(\#eq:AR1Unconditional)
$$
so that individual observations of $(X_t,\,Y_t)$ are distributed according to a perfectly specified linear model.

This does *not* mean that, given observational data $\{(X_t,\,Y_t)\}_{t = 1,\,2,\,\dots,\,T}$, we are allowed to make standard linear model assumptions to perform valid inference on the parameters $\beta$ and $\sigma$ of Eqs. \@ref(eq:YvsX) and \@ref(eq:AR1Unconditional). Since the $\left(X_t,Y_t\right)$ pairs are not independent draws from a joint distributions, but autocorrelated, the usual OLS variance estimate under linear model assumptions will be biased, as we show below ^[For the linear model assumptions to hold, the $(X_t,\,Y_t)$ pairs should come from *independent realizations* of the same time series, which is of course not the type of data we usually have available.].

Suppose, more generally, that the error term $\eta _t$ is a stationary time series with zero unconditional mean ($\mathbb E(\eta_t)=0$) and unconditional variance $\text{Var}(\eta _t)=\sigma ^2$. The OLS estimate of $\beta$ is^[As usual we stack observations vertically in the $\mathbf X$ and $\mathbf Y$ matrices.]:

$$
\hat \beta =(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\mathbf Y=\beta + (\mathbf X^T\mathbf X)^{-1} \mathbf X^T \mathbf{η}, (\#eq:OLSBeta)
$$
which is unbiased since $\mathbb E (\mathbf{η}) = 0$. The estimate of the noise variance $\sigma ^2$, on the other hand:

$$
\begin{split}
\hat \sigma ^2	& = \frac{(\mathbf Y - \mathbf X\hat \beta)^T(\mathbf Y - \mathbf X\hat \beta)}{N-p}= \frac{\mathbf{η}^T(\mathbf 1-\mathbf H)\mathbf{η} }{N-p} \\
\mathbb E (\hat \sigma ^2) & = \dfrac{\text {Tr}\left[(\mathbf 1- \mathbb E(\mathbf H))\cdot  \text {Cor}(\mathbf{η})\right]}{N-p}\sigma ^2						
\end{split}, (\#eq:OLSSigma)
$$
where $\mathbf H = \mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T$ as usual, and we have used the fact that $ \mathbb {V}(\mathbf{η}) =\sigma ^2 \cdot \text {Cor}(\mathbf{η})$ (since each $\eta_t$ has the same unconditional variance $\sigma ^2$. Hence the $\hat \sigma ^2$ OLS estimate is biased if $\text{Cor}(\mathbf{η})\neq \mathbf 1$.

Similarly, the variance of the OLS $\hat \beta$ estimator is:

$$
\mathbb V (\hat \beta) = \mathbb E\left[(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\text {Cor}(\mathbf{η})\mathbf X (\mathbf X^T\mathbf X)^{-1} \right]\sigma^2
$$
whereas the OLS estimate of the variance is:

$$
\hat {\mathbb V} (\hat \beta) = (\mathbf X^T\mathbf X)^{-1} \hat \sigma ^2
$$
which is biased for $\text{Cor}(\mathbf{η})\neq \mathbf 1$.


```{r}
library(dplyr)
library(ggplot2)

rxy_fun <- function(rx, f, reps) {
	res <- function(n) {
		x <- rx(n)  # X has marginal distribution 'rx'
		y <- f(x) + reps(x)  # Y has conditional mean 'f(x)' and noise 'reps(x)'
		return(tibble(x = x, y = y))  
	}
	return(structure(res, class = "rxy"))
}

plot.rxy <- function(x, N = 1000, seed = 840) {
	set.seed(seed)
	
	ggplot(data = x(N), aes(x = x, y = y)) +
		geom_point(alpha = 0.3) + 
		geom_smooth(method = "lm", se = FALSE)
}

lmsim <- function(rxy, N = 100, vcov = stats::vcov, B = 1e3, seed = 840) 
{ 
	set.seed(seed)
	
	res <- list(coef = matrix(nrow = B, ncol = 2), vcov = vector("list", B))
	colnames(res$coef) <- c("(Intercept)", "x")
	class(res) <- "lmsim"
								
	for (b in 1:B) {
		.fit <- lm(y ~ ., data = rxy(N))
		res$coef[b, ] <- coef(.fit)  # Store intercept and slope in B x 2 matrix
		res$vcov[[b]] <- vcov(.fit)  # Store vcov estimates in length B list. 
	}
	
	return(res)
}

print.lmsim <- function(x) 
{
	cat("Simulation results:\n\n")
	cat("* Model-trusting vcov (average of vcov estimates):\n")
	print( avg_est_vcov <- Reduce("+", x$vcov) / length(x$vcov) )
	cat("\n* Simulation-based vcov (vcov of coefficient estimates):\n")
	print( emp_vcov <- cov(x$coef))
	cat("\n* Ratio (1st / 2nd):\n")
	print( avg_est_vcov / emp_vcov )
	return(invisible(x))
}
```

```{r}
rxy_01 <- rxy_fun(
	rx = \(n) 1 + arima.sim(list(order = c(1,0,0), ar = 0.4), n = n)[-1],
	f = \(x) 1 + 0.1*x,
	reps = \(x) arima.sim(list(order = c(1,0,0), ar = +0.1), n = length(x) )
)

plot(rxy_01)
lmsim(rxy_01,
			# vcov = sandwich::vcovHAC, 
			N = 100)
```

