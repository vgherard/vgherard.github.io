---
title: "Finite sample bias of ARMA estimates"
description: |
  A short description of the post.
author:
  - name: vgherard
    url: https://vgherard.github.io
date: 2023-06-08
output:
  distill::distill_article:
    self_contained: false
draft: true
---

```{r setup, include=FALSE}
set.seed(840)
```

## Intro

Fitting ARMA models on short time series datasets can be challenging from 
various points of view. 
A purely technical difficulty is the fact that, even for well specified models, 
coefficient estimates obtained from maximum likelihood estimation are biased. 
More generally, the sampling distribution of estimators 
can deviate substantially from their asymptotic Gaussian distribution, 
invalidating naive constructions like normal confidence intervals (*i.e.* 
$\text{Estimate} \pm k \cdot \text{SE}$)

Consider, for example, a stationary $\text{AR}(1)$ process:

$$
Y_{t+1} =  \mu + \alpha \cdot (Y_t-\mu) +\epsilon _t,\quad \epsilon _t\sim \mathcal N(0,\sigma ^2), (\#eq:AR1)
$$
and denote by $\hat \alpha _n$ the maximum likelihood estimate of $\alpha$ from
a series of $n$ data points. It is only in the $n\to \infty$ limit that 
$\mathbb E (\hat \alpha _n) \to \alpha$, but for small $n$, the bias can be 
severe. Furthermore, as I show below, the distribution of $\hat \alpha _n$ has 
a heavy left tail, which results in a decent probability of estimating a 
negative correlation between $Y_{t+1}$ and $Y_t$. Altogether, even with a simple
model like \@ref(eq:AR1), parameter estimation in the low sample size regime is
not trivial.

With these considerations in mind, I set out myself to construct an 
(approximately) unbiased estimator for the $\alpha$ parameter in Eq. 
\@ref(eq:AR1), and a corresponding confidence interval. Eq. \@ref(eq:AR1) is, 
of course, just a toy model, but the method I discuss below - based on a 
parametric bootstrap - is rather general and can be applied in realistic 
circumstances.

## Distribution of $\hat alpha _n$

To begin with, we study the distribution of the maximum likelihood estimate 
$\hat \alpha _n$ for small $n$. For later convenience, I define a couple of 
wrappers:

```{r}
# Simulate 'n' observations from a given AR(1) model (with unit noise variance).
ar_sim <- function(alpha, mu, n)  
	mu + arima.sim(list(ar = alpha), n = n)

# Fit an AR(1) model to a given time series 'y'.
ar_fit <- function(y, method = "ML")
	arima(y, c(1, 0, 0), method = method)
```

The plot below shows the distribution of $\hat \alpha _n$, for $\mu = 1$, 
$\alpha = 0.7$ and $n = 20$, which presents the features anticipated in the 
introduction.

```{r include=FALSE}
hist_caption <- paste(
	"AR(1) coefficient estimates.",
	"The vertical blue line represents true parameter value,",
	"while the red line is the expected value of the estimator."
)
```
```{r fig.cap=hist_caption}
mu <- 1
alpha <- 0.7
n <- 20

set.seed(840)
nsim <- 1e4
fits <- replicate(nsim, 
									ar_sim(alpha = alpha, mu = mu, n = n) |> ar_fit(), 
									simplify = F
									)

estimates <- sapply(fits, coef)["ar1", ]

hist(estimates, breaks = 100,
		 xlab = "Estimate", 
		 main = "AR(1) coefficient estimates",  
		 )
abline(v = alpha, col = "blue", lwd = 3, lty = 1)
abline(v = mean(estimates), col = "red", lwd = 3, lty = 1)
```

The true coverage rate of the asymptotic $95 \%$ CIs is calculated as follows 
(interestingly, for $\mu = 0$ the function `ar1_confint_naive()` constructs 
valid confidence intervals)^[
R does not have an S3 `confint.Arima` method, and the default method treats the 
$t$ statistic $\frac{\hat \alpha - \alpha}{\widehat{\text{SE}}(\hat \alpha)}$ as
a $Z$-score. A better approximation is obtained if this is treated as a $t$ 
statistic with the correct number of degrees of freedom, as in the 
`ar1_confint_naive()` function below.
]:

```{r}
ar1_confint_naive <- function(fit, conf.level = 0.95) {
	est <- coef(fit)["ar1"]
	se <- sqrt(vcov(fit)["ar1", "ar1"])
	
	df <- nobs(fit) - length(coef(fit))
	
	a <- (1 - conf.level) / 2
	k <- qt(a, df, lower.tail = FALSE)
	
	res <- est + k * se * c(-1, 1)
	unname(res)
	
	return(res)
}

cis <- sapply(fits, ar1_confint_naive)

mean( cis[1,] < alpha & alpha < cis[2,], na.rm = TRUE)
```


## Bootstrap confidence intervals


The bias can be corrected by a bootstrap:

```{r}
ar_t <- function(fit, alpha) {
	alpha_hat <- coef(fit)["ar1"]
	alpha_hat_se <- vcov(fit)["ar1", "ar1"]
	t <- (alpha_hat - alpha) / alpha_hat_se
	unname(t)
}

nsim_max <- 1e3
B <- 1e3

cl <- parallel::makeCluster(8)
parallel::clusterExport(cl, c("ar_sim", "ar_fit", "ar_t", "n", "B", "conf.level"))
boot <- pbapply::pblapply(fits[1:nsim_max], function(fit) {
	alpha_hat <- coef(fit)["ar1"]
	alpha_hat_se <- unname(sqrt(vcov(fit)["ar1", "ar1"]))
	a <- (1 - conf.level) / 2
	
	boot_fits <- replicate(B,
												 tryCatch(
												 	ar_sim(alpha = alpha_hat, mu = 1, n = n) |> ar_fit(),
												 	error = function(cnd) NULL
												 	),
												 simplify = F
												 )
	
	boot_fits <- boot_fits[!sapply(boot_fits, is.null)]
	
	alpha_hat_boot <- sapply(boot_fits, coef)["ar1", ]
	alpha_hat_t_boot <- sapply(boot_fits, ar_t, alpha = alpha_hat)
	
	mask <- is.na(alpha_hat_t_boot) | is.na(alpha_hat_t_boot)
	
	alpha_hat_boot <- alpha_hat_boot[!mask]
	alpha_hat_t_boot <- alpha_hat_t_boot[!mask]
	
	res <- data.frame(
		alpha_hat = alpha_hat, 
		bias = mean(alpha_hat_boot) - alpha_hat,
		alpha_lo = 2 * alpha_hat - quantile(alpha_hat_boot, 1 - a), 
		alpha_up = 2 * alpha_hat - quantile(alpha_hat_boot, a), 
		alpha_lo_stud = alpha_hat - quantile(alpha_hat_t_boot, 1 - a) * alpha_hat_se, 
		alpha_up_stud = alpha_hat - quantile(alpha_hat_t_boot, a) * alpha_hat_se
		)
	row.names(res) <- NULL
	res
}, cl = cl) |>
	Reduce(rbind, x = _)

boot

mean(boot$bias)
sd(boot$bias)
mean(boot$alpha_lo < alpha & alpha < boot$alpha_up)
mean(boot$alpha_lo_stud < alpha & alpha < boot$alpha_up_stud)
```


