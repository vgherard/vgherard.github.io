<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>vgherard</title>
    <link>https://vgherard.github.io/</link>
    <atom:link href="https://vgherard.github.io/notebooks.xml" rel="self" type="application/rss+xml"/>
    <description>Valerio Gherardi's Personal Website
</description>
    <generator>Distill</generator>
    <lastBuildDate>Thu, 14 Mar 2024 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Maximum Likelihood</title>
      <dc:creator>Valerio Gherardi</dc:creator>
      <link>https://vgherard.github.io/notebooks/maximum-likelihood</link>
      <description>


&lt;p&gt;We start off with a functional description of Maximum Likelihood (ML)
estimation. Let &lt;span class="math inline"&gt;\(\mathcal Q \equiv\{\text d
Q_{\theta} = q_\theta \,\text d \mu\}_{\theta \in \Theta}\)&lt;/span&gt; be a
parametric family of probability measures dominated by some common
measure &lt;span class="math inline"&gt;\(\mu\)&lt;/span&gt;. Consider the
functional&lt;a href="#fn1" class="footnote-ref"
id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\theta ^* (P) = \arg \min_{\theta \in \Theta} \intop \text dP\,\ln
(\frac{1}{q_\theta}) (\#eq:FunctionalThethaStar).
\]&lt;/span&gt; This is the parameter of the best (in the cross-entropy sense)
approximation of &lt;span class="math inline"&gt;\(P\)&lt;/span&gt; within &lt;span
class="math inline"&gt;\(\mathcal Q\)&lt;/span&gt;, which we assume to be
unique.&lt;/p&gt;
&lt;p&gt;If &lt;span class="math inline"&gt;\(P\)&lt;/span&gt; represents the true
probability distribution of the data under study, &lt;span
class="math inline"&gt;\(\theta ^*(P)\)&lt;/span&gt; is the target of ML
estimation, in the general case in which &lt;span
class="math inline"&gt;\(P\)&lt;/span&gt; is not necessarily in &lt;span
class="math inline"&gt;\(\mathcal Q\)&lt;/span&gt;. The ML estimate &lt;span
class="math inline"&gt;\(\hat \theta _N\)&lt;/span&gt; of &lt;span
class="math inline"&gt;\(\theta^*\)&lt;/span&gt; from an i.i.d. sample of &lt;span
class="math inline"&gt;\(N\)&lt;/span&gt; observations is&lt;a href="#fn2"
class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat \theta _N \equiv \theta ^*(\hat P _N)=\arg \max_{\theta \in \Theta}
\sum_{i=1}^N \ln ({q_\theta(Z_i)}), (\#eq:ThetaMLE)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(\hat P _N\)&lt;/span&gt; is the
empirical distribution of the sample.&lt;/p&gt;
&lt;p&gt;Denoting:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
c_{P}(\theta) = \intop \text dP\,\ln
(\frac{1}{q_\theta}),(\#eq:CrossEntIntegral)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;we see that &lt;span class="math inline"&gt;\(\theta^*\)&lt;/span&gt; is
determined by the condition &lt;span
class="math inline"&gt;\(c_{P}&amp;#39;(\theta^*)=0\)&lt;/span&gt;. From this, we can
easily derive the first order variation of &lt;span
class="math inline"&gt;\(\theta ^*\)&lt;/span&gt; under a variation &lt;span
class="math inline"&gt;\(P \to P + \delta P\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\delta \theta ^* =\left(\intop \text dP\,I_{\theta ^*}
\right)^{-1}\left(\intop \text d(\delta P)u_{\theta
^*}\right)(\#eq:DifferentialThetaStar)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where we have defined:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
u_\theta = \frac{\partial }{\partial \theta} \ln q_\theta,\quad I_\theta
= -\frac{\partial^2 }{\partial \theta ^2}  \ln
q_\theta.(\#eq:ScoreFisherInfo)
\]&lt;/span&gt; From @ref(eq:DifferentialThetaStar) we can identify the
influence function of the &lt;span class="math inline"&gt;\(\theta ^*\)&lt;/span&gt;
functional:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\psi_P(z)=\left(\intop \text dP\,I_{\theta ^*} \right)^{-1}u_{\theta
^*}(z)(\#eq:InfluenceFunction)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then, from the standard theory of influence functions, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat \theta _N \approx \theta ^*+J_{\theta ^*} ^{-1}
U_{\theta^*}(\#eq:ThetaNFirstOrder)
\]&lt;/span&gt; where we have defined:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
J_{\theta ^*}\equiv \intop \text dP\,I_{\theta ^*},\quad U_{\theta ^*
}\equiv\frac{1}{N}\sum _{i=1}^Nu_{\theta
^*}(Z_i)(\#eq:ScoreFisherInfo2).
\]&lt;/span&gt; In particular, we obtain the Central Limit Theorem (CLT)&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\sqrt N(\hat \theta _N - \theta ^*) \overset{d}{\to} \mathcal N(0,
J_{\theta^*}^{-1}K_{\theta ^*}J_{\theta ^*}^{-1}),(\#eq:ThetaCLT)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
K_{\theta ^*} = \mathbb V(u_{\theta ^*}(Z)). (\#eq:ScoreVariance)
\]&lt;/span&gt; The matrices &lt;span class="math inline"&gt;\(K_{\theta
^*}\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(J_{\theta ^*}\)&lt;/span&gt;
depend on the unknown value &lt;span class="math inline"&gt;\(\theta
^*\)&lt;/span&gt;, but we can readily construct plugin estimators:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat J_N = -\frac{1}{N}\sum _{i=1}^NI_{\hat \theta _N}(z_i),\quad\hat
K_N = \frac{1}{N}\sum _{i=1}^Nu_{\hat \theta _N}(z_i)u_{\hat \theta
_N}(z_i)^T,(\#eq:PluginJK)
\]&lt;/span&gt; and estimate the variance of &lt;span class="math inline"&gt;\(\hat
\theta _N\)&lt;/span&gt; as:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\widehat {\mathbb V}(\hat \theta _N) = \frac{\hat J _N ^{-1}\hat K_N\hat
J_N ^{-1}}{N}(\#eq:SandwichEstimator),
\]&lt;/span&gt; which is the usual Sandwich estimator. Finally, if &lt;span
class="math inline"&gt;\(P = Q_{\theta^*}\)&lt;/span&gt;, then &lt;span
class="math inline"&gt;\(J _{\theta^*} = K _{\theta^*}\)&lt;/span&gt;, and the
CLT @ref(eq:ThetaCLT) becomes simply &lt;span class="math inline"&gt;\(\sqrt
N(\hat \theta _N - \theta ^*) \overset{d}{\to} \mathcal N(0,
J_{\theta^*}^{-1})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let us now consider the following expansion of &lt;span
class="math inline"&gt;\(c_P(\hat \theta _N)\)&lt;/span&gt; which, we recall, is
the cross-entropy of the ML model on the true distribution &lt;span
class="math inline"&gt;\(P\)&lt;/span&gt; (&lt;em&gt;cf.&lt;/em&gt;
@ref(eq:CrossEntIntegral)):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
c_P(\hat \theta _N)
    &amp;amp;= -\intop \text d P(z&amp;#39;)\,\ln (q_{\hat \theta}(z&amp;#39;))\\
    &amp;amp; \approx -\mathbb E&amp;#39;(\ln q_{\theta^*})+\frac{1}{2}(\hat
\theta-\theta ^*)^TJ_{\theta ^*} (\hat \theta-\theta ^*)\\
    &amp;amp; \approx -\mathbb E&amp;#39;(\ln q_{\theta^*})+\frac{1}{2}U_{\theta
^*}^TJ_{\theta ^*}^{-1}U_{\theta ^*}
\end{split}
\]&lt;/span&gt; Taking the expectation with respect to the training dataset,
noting that &lt;span class="math inline"&gt;\(\mathbb E(U_{\theta ^*}U_{\theta
^*}^T)=K_{\theta ^*}\)&lt;/span&gt;, we get:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E (c_P(\hat \theta _N))\approx -\mathbb E&amp;#39;(\ln
q_{\theta^*})+\frac{1}{2N}\text {Tr}(J_{\theta ^*}^{-1}K_{\theta^*})
(\#eq:CrossEntExp)
\]&lt;/span&gt; Now consider the in-sample estimate:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
c_{\hat P _N}(\hat \theta _N) &amp;amp;= -\frac{1}{N}\sum _{i=1}^N\ln
q_{\hat \theta}(Z_i)\\
&amp;amp; \approx - \frac{1}{N}\sum _{i=1} ^N \ln q_{\theta^*}(Z_i)-
U_{\theta ^*}^T(\hat \theta _N-\theta^*)+ \frac{1}{2}(\hat \theta
_N-\theta^*)^TJ_{\theta ^*}(\hat \theta _N-\theta^*)\\
&amp;amp; \approx - \frac{1}{N}\sum _{i=1} ^N \ln q_{\theta^*}(Z_i)-
U_{\theta ^*}^TJ_{\theta ^*} ^{-1} U_{\theta^*}+ \frac{1}{2}U_{\theta
^{*}}^TJ_{\theta ^*} ^{-1}\hat J_N J_{\theta ^*} ^{-1}U_{\theta ^{*}}\\
&amp;amp; \approx - \frac{1}{N}\sum _{i=1} ^N \ln q_{\theta^*}(Z_i)-
\frac{1}{2}U_{\theta ^*}^TJ_{\theta ^*} ^{-1} U_{\theta^*}.
\end{split}
\]&lt;/span&gt; Taking the expectation :&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E (c_{\hat P _N}(\hat \theta _N)) = -\mathbb E&amp;#39;(\ln
q_{\theta^*})-\frac{1}{2N}\text{Tr}(J_{\theta
^*}^{-1}K_{\theta^*})(\#eq:InSampleCrossEntExp)
\]&lt;/span&gt; Comparing Eqs. @ref(eq:InSampleCrossEntExp) and
@ref(eq:CrossEntExp) we see that:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text{TIC}\equiv -\frac{1}{N}\sum _{i=1}^N\ln q_{\hat
\theta}(Z_i)+\frac{1}{N}\text{Tr}(J_{\theta
^*}^{-1}K_{\theta^*})(\#eq:TIC)
\]&lt;/span&gt; provides an asymptotically unbiased estimate of &lt;span
class="math inline"&gt;\(\mathbb E (c_P(\hat \theta _N))\)&lt;/span&gt;, the
expected cross-entropy of a model from family &lt;span
class="math inline"&gt;\(\mathcal Q\)&lt;/span&gt; estimated on a sample of &lt;span
class="math inline"&gt;\(N\)&lt;/span&gt; observations.&lt;/p&gt;
&lt;p&gt;In the previous derivation, we could take each &lt;span
class="math inline"&gt;\(Z_i\)&lt;/span&gt; to be a pair &lt;span
class="math inline"&gt;\((X_i,\,Y_i)\)&lt;/span&gt; drawn from a joint &lt;span
class="math inline"&gt;\(X-Y\)&lt;/span&gt; distribution. If we replace the model
family &lt;span class="math inline"&gt;\(\mathcal Q\)&lt;/span&gt; with a parametric
family of conditional densities &lt;span class="math inline"&gt;\(\mathcal Q
\equiv\{\text d Q_{\theta}(\cdot\vert X) = q_{\theta}(\cdot\vert X)
\,\text d \mu\}_{\theta \in \Theta}\)&lt;/span&gt;, and change the objective
function to:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
c_P(\theta) = \intop \text dP(x,y)\,\ln (\frac{1}{q_{\theta}(y\vert
x)}),(\#eq:CondCrossEntIntegral)
\]&lt;/span&gt; we can repeat our above argument without any further change.
This provides a quick and dirty derivation of the CLT @ref(eq:ThetaCLT)
and of the information criterion @ref(eq:TIC) in a regression setting
with random regressors.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="citation"&gt;(Shalizi 2024)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="citation"&gt;(Claeskens and Hjort 2008)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="citation"&gt;(Freedman 2006)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="citation"&gt;(White 1982)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references csl-bib-body hanging-indent"&gt;
&lt;div id="ref-claeskens2008model" class="csl-entry"&gt;
Claeskens, Gerda, and Nils Lid Hjort. 2008. &lt;span&gt;“Model Selection and
Model Averaging.”&lt;/span&gt; &lt;em&gt;Cambridge Books&lt;/em&gt;.
&lt;/div&gt;
&lt;div id="ref-freedman2006so" class="csl-entry"&gt;
Freedman, David A. 2006. &lt;span&gt;“On the so-Called &lt;span&gt;‘Huber Sandwich
Estimator’&lt;/span&gt; and &lt;span&gt;‘Robust Standard Errors’&lt;/span&gt;.”&lt;/span&gt;
&lt;em&gt;The American Statistician&lt;/em&gt; 60 (4): 299–302.
&lt;/div&gt;
&lt;div id="ref-shaliziADA" class="csl-entry"&gt;
Shalizi, Cosma. 2024. &lt;em&gt;Advanced Data Analysis from an Elementary
Point of View&lt;/em&gt;. &lt;a
href="https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/"&gt;https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-white1982maximum" class="csl-entry"&gt;
White, Halbert. 1982. &lt;span&gt;“Maximum Likelihood Estimation of
Misspecified Models.”&lt;/span&gt; &lt;em&gt;Econometrica: Journal of the
Econometric Society&lt;/em&gt;, 1–25.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes footnotes-end-of-document"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;The definition does not depend on the representations
&lt;span class="math inline"&gt;\(q_\theta = \frac{\text d Q_\theta}{\text d
\mu}\)&lt;/span&gt; chosen for the &lt;span
class="math inline"&gt;\(\mu\)&lt;/span&gt;-density of &lt;span
class="math inline"&gt;\(Q_\theta\)&lt;/span&gt; if &lt;span
class="math inline"&gt;\(P\)&lt;/span&gt; is also absolutely continuous with
respect to &lt;span class="math inline"&gt;\(\mu\)&lt;/span&gt;, which we tacitly
assume. Typically &lt;span class="math inline"&gt;\(\mu\)&lt;/span&gt; would be some
relative of Lebesgue or counting measures, in continuous and discrete
settings respectively.&lt;a href="#fnref1"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;As a random variable, &lt;span class="math inline"&gt;\(\hat
\theta _N\)&lt;/span&gt; is also independent (modulo a measure zero set) of
the specific representer &lt;span class="math inline"&gt;\(q_\theta\)&lt;/span&gt;
if &lt;span class="math inline"&gt;\(P\)&lt;/span&gt; is absolutely continuous with
respect to &lt;span class="math inline"&gt;\(\mu\)&lt;/span&gt;.&lt;a href="#fnref2"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5>5752701da8bd0c0a178e71feaa8b8068</distill:md5>
      <guid>https://vgherard.github.io/notebooks/maximum-likelihood</guid>
      <pubDate>Thu, 14 Mar 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Exponential Dispersion Models</title>
      <dc:creator>Valerio Gherardi</dc:creator>
      <link>https://vgherard.github.io/notebooks/exponential-dispersion-models</link>
      <description>


&lt;h2 id="intro"&gt;Intro&lt;/h2&gt;
&lt;p&gt;Exponential Dispersion Models (EDMs) provide a natural generalization
of the normal distribution, in which the modeled variable &lt;span
class="math inline"&gt;\(Y\)&lt;/span&gt; is assumed to follow a probability
density:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text d P _{\lambda,\,\mu}(y)=e^{-\frac{\lambda}{2}d(y,\,\mu)}\text d
\nu _{\lambda}(y)
\]&lt;/span&gt; with respect to a certain dominating measure &lt;span
class="math inline"&gt;\(\nu _{\lambda}\)&lt;/span&gt;. Here &lt;span
class="math inline"&gt;\(\mu = \intop y\,\text d
P_{\lambda,\,\mu}(y)\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(d(y,\,\mu)
\geq 0\)&lt;/span&gt;, with equality only for &lt;span class="math inline"&gt;\(y =
\mu\)&lt;/span&gt;. The function &lt;span
class="math inline"&gt;\(d(y,\,\mu)\)&lt;/span&gt; is called the &lt;em&gt;unit
deviance&lt;/em&gt;, and plays for EDMs the same role of squared distance
&lt;span class="math inline"&gt;\((y-\mu)^2\)&lt;/span&gt; for the normal model. Not
surprisingly, EDMs provide a sound framework for the maximum-likelihood
based formulation of generalized linear models, additive models, and
similar beasts.&lt;/p&gt;
&lt;h2 id="exponential-dispersion-models"&gt;Exponential Dispersion
Models&lt;/h2&gt;
&lt;p&gt;We start with a probability measure on &lt;span
class="math inline"&gt;\(\mathbb R ^n\)&lt;/span&gt; in the form of an
&lt;em&gt;additive EDM&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text d P ^* _{\lambda, \theta} (z) = e^{\theta ^T
z-\lambda\kappa(\theta)}\text dQ^*_\lambda (z) (\#eq:EDstar)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(\lambda &amp;gt; 0\)&lt;/span&gt;,
&lt;span class="math inline"&gt;\(\text Q^*_\lambda\)&lt;/span&gt; is a Borelian
probability measure on &lt;span class="math inline"&gt;\(\mathbb R\)&lt;/span&gt;,
and &lt;span class="math inline"&gt;\(\kappa(\theta)\)&lt;/span&gt; is a
differentiable strictly convex function, with &lt;span
class="math inline"&gt;\(\kappa&amp;#39;&amp;#39;(\theta) &amp;gt; 0\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\kappa(0) =0\)&lt;/span&gt;. For a random variable &lt;span
class="math inline"&gt;\(Z\)&lt;/span&gt; distributed according to @ref(eq:ED) we
write &lt;span class="math inline"&gt;\(Z\sim \text{ED}^*(\lambda,
\,\theta,\,\kappa)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For any given &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt;,
normalization of @ref(eq:EDstar) requires: &lt;span class="math display"&gt;\[
e^{\lambda \kappa(\theta)}=\intop e^{\theta ^Ty}\text
dQ^*_\lambda(z)(\#eq:NormCond)
\]&lt;/span&gt; to hold for all &lt;span class="math inline"&gt;\(\theta\)&lt;/span&gt;
and &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt;. In other words, &lt;span
class="math inline"&gt;\(M_\lambda(\theta) \equiv e^{\lambda
\kappa(\theta)}\)&lt;/span&gt; must be the moment generating function of the
measure &lt;span class="math inline"&gt;\(Q^* _\lambda(y)\)&lt;/span&gt; for a given
&lt;span class="math inline"&gt;\(\theta\)&lt;/span&gt;, which we assume to be
uniquely determined by its moments&lt;a href="#fn1" class="footnote-ref"
id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, so that we can omit the mention of the
measure &lt;span class="math inline"&gt;\(Q^*_\lambda\)&lt;/span&gt; in the notation
&lt;span class="math inline"&gt;\(\text{ED}^*(\lambda,
\,\theta,\,\kappa)\)&lt;/span&gt;^. This requires, in particular &lt;span
class="math inline"&gt;\(\kappa (0) = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A closely related parametrization is the so-called &lt;em&gt;reproductive
EDM&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text d P _{\lambda, \theta} (y) = e^{\lambda(\theta ^T
y-\kappa(\theta))}\text dQ_\lambda (y)(\#eq:ED).
\]&lt;/span&gt; For a random variable &lt;span class="math inline"&gt;\(Y\)&lt;/span&gt;
distributed according to @ref(eq:ED) we write &lt;span
class="math inline"&gt;\(Y\sim \text{ED}(\lambda,
\,\theta,\,\kappa)\)&lt;/span&gt;. The link between @ref(eq:ED) and
@ref(eq:EDstar) is that &lt;span class="math inline"&gt;\(Y\sim
\text{ED}(\lambda, \,\theta,\,\kappa)\)&lt;/span&gt; if and only if &lt;span
class="math inline"&gt;\(Z=\lambda Y\sim \text{ED}^*(\lambda,
\,\theta,\,\kappa)\)&lt;/span&gt;, so that reproductive and additive EDMs can
be interchanged whenever convenient, at least for theoretical
considerations. The probability measures &lt;span
class="math inline"&gt;\(\text d Q_\lambda\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\text d Q ^*_\lambda\)&lt;/span&gt;, which are uniquely
determined by normalization, are related by push-forward:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
Q_\lambda ^* = (m_\lambda )_*(Q_\lambda),(\#eq:PushForward)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(m_\lambda\)&lt;/span&gt; denotes
multiplication by &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt;,
&lt;em&gt;i.e.&lt;/em&gt; &lt;span class="math inline"&gt;\(m_\lambda(y)=\lambda
y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In cases of practical interest (see the examples below), &lt;span
class="math inline"&gt;\(Q_\lambda\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(Q_\lambda^*\)&lt;/span&gt; are absolutely continuous
either with respect to the Lebesgue measure, or with respect some
measure concentrated on &lt;span class="math inline"&gt;\(c \cdot \mathbb
N\)&lt;/span&gt; for some &lt;span class="math inline"&gt;\(c&amp;gt;0\)&lt;/span&gt;. The two
cases are referred to as the “continuous” and “discrete” case,
respectively, for obvious reasons.&lt;/p&gt;
&lt;h2 id="general-properties"&gt;General Properties&lt;/h2&gt;
&lt;h3 id="moment-generating-function"&gt;Moment generating function&lt;/h3&gt;
&lt;p&gt;Consider first the reproductive EDM @ref(eq:ED). If &lt;span
class="math inline"&gt;\(Y\sim \text
{ED}(\lambda,\,\theta,\,\kappa)\)&lt;/span&gt;, its moment generating function
is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
M_Y(s)=\mathbb E(e^{sY})=\exp\left[\lambda\left(\kappa(\theta
+\frac{s}{\lambda})-\kappa(\theta)\right)\right],(\#eq:MGF)
\]&lt;/span&gt; from which we can derive, in particular:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
\mathbb E(Y) &amp;amp;= \frac{\text d}{\text ds}\vert_{s=0}\log M(s)
=\kappa&amp;#39;(\theta),\\
\mathbb V(Y) &amp;amp;= \frac{\text d^2}{\text ds ^2}\vert_{s=0}\log M(s)
=\frac{\kappa&amp;#39;&amp;#39;(\theta)}{\lambda}.\\
\end{split}(\#eq:ExpVar)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the additive EDM @ref(eq:EDstar), the corresponding results for
&lt;span class="math inline"&gt;\(Z\sim
\text{ED}^*(\lambda,\,\theta,\,\kappa)\)&lt;/span&gt; are:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
M_Z(s)&amp;amp;=\exp\left[\lambda\left(\kappa(\theta +
s)-\kappa(\theta)\right)\right],\\
\mathbb E(Z) &amp;amp;= \lambda\kappa&amp;#39;(\theta),\\
\mathbb V(Z) &amp;amp;= \lambda \kappa&amp;#39;&amp;#39;(\theta).
\end{split}(\#eq:MGFResultsEDstar)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id="legendre-transform-of-kappa-theta"&gt;Legendre Transform of &lt;span
class="math inline"&gt;\(\kappa (\theta)\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Since &lt;span class="math inline"&gt;\(\kappa\)&lt;/span&gt; is strictly convex,
the mapping:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mu = \frac{\partial\kappa}{\partial\theta}(\#eq:MuThetaMapping)
\]&lt;/span&gt; is invertible, and we may equivalently parametrize the
reproductive EDM in terms of &lt;span class="math inline"&gt;\(\mu\)&lt;/span&gt;
and &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt; as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text d P _{\lambda, \mu} (y) = e^{\lambda(\theta(\mu) ^T
(y-\mu)+\tau(\mu))}\text dQ_\lambda (y)(\#eq:EDmu).
\]&lt;/span&gt; where:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\tau(\mu) = \theta(\mu)^T\mu - \kappa(\theta(\mu))
(\#eq:LegendreTransform).
\]&lt;/span&gt; is the Legendre transform of &lt;span
class="math inline"&gt;\(\kappa\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id="deviance"&gt;Deviance&lt;/h2&gt;
&lt;p&gt;Consider two reproductive EDMs &lt;span class="math inline"&gt;\(P
_{\lambda,\mu _1}\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(P
_{\lambda,\mu _2}\)&lt;/span&gt; with the same dispersion parameter &lt;span
class="math inline"&gt;\(\lambda\)&lt;/span&gt; (the function &lt;span
class="math inline"&gt;\(\kappa\)&lt;/span&gt; is assumed to be fixed
throughout). The likelihood ratio at a given &lt;span
class="math inline"&gt;\(Y=y\)&lt;/span&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\ln (\frac{\text d P_{\lambda,\mu_1}}{\text d
P_{\lambda,\mu_2}}(y))=\lambda\cdot\left[(\theta_1-\theta_2)y-(\kappa_1-\kappa_2)\right],(\#eq:LogLikelihood)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(\theta _1 =
\theta(\mu_1)\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\kappa_1=
\kappa(\theta(\mu _1))\)&lt;/span&gt;, &lt;em&gt;etc.&lt;/em&gt;. Setting &lt;span
class="math inline"&gt;\(\mu _1 = y\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\mu _2 = \mu\)&lt;/span&gt; in this expression and
multiplying by a convenient factor, we obtain the so called &lt;em&gt;unit
scaled deviance&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
d_\lambda(y,\mu) &amp;amp;\equiv 2\left.\ln (\frac{\text d
P_{\lambda,\mu_0}}{\text d P_{\lambda,\mu}}(y)) \right \vert
_{\mu_0=y}\\&amp;amp;=2\lambda\cdot\left[(\theta(y)-\theta(\mu))y-\kappa(\theta(y))+\kappa(\theta(\mu))\right].
\end{split}(\#eq:UnitScaledDeviance)
\]&lt;/span&gt; The &lt;em&gt;unit deviance&lt;/em&gt; is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
d(y,\mu) &amp;amp;\equiv
d_1(y,\mu)\\&amp;amp;=2\cdot\left[(\theta(y)-\theta(\mu))y-\kappa(\theta(y))+\kappa(\theta(\mu))\right]
\end{split}(\#eq:UnitDeviance)
\]&lt;/span&gt; It is also useful to express &lt;span
class="math inline"&gt;\(d_\lambda\)&lt;/span&gt; in terms of the Legendre
transform &lt;span class="math inline"&gt;\(\tau\)&lt;/span&gt; of &lt;span
class="math inline"&gt;\(\kappa\)&lt;/span&gt;, as defined in
@ref(eq:LegendreTransform):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
d_\lambda(y,\mu)
=2\lambda\cdot\left[-\theta(\mu)^T(y-\mu)+\tau(y)-\tau(\mu)\right](\#eq:UnitDevianceLegendre)
\]&lt;/span&gt; Using the convexity of &lt;span
class="math inline"&gt;\(\kappa\)&lt;/span&gt;, it is easy to show that &lt;span
class="math inline"&gt;\(d_\lambda(y,\mu) \geq 0\)&lt;/span&gt; for all &lt;span
class="math inline"&gt;\(y\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\mu\)&lt;/span&gt;, and that &lt;span
class="math inline"&gt;\(d_\lambda(y,\mu) = 0\)&lt;/span&gt; requires &lt;span
class="math inline"&gt;\(\mu = y\)&lt;/span&gt;. The probability measure can be
expressed in terms of the unit deviance as:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text d P _{\lambda, \mu} (y) =
e^{-\frac{\lambda}{2}d(y,\,\mu)}e^{\lambda \tau(y)}\text dQ_\lambda
(y)(\#eq:EDvsDeviance).
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id="maximum-likelihood-estimation"&gt;Maximum Likelihood
Estimation&lt;/h2&gt;
&lt;p&gt;Let &lt;span class="math inline"&gt;\(Y_i\sim
\text{ED}(\lambda,\,\mu^{(0)}_ i ,\,\kappa)\)&lt;/span&gt; be independent for
&lt;span class="math inline"&gt;\(i=1,\,2,\,\dots,\,N\)&lt;/span&gt; and let &lt;span
class="math inline"&gt;\(M\subseteq \mathbb R ^N\)&lt;/span&gt; be a family of
models for the mean &lt;span class="math inline"&gt;\(\boldsymbol \mu ^{(0)} =
(\mu _1^{(0)},\,\mu_2^{(0)},\dots,\,\mu_N^{(0)})^T\)&lt;/span&gt; (in a GLM
context, &lt;span class="math inline"&gt;\(M\)&lt;/span&gt; would be the linear
subspace spanned by the covariates, &lt;span
class="math inline"&gt;\(\boldsymbol \mu _\beta = \mathbf X
\beta\)&lt;/span&gt;). From Eq. @ref(eq:EDvsDeviance), we see that the
likelihood of a model &lt;span class="math inline"&gt;\(\boldsymbol \mu \in
M\)&lt;/span&gt; is, modulo a &lt;span class="math inline"&gt;\(\boldsymbol
\mu\)&lt;/span&gt;-independent term, equal to its total deviance:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\log \mathcal L (\boldsymbol \mu,\,\lambda;\mathbf Y) =
-\frac{\lambda}{2}\mathcal D(\mathbf Y,\boldsymbol
\mu)+g_\lambda(\mathbf Y),(\#eq:LikelihoodVsDeviance)
\]&lt;/span&gt; with:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathcal D (\mathbf Y,\boldsymbol \mu)\equiv\sum_{i=1}^Nd(Y_i,\mu_i)
(\#eq:TotalDeviance)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence, the Maximum Likelihood Estimate (MLE) of &lt;span
class="math inline"&gt;\(\boldsymbol \mu\)&lt;/span&gt; corresponds to the
minimum deviance estimate:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat {\boldsymbol \mu}\equiv \arg \max _{\boldsymbol \mu \in M} \mathcal
L (\boldsymbol \mu,\,\lambda;\,\mathbf Y)=\arg \min _{\boldsymbol \mu
\in M} \mathcal D (\mathbf Y;\boldsymbol \mu).(\#eq:MLEisMDE)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In particular, the MLE &lt;span class="math inline"&gt;\(\hat {\boldsymbol
\mu}\)&lt;/span&gt; is obtained by minimizing a function of &lt;span
class="math inline"&gt;\(\boldsymbol \mu\)&lt;/span&gt; only, and is independent
on whether the dispersion parameter &lt;span
class="math inline"&gt;\(\lambda\)&lt;/span&gt; is being estimated itself or
not.&lt;/p&gt;
&lt;p&gt;These results are sometimes formulated in terms of a “saturated”
model &lt;span class="math inline"&gt;\(\boldsymbol \mu _\text{s} = \mathbf
Y\)&lt;/span&gt;. From Eq. @ref(eq:LikelihoodVsDeviance) we see that such a
model has likelihood equal to &lt;span
class="math inline"&gt;\(g_{\lambda}(\boldsymbol \mu)\)&lt;/span&gt;, implying
that:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\lambda\mathcal D(\mathbf Y,\boldsymbol \mu) = -2\log
\left(\frac{\mathcal L (\boldsymbol \mu,\lambda;\mathbf Y)}{\mathcal L
(\mathbf Y,\lambda;\mathbf Y)}\right) (\#eq:DevianceLogLik).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We note the asymptotic results &lt;span class="citation"&gt;(B. Jørgensen
1992, sec. 3.6)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\lambda \mathcal D(\mathbf Y,\,\hat {\boldsymbol
\mu})\overset{d}{\to}  \chi ^2 _{N-p} \qquad (\lambda \to
\infty)(\#eq:DevianceAsymptotics),\\
\]&lt;/span&gt; for a correctly specified model family &lt;span
class="math inline"&gt;\(M\)&lt;/span&gt; with &lt;span class="math inline"&gt;\(\dim
(M) = p\)&lt;/span&gt;, and:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\lambda \mathcal D(\mathbf Y,\,\hat {\boldsymbol \mu}_1)-\lambda
D(\mathbf Y,\,\hat {\boldsymbol \mu}_2)\overset{d}{\to} \chi ^2
_{p_2-p_1} \qquad (\lambda \to \infty \text { or } N\to
\infty)(\#eq:DevianceDiffAsymptotics),
\]&lt;/span&gt; for a correctly specified model family &lt;span
class="math inline"&gt;\(M_1\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(M_2
\supseteq M_1\)&lt;/span&gt;, with &lt;span class="math inline"&gt;\(p_i =\dim
(M_i)\)&lt;/span&gt;. Eq. @ref(eq:DevianceAsymptotics) can be seen as the
limiting case of @ref(eq:DevianceDiffAsymptotics) when &lt;span
class="math inline"&gt;\(M_2 = \mathbb R ^N\)&lt;/span&gt;, as in the saturated
model. The manifolds &lt;span class="math inline"&gt;\(M\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(M_i\)&lt;/span&gt; are not strictly required to be
linear subspaces of &lt;span class="math inline"&gt;\(\mathbb R^N\)&lt;/span&gt;,
because in the limits and under the null hypotheses implied by Eqs.
@ref(eq:DevianceAsymptotics) and @ref(eq:DevianceDiffAsymptotics) the
distributions of MLEs are concentrated around the true value &lt;span
class="math inline"&gt;\(\boldsymbol \mu ^{(0)}\)&lt;/span&gt;, so that the
manifolds &lt;span class="math inline"&gt;\(M\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(M_i\)&lt;/span&gt; can be effectivley approximated by
their tangent spaces.&lt;/p&gt;
&lt;p&gt;Noteworthy, limit @ref(eq:DevianceAsymptotics) holds in the small
dispersion limit &lt;span class="math inline"&gt;\(\lambda \to \infty\)&lt;/span&gt;
only, whereas limit @ref(eq:DevianceDiffAsymptotics) is also valid in
the large sample limit, essentially due to Wilks’ theorem.&lt;/p&gt;
&lt;h2 id="examples-of-edms"&gt;Examples of EDMs&lt;/h2&gt;
&lt;h4 id="univariate-gaussian"&gt;Univariate Gaussian&lt;/h4&gt;
&lt;p&gt;The univariate gaussian family &lt;span class="math inline"&gt;\(N (\mu,
\sigma^2)\)&lt;/span&gt;, with probability density function (PDF):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
f_{\mu,\sigma}(y)=\frac{1}{\sqrt {2 \pi \sigma
^2}}\exp\left[-\frac{(y-\mu)^2}{2\sigma ^2}\right] (\#eq:GaussianPDF)
\]&lt;/span&gt; corresponds to the reproductive EDM &lt;span
class="math inline"&gt;\(\text{ED}(\lambda, \,\theta,\,\kappa)\)&lt;/span&gt;
with:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\kappa (\theta) = \frac{\theta ^2}{2},\quad \theta \in \mathbb R,\quad
\lambda \in \mathbb R^+.(\#eq:GaussianEDM)
\]&lt;/span&gt; The correspondence is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\theta = \mu,\quad \lambda =
\frac{1}{\sigma^2}.(\#eq:GaussianIdentification)
\]&lt;/span&gt; The base probability measure is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text d Q_\lambda (y)=\sqrt{\frac \lambda {2\pi}}e^{-\lambda y^2/2}
\text dy(\#eq:GaussianBaseMeasure)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Legendre transform of &lt;span class="math inline"&gt;\(\kappa\)&lt;/span&gt;
is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\tau(\mu) = \frac{\mu ^2}{2} (\#eq:GaussianTau)
\]&lt;/span&gt; and the unit deviance reads:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
d(y, \hat \mu) =(y-\hat \mu)^2.(\#eq:GaussianDeviance)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id="binomial"&gt;Binomial&lt;/h4&gt;
&lt;p&gt;The binomial family &lt;span class="math inline"&gt;\(\mathcal
B(p,N)\)&lt;/span&gt; with probability mass function (PMF):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
f_{p,N}(z) = \binom{N}{z} p^z(1-p)^{N-z}(\#eq:BinomialPMF)
\]&lt;/span&gt; corresponds to the additive EDM &lt;span
class="math inline"&gt;\(\text{ED}^*(\lambda, \,\theta;\,\kappa)\)&lt;/span&gt;
with:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\kappa(\theta) = \ln (\dfrac{1+e^\theta}{2}),\quad \theta\in \mathbb R
,\quad \lambda \in \mathbb N.(\#eq:BinomialEDM)
\]&lt;/span&gt; The correspondence is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\theta = \ln\frac{p}{1-p},\quad\lambda =N.(\#eq:BinomialIdentification)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The base probability measure reads:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\frac{\text d Q_\lambda^*(z)}{\text d z} =2^{-\lambda}\sum_{i=0}
^\lambda \binom{\lambda}{i} \delta (z-i) (\#eq:BinomialBaseMeasure).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Legendre transform of &lt;span class="math inline"&gt;\(\kappa\)&lt;/span&gt;
(using &lt;span class="math inline"&gt;\(p\)&lt;/span&gt; for the mean parameter)
is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\tau(p)= \ln2+p\ln p+(1-p)\ln(1-p)(\#eq:BinomialTau)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and the unit deviance for the &lt;em&gt;reproductive&lt;/em&gt; EDM:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
d(y,\hat p)=-2y\ln (\dfrac{\hat p}{y})-2(1-y)\ln (\dfrac{1-\hat
p}{1-y}).(\#eq:BinomialDeviance)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the additive EDM, appropriate to an integer valued binomial
variable, this is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
d(z,\hat p)=-2\frac{z}{N}\ln (\dfrac{N\hat p}{z})-2(1-\frac{z}{N})\ln
(\dfrac{N-N\hat p}{N-z}).(\#eq:BinomialDevianceBis)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id="multinomial"&gt;Multinomial&lt;/h4&gt;
&lt;p&gt;The multinomial family &lt;span class="math inline"&gt;\(\text{Mult}
_{K+1}(p_1,\,p_2,\dots ,p_{K+1},\,N)\)&lt;/span&gt; for &lt;span
class="math inline"&gt;\(K+1\)&lt;/span&gt; categories is given by the PMF:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
f_{\boldsymbol p ,N}(z) = \binom{N}{z_1,\,z_2,\,\dots,\,z_{K+1}}\prod
_{k=1}^{K+1}p_k^{z_k}(\#eq:MultPMF),
\]&lt;/span&gt; In order to identify this with an EDM, we use the constraints
&lt;span class="math inline"&gt;\(\sum _{i=1}^{M+1}z_i =1\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\sum _{i=1}^{M+1}p_i =1\)&lt;/span&gt; to eliminate one
dependent variable and parameter, say &lt;span
class="math inline"&gt;\(z_{M+1}\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(p_{M+1}\)&lt;/span&gt;, respectively. The family of
densities for the resulting &lt;span
class="math inline"&gt;\(M\)&lt;/span&gt;-dimensional vector &lt;span
class="math inline"&gt;\(\boldsymbol z=(z_1\,z_2\,\dots\,z_M)^T\)&lt;/span&gt;
corresponds to the additive EDM &lt;span
class="math inline"&gt;\(\text{ED}^*(\lambda, \,\theta;\,\kappa)\)&lt;/span&gt;
with:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\quad \kappa(\theta) = \ln(\dfrac{1+\sum_{i=1}^K e^{\theta _k}}{K+1}),
\quad \theta \in \mathbb R^K,\quad \lambda \in \mathbb N(\#eq:MultEDM),
\]&lt;/span&gt; the correspondence being given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\theta _i = \ln \frac{p_i}{p_{K+1}},\quad \lambda =
N.(\#eq:MultIdentification)
\]&lt;/span&gt; The base measure:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\frac{\text d Q_\lambda^*(z)}{\text d \boldsymbol z}
=(K+1)^{-\lambda}\sum_{\boldsymbol i\in \mathbb N ^{K}\,\colon \,\sum
_{k=1}^{K}i_k\leq\lambda} \binom{\lambda}{i_1,\,i_2,\dots,i_{K+1}}
\delta (\boldsymbol z-\boldsymbol i), (\#eq:MultBaseMeasure)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(i_{K+1} = N - \sum _{k=1}
^{K} i_k\)&lt;/span&gt;. The Legendre transform of &lt;span
class="math inline"&gt;\(\kappa\)&lt;/span&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\tau(\boldsymbol p)= \ln (K+1)+\sum _{k=1}^{K+1}p_k\ln p_k(\#eq:MultTau)
\]&lt;/span&gt; and deviance is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
d(y,\hat {\boldsymbol p}) = -2\sum _{k=1}^{K+1}y_k\ln (\frac{\hat
p_k}{y_k}).(\#eq:MultDeviance)
\]&lt;/span&gt; For the additive EDM, appropriate to an integer valued
multinomial variable, this is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
d(z,\hat p)=-2\sum _{k=1}^{K+1}\frac{z_k}{N}\ln (\dfrac{N\hat
p}{z_k}).(\#eq:MultDevianceBis)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id="poisson"&gt;Poisson&lt;/h4&gt;
&lt;p&gt;The Poisson PMF is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
f(y) = \frac {\nu ^z} {z!} e^{-\nu}(\#eq:PoissonPMF)
\]&lt;/span&gt; This can be interpreted as coming from an additive EDM
with:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\kappa(\theta)=e^\theta-1,\quad \theta \in \mathbb R,\quad \lambda \in
\mathbb R^+(\#eq:PoissonEDM).
\]&lt;/span&gt; However, the correspondence is not unique, being given by the
single relation:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\lambda e^\theta = \nu (\#eq:PoissonIdentification)
\]&lt;/span&gt; which describes a curve in the &lt;span
class="math inline"&gt;\(\Theta \times \Lambda\)&lt;/span&gt; space. The
corresponding base measure is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\dfrac{\text d Q _\lambda (z)}{\text d z} = e^{-\lambda}\sum
_{k=0}^{\infty}\frac{\lambda ^k\delta(z-k)}{k!}(\#eq:PoissonBaseMeasure)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is nothing but the Poisson measure itself.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;I have mostly followed &lt;span class="citation"&gt;(Bent Jørgensen
1987)&lt;/span&gt;. References &lt;span class="citation"&gt;(B. Jørgensen
1992)&lt;/span&gt; &lt;span class="citation"&gt;(Jorgensen 1997)&lt;/span&gt; from the
same author provide more extensive expositions. A good reference for
GLMs is &lt;span class="citation"&gt;(McCullagh 2019)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references csl-bib-body hanging-indent"&gt;
&lt;div id="ref-jorgensen1997theory" class="csl-entry"&gt;
Jorgensen, Bent. 1997. &lt;em&gt;The Theory of Dispersion Models&lt;/em&gt;. CRC
Press.
&lt;/div&gt;
&lt;div id="ref-jorgensen1992theory" class="csl-entry"&gt;
Jørgensen, B. 1992. &lt;em&gt;The Theory of Exponential Dispersion Models and
Analysis of Deviance&lt;/em&gt;. Monografias de Matem&lt;span&gt;á&lt;/span&gt;tica. IMPA.
&lt;a
href="https://books.google.es/books?id=twN3vgAACAAJ"&gt;https://books.google.es/books?id=twN3vgAACAAJ&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-jorgensen1987exponential" class="csl-entry"&gt;
Jørgensen, Bent. 1987. &lt;span&gt;“Exponential Dispersion Models.”&lt;/span&gt;
&lt;em&gt;Journal of the Royal Statistical Society: Series B
(Methodological)&lt;/em&gt; 49 (2): 127–45.
&lt;/div&gt;
&lt;div id="ref-mccullagh2019generalized" class="csl-entry"&gt;
McCullagh, Peter. 2019. &lt;em&gt;Generalized Linear Models&lt;/em&gt;. Routledge.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes footnotes-end-of-document"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Whether a set of moments determines a unique probability
measure is called the &lt;a
href="https://en.wikipedia.org/wiki/Hamburger_moment_problem"&gt;Hamburger
moment problem&lt;/a&gt;.&lt;a href="#fnref1"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5>b23eab3a239ec49cf932769538e2531d</distill:md5>
      <guid>https://vgherard.github.io/notebooks/exponential-dispersion-models</guid>
      <pubDate>Thu, 07 Mar 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Bootstrap</title>
      <dc:creator>Valerio Gherardi</dc:creator>
      <link>https://vgherard.github.io/notebooks/bootstrap</link>
      <description>Bootstrap</description>
      <guid>https://vgherard.github.io/notebooks/bootstrap</guid>
      <pubDate>Wed, 07 Feb 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Ordinary Least Squares</title>
      <dc:creator>Valerio Gherardi</dc:creator>
      <link>https://vgherard.github.io/notebooks/ordinary-least-squares</link>
      <description>


&lt;h1 id="generalities"&gt;Generalities&lt;/h1&gt;
&lt;p&gt;Ordinary Least Squares (OLS) is a regression algorithm for estimating
the best linear predictor (BLP) &lt;span class="math inline"&gt;\(f^*(X) =
X\beta\)&lt;/span&gt; of a response &lt;span class="math inline"&gt;\(Y \in \mathbb
R\)&lt;/span&gt; in terms of a vector of regressors &lt;span
class="math inline"&gt;\(X\in \mathbb R ^p\)&lt;/span&gt;, which we will
frequently identify with an &lt;span class="math inline"&gt;\(1\times
p\)&lt;/span&gt; row matrix. Here, “best” is understood in terms of the &lt;span
class="math inline"&gt;\(L_2\)&lt;/span&gt; error:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\beta = \arg \min _{\beta &amp;#39;}  \mathbb E[(Y - X\beta
^\prime)^2]=\mathbb E (X^TX)^{-1}\mathbb E(X^T Y), (\#eq:BLP)
\]&lt;/span&gt; where the first equation is the defining one, while the second
one follows from elementary calculus.&lt;/p&gt;
&lt;p&gt;Given i.i.d. data &lt;span
class="math inline"&gt;\(\{(X_i,\,Y_i)\}_{i=1,\,2,\,\dots,\,N}\)&lt;/span&gt;,
and denoting by &lt;span class="math inline"&gt;\(\mathbf Y\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\mathbf X\)&lt;/span&gt; the &lt;span
class="math inline"&gt;\(N\times 1\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(N\times p\)&lt;/span&gt; matrices obtained by vertically
stacking independent observations of &lt;span
class="math inline"&gt;\(Y\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt;, respectively, the OLS estimate of
@ref(eq:BLP) is defined by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat \beta = \arg \min _{\beta &amp;#39;}  \sum _{i=1} ^N \frac{1}{N}(Y_i -
X_i\beta ^\prime)^2 = (\mathbf X ^T \mathbf X) ^{-1} \mathbf X ^T
\mathbf Y, (\#eq:OLS)
\]&lt;/span&gt; which is readily recognized to be the plugin estimate of &lt;span
class="math inline"&gt;\(\beta\)&lt;/span&gt;. Correspondingly, we define the OLS
predictor:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat Y (x) = x \hat \beta. (\#eq:OLSPredictor)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What motivates the use of an &lt;span class="math inline"&gt;\(L_2\)&lt;/span&gt;
criterion in @ref(eq:BLP)?&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Mathematical tractability.&lt;/em&gt; The fact that @ref(eq:BLP)
admits a closed form solution, which is furthermore linear in the
response variable &lt;span class="math inline"&gt;\(Y\)&lt;/span&gt;, greatly
simplifies the analysis of the properties of &lt;span
class="math inline"&gt;\(\beta\)&lt;/span&gt; and its estimators such as the OLS
one @ref(eq:OLS), making it a perfect study case.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Numerical tractability.&lt;/em&gt; A consequence of the previous
point, but worth a separate mention. Computing the plugin estimate in
@ref(eq:OLS) is just a matter of basic linear algebra manipulations,
which, with modern software libraries, is a relatively cheap
operation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Normal theory.&lt;/em&gt; Focusing on the consequence of Eq.
@ref(eq:BLP), namely the plugin estimate @ref(eq:OLS), if the
conditional distribution of &lt;span class="math inline"&gt;\(Y\)&lt;/span&gt; given
&lt;span class="math inline"&gt;\(X\)&lt;/span&gt; is normal with constant variance,
and if &lt;span class="math inline"&gt;\(\mathbb E(Y\vert X)\)&lt;/span&gt; is truly
linear, &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; coincides with the
maximum likelihood estimate of &lt;span
class="math inline"&gt;\(\beta\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="the-linear-model"&gt;The linear model&lt;/h2&gt;
&lt;p&gt;The term generally refers to a model for the conditional distribution
of &lt;span class="math inline"&gt;\(Y\vert X\)&lt;/span&gt; that requires the
conditional mean &lt;span class="math inline"&gt;\(\mathbb E(Y\vert
X)\)&lt;/span&gt; to be a linear function of &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt;. In its most parsimonious form, this is
just:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
Y = X \beta + \varepsilon, \quad \mathbb E(\varepsilon\vert X) =
0.(\#eq:LinearModel)
\]&lt;/span&gt; That said, depending on context, @ref(eq:LinearModel) is
usually supplemented with additional assumptions that further
characterise the conditional distribution of the error term&lt;a
href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, typically
(with increasing strength of assumptions):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Constant Variance.&lt;/em&gt; &lt;span class="math inline"&gt;\(\mathbb
V(\varepsilon \vert X) = \sigma ^2\)&lt;/span&gt;, independently of &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;span class="math inline"&gt;\(X\)&lt;/span&gt;-Independent Errors.&lt;/em&gt;
&lt;span class="math inline"&gt;\(\varepsilon \perp X\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Normal Errors.&lt;/em&gt; &lt;span class="math inline"&gt;\(\varepsilon
\vert X \sim \mathcal N (0,\sigma ^2)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the OLS estimator is well-defined irrespective of the validity
of any of these models, it is clear that, in order for &lt;span
class="math inline"&gt;\(\hat \beta\)&lt;/span&gt; to represent a meaningful
summary of the &lt;span class="math inline"&gt;\(Y\)&lt;/span&gt;-&lt;span
class="math inline"&gt;\(X\)&lt;/span&gt; dependence, one should require at least
@ref(eq:LinearModel) to hold in some approximate sense. Correspondingly,
while some general features of &lt;span class="math inline"&gt;\(\hat
\beta\)&lt;/span&gt; can be discussed independently of linear model
assumptions, its most important properties crucially depend on Eq.
@ref(eq:LinearModel).&lt;/p&gt;
&lt;h1 id="properties"&gt;Properties&lt;/h1&gt;
&lt;h2 id="algebraic-properties-of-blp-and-ols-estimates"&gt;Algebraic
properties of BLP and OLS estimates&lt;/h2&gt;
&lt;p&gt;Consider the BLP &lt;span class="math inline"&gt;\(f^*(X) =
X\beta\)&lt;/span&gt;, with &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; defined
as in Eq. @ref(eq:BLP). We usually assume that the covariate vector
&lt;span class="math inline"&gt;\(X\)&lt;/span&gt; contains an intercept term, that
is &lt;span class="math display"&gt;\[
X=\begin{pmatrix}1 &amp;amp; Z\end{pmatrix},\quad Z\in \mathbb R^{p-1}
(\#eq:XtoZ)
\]&lt;/span&gt; for some &lt;span class="math inline"&gt;\(p-1\)&lt;/span&gt; dimensional
random vector &lt;span class="math inline"&gt;\(Z\)&lt;/span&gt;. The presence of an
intercept term leads to &lt;span class="math inline"&gt;\(f^*(X)\)&lt;/span&gt;
having a bunch of nice properties, such as being unconditionally
unbiased (&lt;span class="math inline"&gt;\(\mathbb E(Y-f^*(X))=0\)&lt;/span&gt;),
as we show below.&lt;/p&gt;
&lt;p&gt;Let us decompose &lt;span class="math inline"&gt;\(\beta = \begin{pmatrix}a
&amp;amp; b\end{pmatrix}\)&lt;/span&gt;, where &lt;span class="math inline"&gt;\(a\in
\mathbb R\)&lt;/span&gt; is the intercept term, and &lt;span
class="math inline"&gt;\(b \in \mathbb R^{p-1}\)&lt;/span&gt; is the coefficient
of &lt;span class="math inline"&gt;\(Z\)&lt;/span&gt;. We can easily prove that:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
b  &amp;amp;=
\mathbb V (Z)^{-1}\mathbb V(Z,Y),\\
a  &amp;amp;= \mathbb E(Y)-\mathbb E(Z)b,
\end{split}(\#eq:BLP0)
\]&lt;/span&gt; where we denote by &lt;span class="math inline"&gt;\(\mathbb V
(A,B)\)&lt;/span&gt; the covariance matrix of &lt;span
class="math inline"&gt;\(A\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(B\)&lt;/span&gt;, and by &lt;span
class="math inline"&gt;\(\mathbb V (A)\equiv \mathbb V (A,A)\)&lt;/span&gt;.
These expressions can be used to recast the error term as:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
Y-X\beta = (Y-\mathbb E(Y))-(Z-\mathbb E(Z))\mathbb V (Z)^{-1}\mathbb
V(Z,Y)(\#eq:BLPErrorTerm).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From this expression we can easily find the first two moments:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
\mathbb E(Y-X\beta)&amp;amp;=0,\\
\mathbb E((Y-X\beta)^2)&amp;amp;=\mathbb V(Y)-\mathbb V(Z,Y)^T\mathbb
V(Z)^{-1}\mathbb V(Z,Y),
\end{split}(\#eq:BLPExpectedError)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In particular, as anticipated the best linear predictor is
unconditionally unbiased. More generally, from Eq. @ref(eq:BLPErrorTerm)
it follows one has:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E(X^T(Y-X\beta))=0(\#eq:OrthogonalityErrorTerm),
\]&lt;/span&gt; which, since &lt;span class="math inline"&gt;\(\mathbb E(Y-X\beta) =
0\)&lt;/span&gt;, can also be interpreted as saying that the error term &lt;span
class="math inline"&gt;\(Y-X\beta\)&lt;/span&gt; and the covariate vector &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt; are uncorrelated.&lt;/p&gt;
&lt;p&gt;These properties directly translate to corresponding properties of
the empirical residuals &lt;span class="math inline"&gt;\(\mathbf Y-\mathbf
X\hat \beta\)&lt;/span&gt;. Notice that, up to now no result depends on the
particular probability measure to which expectations refer to. In
particular, we can choose this measure to be the empirical distribution
realized in a specific sample, which amounts to replace all expectations
with sample means. Thus, Eq. @ref(eq:OrthogonalityErrorTerm) translates
to:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\frac{1}{N}\mathbf X^T(\mathbf Y-\mathbf X\hat
\beta)=0(\#eq:OrthogonalitySampleResiduals),
\]&lt;/span&gt; which implies in particular that sample residuals have
vanishing sample means.&lt;/p&gt;
&lt;h2 id="distribution-of-coefficient-estimates-hat-beta"&gt;Distribution of
coefficient estimates &lt;span class="math inline"&gt;\(\hat
\beta\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;As an estimator of &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; as
defined in Eq. @ref(eq:BLP), the OLS estimator &lt;span
class="math inline"&gt;\(\hat \beta\)&lt;/span&gt; is consistent (converges in
probability to &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt;) but generally
biased (an example is provided &lt;a
href="https://vgherard.github.io/posts/2023-05-12-consistency-and-bias-of-ols-estimators/"&gt;here&lt;/a&gt;).
However, due to the plugin nature of &lt;span class="math inline"&gt;\(\hat
\beta\)&lt;/span&gt;, the bias is generally of order &lt;span
class="math inline"&gt;\(\mathcal O (N^{-1})\)&lt;/span&gt;, which makes it often
negligible in comparison to its &lt;span class="math inline"&gt;\(\mathcal
O(N^{-1/2})\)&lt;/span&gt; sampling variability (see below).&lt;/p&gt;
&lt;p&gt;Explicitly, the bias is given by: &lt;span class="math display"&gt;\[
\mathbb E(\hat \beta) - \beta = \mathbb E\lbrace((\mathbf X ^T \mathbf
X) ^{-1}-\mathbb E[(\mathbf X ^T \mathbf X) ^{-1}])\cdot \mathbf X ^T f
(\mathbf X)\rbrace, (\#eq:BiasOLS)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(f(X) \equiv \mathbb E(Y
\vert X)\)&lt;/span&gt; is the true conditional mean function. In general,
this vanishes only if &lt;span class="math inline"&gt;\(f(X)=X\beta\)&lt;/span&gt;,
as in the linear expectation model @ref(eq:LinearModel).&lt;/p&gt;
&lt;p&gt;The &lt;span class="math inline"&gt;\(\mathbf X\)&lt;/span&gt;-conditional
variance of &lt;span class="math inline"&gt;\(\hat \beta\)&lt;/span&gt; can be
derived directly from Eq. @ref(eq:OLS):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb V (\hat \beta \vert \mathbf X)=(\mathbf X ^T \mathbf X) ^{-1}
\mathbf X ^T  \mathbb V (\mathbf Y\vert \mathbf X) \mathbf X (\mathbf X
^T \mathbf X), (\#eq:VarBetaHatCond)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(\mathbb V (\mathbf Y\vert
\mathbf X)\)&lt;/span&gt; is diagonal for i.i.d. observations. For
homoskedastic errors we get:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb V (\hat \beta \vert \mathbf X)=(\mathbf X ^T \mathbf X) ^{-1}
\sigma ^2 \quad (\mathbb V(Y\vert X)=\sigma
^2).\  (\#eq:VarBetaHatCondHomo)
\]&lt;/span&gt; Under the normal linear model, this allows to obtain
finite-sample correct confidence sets for &lt;span
class="math inline"&gt;\(\beta\)&lt;/span&gt;. In the general case, confidence
sets can be derived from the Central Limit Theorem satisfied by &lt;span
class="math inline"&gt;\(\hat \beta\)&lt;/span&gt; &lt;span class="citation"&gt;(Buja
et al. 2019)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\sqrt N (\hat \beta -\beta) \to \mathcal N (0,V ) (\#eq:CLTBetaHat)
\]&lt;/span&gt; where the asymptotic variance is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
V = \mathbb E[X^TX] ^{-1} \cdot \mathbb E[X^T(Y-X\beta)^2X] \cdot
\mathbb E[X^TX] ^{-1}.(\#eq:AsyVarBetaHat)
\]&lt;/span&gt; The plugin estimate of @ref(eq:AsyVarBetaHat) leads to the so
called Sandwich variance estimator:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
V_\text{sand} \equiv  (\mathbf X^T \mathbf X)^{-1} \mathbf X^T D_\mathbf
{r^2}\mathbf X (\mathbf X^T \mathbf X)^{-1},(\#eq:SandwichEstimate)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(D_{\mathbf r^2}\)&lt;/span&gt; is
the diagonal matrix whose &lt;span class="math inline"&gt;\(i\)&lt;/span&gt;-th
entry is the squared residual &lt;span class="math inline"&gt;\(r_i ^2 =
(Y_i-X_i\beta)^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id="variance-estimates"&gt;Variance estimates&lt;/h2&gt;
&lt;p&gt;These can be based on:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
s ^2 = \frac{1}{N}(\mathbf Y-\mathbf X \hat \beta)^T(\mathbf Y-\mathbf X
\hat \beta)
\]&lt;/span&gt; which has expectation&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E\left[s^2\right]=\frac{1}{N}\text {Tr}\,\mathbb E \left[
(1-\mathbf H) \cdot \left(\mathbb E(\mathbf Y \vert \mathbf X)\mathbb
E(\mathbf Y \vert \mathbf X)^T + \mathbb V(\mathbf Y\vert\mathbf
X)\right) \right] (\#eq:ExpectedInSampleError)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the hat matrix &lt;span class="math inline"&gt;\(\mathbf H\)&lt;/span&gt;
is defined as usual:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbf H \equiv \mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf
X^T(\#eq:HatMatrix)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the general linear model @ref(eq:LinearModel) holds, so that &lt;span
class="math inline"&gt;\(E(\mathbf Y \vert \mathbf X) = \mathbf X
\beta\)&lt;/span&gt;, we have &lt;span class="math inline"&gt;\((1-\mathbf H)
\mathbb E(\mathbf Y \vert \mathbf X) = 0\)&lt;/span&gt;. If we furthermore
assume homoskedasticity, we obtain:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E\left[s^2\right]=\frac{N-p}{N}\sigma^2
\quad(\text{Homoskedastic linear model}), (\#eq:VarianceEstimateHomo)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(p = \text {Tr}(\mathbf
H)\)&lt;/span&gt; is the number of independent covariates. On the other hand,
if homoskedasticity holds, but &lt;span class="math inline"&gt;\(E(\mathbf Y
\vert \mathbf X)\)&lt;/span&gt; is not linear, the left-hand side of the
previous equation is an overestimate of &lt;span
class="math inline"&gt;\(\mathbf V \vert X\)&lt;/span&gt;.&lt;/p&gt;
&lt;h1 id="measurement-error"&gt;Measurement error&lt;/h1&gt;
&lt;p&gt;Suppose that, rather than the variables of interest &lt;span
class="math inline"&gt;\(Y\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt;, we can only observe noisy
versions:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\widetilde Y = Y+\delta_Y,\qquad \widetilde Z=Z+\delta
_Z(\#eq:NoisyYandZ)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class="math inline"&gt;\(X=\begin{pmatrix}1 &amp;amp;
Z\end{pmatrix}\)&lt;/span&gt; as in Eq. @ref(eq:XtoZ). We are truly interested
in &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; defined by Eq.
@ref(eq:BLP), but OLS will actually estimate &lt;span
class="math inline"&gt;\(\widetilde \beta =\begin{pmatrix}\widetilde a
&amp;amp; \widetilde b\end{pmatrix}\)&lt;/span&gt;, the coefficient of the BLP of
&lt;span class="math inline"&gt;\(\widetilde Y\)&lt;/span&gt; in terms of &lt;span
class="math inline"&gt;\(\widetilde X\)&lt;/span&gt;. Focusing on the slope term
&lt;span class="math inline"&gt;\(\widetilde b\)&lt;/span&gt;, a comparison with Eq.
@ref(eq:BLP0) yields:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
\widetilde b -b&amp;amp;= \Delta_{\mathbb V(Z)^{-1}}\cdot\mathbb V(Z,Y)
+\mathbb V(Z)^{-1} \cdot \Delta _{\mathbb V(Z,Y)}+\Delta_{\mathbb
V(Z)^{-1}}\cdot\Delta _{\mathbb V(Z,Y)},\\
\Delta_{\mathbb V(Z)^{-1}}&amp;amp;=\mathbb V (\widetilde Z)^{-1}-V(Z)^{-1},
\\
\Delta _{\mathbb V(Z,Y)} &amp;amp;=\mathbb V(\widetilde Z,\widetilde
Y)-\mathbb V(Z,Y).
\end{split}(\#eq:Deltab)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where, explicitly:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
\mathbb V(\widetilde Z)&amp;amp;=\mathbb V(Z)+\mathbb V(\delta _Z)+2\mathbb
V(Z,\delta _Z),\\
\mathbb V(\widetilde Z,\widetilde Y)&amp;amp;=\mathbb V(Z,Y)+\mathbb
V(\delta _Z,Y)+\mathbb V(\delta _Y,Z)+\mathbb V(\delta _Y,\delta _Z).
\end{split}(\#eq:DeltaVvsNoise)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The effect on the slope estimate generally depends on the correlation
structure of signal and noise. In the special case of “totally random”
noise, that is if &lt;span class="math inline"&gt;\(\delta _Z\)&lt;/span&gt; and
&lt;span class="math inline"&gt;\(\delta _Y\)&lt;/span&gt; are independent of each
other and of &lt;span class="math inline"&gt;\(Z\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(Y\)&lt;/span&gt;, we see that the only effect is an
increase &lt;span class="math inline"&gt;\(\mathbb V(\widetilde Z) = \mathbb
V(Z)+ \mathbb V(\delta _Z)\)&lt;/span&gt;, which shrinks the slope coefficient
&lt;span class="math inline"&gt;\(\widetilde b\)&lt;/span&gt; towards zero. In
particular, if random noise is only present in the response (&lt;span
class="math inline"&gt;\(\delta _Z = 0\)&lt;/span&gt;), the estimation target is
unaffected (although the variance of the $b $ estimator is, in
general).&lt;/p&gt;
&lt;h1 id="proofs"&gt;Proofs&lt;/h1&gt;
&lt;h2 id="proof-of-central-limit-theorem-for-hat-beta"&gt;Proof of Central
Limit Theorem for &lt;span class="math inline"&gt;\(\hat \beta\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Convergence to the normal distribution @ref(eq:CLTBetaHat) with
variance @ref(eq:AsyVarBetaHat) can be proved using the formalism of
influence functions. From Eq. @ref(eq:BLP), we see that a small
variation &lt;span class="math inline"&gt;\(P\to P+\delta P\)&lt;/span&gt; to the
joint &lt;span class="math inline"&gt;\(XY\)&lt;/span&gt; probability measure
induces a first order shift:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\delta \beta =  \intop \mathbb E(X^TX)^{-1}X^T (Y-\beta X) \text
d(\delta P)(\#eq:BetaDifferential)
\]&lt;/span&gt; in the best linear predictor. The influence function of &lt;span
class="math inline"&gt;\(\beta\)&lt;/span&gt; is defined by the measurable
representation of &lt;span class="math inline"&gt;\(\delta \beta\)&lt;/span&gt;,
namely:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\phi _\beta = \mathbb E(X^TX)^{-1}X^T (Y-\beta X).
(\#eq:BetaInfluenceFunction)
\]&lt;/span&gt; A general result for plugin estimates then tells us that &lt;span
class="math inline"&gt;\(\sqrt N (\hat \beta -\beta) \to \mathcal N (0,
\mathbb E (\phi _\beta ^2))\)&lt;/span&gt; in distribution, and using the
explicit form of @ref(eq:BetaInfluenceFunction) we readily obtain
@ref(eq:AsyVarBetaHat).&lt;/p&gt;
&lt;h1 id="related-posts"&gt;Related posts&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a
href="https://vgherard.github.io/posts/2023-05-12-consistency-and-bias-of-ols-estimators/"&gt;Consistency
and bias of OLS estimators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a
href="https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches/"&gt;Model
misspecification and linear sandwiches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a
href="https://vgherard.github.io/posts/2023-05-20-linear-regression-with-autocorrelated-noise/"&gt;Linear
regression with autocorrelated noise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a
href="https://vgherard.github.io/posts/2023-07-11-testing-functional-specification-in-linear-regression/"&gt;Testing
functional specification in linear regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references csl-bib-body hanging-indent"&gt;
&lt;div id="ref-buja2019models" class="csl-entry"&gt;
Buja, Andreas, Richard Berk, Lawrence Brown, Edward George, Emil Pitkin,
Mikhail Traskin, Linda Zhao, and Kai Zhang. 2019. &lt;span&gt;“Models as
Approximations i: Consequences Illustrated with Linear
Regression.”&lt;/span&gt; &lt;a
href="https://arxiv.org/abs/1404.1578"&gt;https://arxiv.org/abs/1404.1578&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes footnotes-end-of-document"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;I use the notation &lt;span class="math inline"&gt;\(A\perp
B\)&lt;/span&gt; to indicate that the random variables &lt;span
class="math inline"&gt;\(A\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(B\)&lt;/span&gt; are statistically independent.&lt;a
href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5>639b544ad1bc9e40504b8cb76a6eccf1</distill:md5>
      <guid>https://vgherard.github.io/notebooks/ordinary-least-squares</guid>
      <pubDate>Tue, 06 Feb 2024 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
