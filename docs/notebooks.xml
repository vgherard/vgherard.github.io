<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>vgherard</title>
    <link>https://vgherard.github.io/</link>
    <atom:link href="https://vgherard.github.io/notebooks.xml" rel="self" type="application/rss+xml"/>
    <description>Valerio Gherardi's Personal Website
</description>
    <generator>Distill</generator>
    <lastBuildDate>Wed, 13 Mar 2024 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Maximum Likelihood</title>
      <dc:creator>Valerio Gherardi</dc:creator>
      <link>https://vgherard.github.io/notebooks/maximum-likelihood</link>
      <description>


&lt;p&gt;&lt;strong&gt;Disclaimer.&lt;/strong&gt; These are wild notes on Maximum
Likelihood that require some deep &lt;em&gt;labor limae&lt;/em&gt; session. Use at
your own risk!&lt;/p&gt;
&lt;p&gt;Let &lt;span class="math inline"&gt;\(\mathcal Q \equiv\{\text d Q_{\theta}
= q_\theta \,\text d \mu\}_{\theta \in \Theta}\)&lt;/span&gt; be a parametric
family of probability measures dominated by some common measure &lt;span
class="math inline"&gt;\(\mu\)&lt;/span&gt;. Consider the functional&lt;a
href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\theta ^* (P) = \arg \min_{\theta \in \Theta} \intop \text dP\,\ln
\left(\frac{1}{q_\theta}\right) (\#eq:FunctionalThethaStar).
\]&lt;/span&gt; This is the parameter of the best (in the cross-entropy sense)
approximation of &lt;span class="math inline"&gt;\(P\)&lt;/span&gt; within &lt;span
class="math inline"&gt;\(\mathcal Q\)&lt;/span&gt;, which we assume to be
unique.&lt;/p&gt;
&lt;p&gt;If &lt;span class="math inline"&gt;\(P\)&lt;/span&gt; represents the true
probability distribution of the data under study, &lt;span
class="math inline"&gt;\(\theta ^*(P)\)&lt;/span&gt; is the target of ML
estimation, in the general case in which &lt;span
class="math inline"&gt;\(P\)&lt;/span&gt; is not necessarily in &lt;span
class="math inline"&gt;\(\mathcal Q\)&lt;/span&gt;. The ML estimate &lt;span
class="math inline"&gt;\(\hat \theta _N\)&lt;/span&gt; of &lt;span
class="math inline"&gt;\(\theta^*\)&lt;/span&gt; from an i.i.d. sample of &lt;span
class="math inline"&gt;\(N\)&lt;/span&gt; observations is&lt;a href="#fn2"
class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat \theta _N \equiv \theta ^*(\hat P _N)=\arg \max_{\theta \in \Theta}
\sum_{i=1}^N \ln ({q_\theta(Y_i)}), (\#eq:ThetaMLE)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(\hat P _N\)&lt;/span&gt; is the
empirical distribution of the sample.&lt;/p&gt;
&lt;p&gt;Denoting:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
c_{P}(\theta) = \intop \text dP\,\ln
\left(\frac{1}{q_\theta}\right),(\#eq:CrossEntIntegral)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;we see that &lt;span class="math inline"&gt;\(\theta^*\)&lt;/span&gt; is
determined by the condition &lt;span
class="math inline"&gt;\(c_{P}&amp;#39;(\theta^*)=0\)&lt;/span&gt;. From this, we can
easily derive the first order variation of &lt;span
class="math inline"&gt;\(\theta ^*\)&lt;/span&gt; under a variation &lt;span
class="math inline"&gt;\(P \to P + \delta P\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\delta \theta ^* =\left(\intop \text dP\,I_{\theta ^*}
\right)^{-1}\left(\intop \text d(\delta P)u_{\theta
^*}\right)(\#eq:DifferentialThetaStar)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where we have defined:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
u_\theta = \frac{\partial }{\partial \theta} \ln q_\theta,\quad I_\theta
= -\frac{\partial^2 }{\partial \theta ^2}  \ln
q_\theta.(\#eq:ScoreFisherInfo)
\]&lt;/span&gt; From @ref(eq:DifferentialThetaStar) we can identify the
influence function of the &lt;span class="math inline"&gt;\(\theta ^*\)&lt;/span&gt;
functional:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\psi_P(y)=\left(\intop \text dP\,I_{\theta ^*} \right)^{-1}u_{\theta
^*}(y)(\#eq:InfluenceFunction)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then, from the standard theory of influence functions, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat \theta _N \approx \theta ^*+J ^{-1} U(\#eq:ThetaNFirstOrder)
\]&lt;/span&gt; where we have defined:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
J\equiv \intop \text dP\,I_{\theta ^*},\quad U\equiv\frac{1}{N}\sum
_{i=1}^Nu_{\theta ^*}(Y_i)(\#eq:ScoreFisherInfo2).
\]&lt;/span&gt; In particular, we obtain the Central Limit Theorem (CLT)&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\sqrt N(\hat \theta _N - \theta ^*) \overset{d}{\to} \mathcal N(0,
J^{-1}KJ^{-1}),(\#eq:ThetaCLT)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
K = \mathbb V(u_{\theta ^*}(Y)). (\#eq:ScoreVariance)
\]&lt;/span&gt; The matrices &lt;span class="math inline"&gt;\(K\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(J\)&lt;/span&gt; depend on the unknown value &lt;span
class="math inline"&gt;\(\theta ^*\)&lt;/span&gt;, but we can readily construct
plugin estimators:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat J_N = -\frac{1}{N}\sum _{i=1}^NI_{\hat \theta _N}(Y_i),\quad\hat
K_N = \frac{1}{N}\sum _{i=1}^Nu_{\hat \theta _N}(Y_i)u_{\hat \theta
_N}(Y_i)^T,(\#eq:PluginJK)
\]&lt;/span&gt; and estimate the variance of &lt;span class="math inline"&gt;\(\hat
\theta _N\)&lt;/span&gt; as:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\widehat {\mathbb V}(\hat \theta _N) = \frac{\hat J _N ^{-1}\hat K_N\hat
J_N ^{-1}}{N}(\#eq:SandwichEstimator),
\]&lt;/span&gt; which is the usual Sandwich estimator. Finally, if &lt;span
class="math inline"&gt;\(P = Q_{\theta^*}\)&lt;/span&gt;, then $J = K $, and the
CLT @ref(eq:ThetaCLT) becomes simply &lt;span class="math inline"&gt;\(\sqrt
N(\hat \theta _N - \theta ^*) \overset{d}{\to} \mathcal N(0,
J^{-1})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let us now consider the following expansion of &lt;span
class="math inline"&gt;\(c_P(\hat \theta _N)\)&lt;/span&gt; which, we recall, is
the cross-entropy of the ML model on the true distribution &lt;span
class="math inline"&gt;\(P\)&lt;/span&gt; (&lt;em&gt;cf.&lt;/em&gt;
@ref(eq:CrossEntIntegral)):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
c_P(\hat \theta _N)
    &amp;amp;= -\intop \text d P(y&amp;#39;)\,\ln (q_{\hat \theta}(y&amp;#39;))\\
    &amp;amp; \approx -\mathbb E&amp;#39;(\ln q_{\theta^*})+\frac{1}{2}(\hat
\theta-\theta ^*)^TJ (\hat \theta-\theta ^*)\\
    &amp;amp; \approx -\mathbb E&amp;#39;(\ln
q_{\theta^*})+\frac{1}{2}U^TJ^{-1}U
\end{split}
\]&lt;/span&gt; Taking the expectation with respect to the training dataset,
noting that &lt;span class="math inline"&gt;\(\mathbb E(U_{\theta ^*}U_{\theta
^*}^T)=K_{\theta ^*}\)&lt;/span&gt;, we get:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E (c_P(\hat \theta _N))\approx -\mathbb E&amp;#39;(\ln
q_{\theta^*})+\frac{1}{2N}\text {Tr}(J^{-1}K) (\#eq:CrossEntExp)
\]&lt;/span&gt; Now consider the in-sample estimate:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
c_{\hat P _N}(\hat \theta _N) &amp;amp;= -\frac{1}{N}\sum _{i=1}^N\ln
q_{\hat \theta}(Y_i)\\
&amp;amp; \approx - \frac{1}{N}\sum _{i=1} ^N \ln q_{\theta^*}(Y_i)-
U^T(\hat \theta _N-\theta^*)+ \frac{1}{2}(\hat \theta
_N-\theta^*)^TJ(\hat \theta _N-\theta^*)\\
&amp;amp; \approx - \frac{1}{N}\sum _{i=1} ^N \ln q_{\theta^*}(Y_i)- U^TJ
^{-1} U+ \frac{1}{2}U^TJ ^{-1}\hat J_N J^{-1}U\\
&amp;amp; \approx - \frac{1}{N}\sum _{i=1} ^N \ln q_{\theta^*}(Y_i)-
\frac{1}{2}U^TJ ^{-1} U.
\end{split}
\]&lt;/span&gt; Taking the expectation:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E (c_{\hat P _N}(\hat \theta _N)) = -\mathbb E&amp;#39;(\ln
q_{\theta^*})-\frac{1}{2N}\text{Tr}(J^{-1}K)(\#eq:InSampleCrossEntExp)
\]&lt;/span&gt; Comparing Eqs. @ref(eq:InSampleCrossEntExp) and
@ref(eq:CrossEntExp) we see that:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text{TIC}\equiv -\frac{1}{N}\sum _{i=1}^N\ln q_{\hat
\theta}(Y_i)+\frac{1}{N}\text{Tr}(J^{-1}K)(\#eq:TIC)
\]&lt;/span&gt; provides an asymptotically unbiased estimate of &lt;span
class="math inline"&gt;\(\mathbb E (c_P(\hat \theta _N))\)&lt;/span&gt;, the
expected cross-entropy of a model from family &lt;span
class="math inline"&gt;\(\mathcal Q\)&lt;/span&gt; estimated on a sample of &lt;span
class="math inline"&gt;\(N\)&lt;/span&gt; observations.&lt;/p&gt;
&lt;p&gt;The previous derivation assumed the &lt;span
class="math inline"&gt;\(Y_i\)&lt;/span&gt; to be i.i.d. and does not apply,
strictly speaking, to the case of regression, for which we need some
more machinery. Assume that the pairs &lt;span
class="math inline"&gt;\((X_i,\,Y_i)\)&lt;/span&gt; are drawn independently from
a joint &lt;span class="math inline"&gt;\(X-Y\)&lt;/span&gt; distribution. Instead
of @ref(eq:CrossEntIntegral), we consider:&lt;/p&gt;
&lt;p&gt;We define, as in the i.i.d. case:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
\theta ^*(P;\mathbf X)&amp;amp;=\arg\max _{\theta}
\frac{1}{N}\sum_{i=1}^N\intop \text dP(y\vert X_i)\,\ln
\left(\frac{1}{q_{\theta}(y\vert X_i)}\right),\\
\theta ^*(P)&amp;amp;=\arg\max _{\theta} \intop \text dP(y,x)\,\ln
\left(\frac{1}{q_{\theta}(y\vert X_i)}\right),\\
\hat \theta _N&amp;amp;=\arg\max _{\theta} \sum _{i=1}^N\ln
\left(\frac{1}{q_{\theta}(Y_i\vert X_i)}\right)
\end{split}(\#eq:ThetaConditional)
\]&lt;/span&gt; Noticing that &lt;span class="math inline"&gt;\(\hat \theta
_N\)&lt;/span&gt; is a plugin estimate of &lt;span class="math inline"&gt;\(\theta
^*\)&lt;/span&gt;, we can repeat &lt;em&gt;mutatis mutandis&lt;/em&gt; the steps leading
to the CLT @ref(eq:ThetaCLT), which is also valid in this case.&lt;/p&gt;
&lt;p&gt;Rather than doing so, let us consider &lt;span
class="math inline"&gt;\(\hat \theta _N\)&lt;/span&gt; as the &lt;span
class="math inline"&gt;\(\mathbf X\)&lt;/span&gt;-conditional plugin estimate of
&lt;span class="math inline"&gt;\(\theta ^*(P;\mathbf X)\)&lt;/span&gt;, and the
latter as a plugin estimate of &lt;span class="math inline"&gt;\(\theta
^*(P)\)&lt;/span&gt; interpreted as a functional of the &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt; marginal distribution. Then, a parallel
derivation to the one provided above for the i.i.d. case shows the
conditional convergence in distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\sqrt N(\hat \theta _N - \theta ^*(P;\mathbf X))\overset{d \vert \mathbf
X}{\to} \mathcal N(0, J_{N}^{-1}(\mathbf X)K_{N}(\mathbf
X)J_{N}^{-1}(\mathbf X))(\#eq:CLTConditional).
\]&lt;/span&gt; as well as the unconditional convergence:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\sqrt N(\theta ^*(P;\mathbf X) - \theta ^*(P))\overset{d }{\to} \mathcal
N(0, J^{-1}\tilde K J^{-1})(\#eq:CLTUnconditional).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the various matrices are defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
J_N(\mathbf X)&amp;amp;\equiv \frac{1}{N}\sum _{i=1}^N\mathbb E\left[I
_{\theta} \bigg\vert X=X_i\right]\bigg\vert_{\theta = \theta ^*(\mathbf
X)},\\
\quad K_N(\mathbf X)&amp;amp;\equiv\frac{1}{N}\sum _{i=1}^N\mathbb V\left[u
_{\theta }\bigg\vert X=X_i\right]\bigg\vert_{\theta = \theta ^*(\mathbf
X)}
\end{split}
(\#eq:JKConditional).
\]&lt;/span&gt; and: &lt;span class="math display"&gt;\[
\begin{split}
J&amp;amp;\equiv \mathbb E\left[I_{\theta^*} \right],\\
\quad \tilde K&amp;amp;\equiv\mathbb V\left[\mathbb E\left(u_{\theta ^*}
\vert X\right)\right]
\end{split}
(\#eq:JKUnconditional).
\]&lt;/span&gt; Here &lt;span class="math inline"&gt;\(I_\theta\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(u_\theta\)&lt;/span&gt; are again defined as in
@ref(eq:ScoreFisherInfo), but regarded as functions of the random pair
&lt;span class="math inline"&gt;\(\{(X,\,Y)\}\)&lt;/span&gt;, rather than &lt;span
class="math inline"&gt;\(Y\)&lt;/span&gt; alone. Although Eqs.
@ref(eq:JKConditional) are written for &lt;span
class="math inline"&gt;\(\theta = \theta ^*(\mathbf X)\)&lt;/span&gt;, to the
order of the present approximation we may as well substitute &lt;span
class="math inline"&gt;\(\theta ^*(\mathbf X) \approx \theta ^*\)&lt;/span&gt;.
Doing this, we can easily see that &lt;span
class="math inline"&gt;\(J_N(\mathbf X) \to J\)&lt;/span&gt;, and &lt;span
class="math inline"&gt;\(K_N(\mathbf X) \to \mathbb E\left[\mathbb
V\left(u_{\theta } \vert X\right)\right]\bigg\vert_{\theta = \theta
^*}\)&lt;/span&gt;. This can be used to find the unconditional variance of
&lt;span class="math inline"&gt;\(\hat \theta _N\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
\mathbb V(\hat \theta _N)
    &amp;amp;=\mathbb E (\mathbb V(\hat \theta _N \vert \mathbf X))+\mathbb
V (\mathbb E(\hat \theta _N \vert \mathbf X))\\
    &amp;amp;=\mathbb E (\mathbb V(\hat \theta _N \vert \mathbf X))+\mathbb
V (\theta ^*(\mathbf X))\\
    &amp;amp;=J^{-1}\left(\mathbb V\left[\mathbb E\left(u_{\theta ^*} \vert
X\right)\right]+\mathbb E\left[\mathbb V\left(u_{\theta ^*} \vert
X\right)\right]\right)J^{-1}\\
    &amp;amp;= J^{-1} KJ^{-1}
\end{split}
\]&lt;/span&gt; with &lt;span class="math inline"&gt;\(K = \mathbb
V(u_{\theta^*})\)&lt;/span&gt; as in the i.i.d. case, in agreement with the
CLT @ref(eq:ThetaCLT). Our derivation here shows how the variance of
&lt;span class="math inline"&gt;\(\hat \theta _N\)&lt;/span&gt; decomposes into a
component due to the variability of &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt;, and a component due to the residual
variability of &lt;span class="math inline"&gt;\(Y\)&lt;/span&gt; given &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The corresponding result for the TIC @ref(eq:TIC) is slightly less
straightforward. Repeating the steps leading to this equation for a
fixed sample of regressors &lt;span class="math inline"&gt;\(\mathbf
X\)&lt;/span&gt;, we find that:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E (\text{TIC}\vert \mathbf X)=\intop \prod_{i=1}^N\text
dP(y_i\vert X_i)\,\,\frac{1}{N}\sum_{j=1}^N\intop \text dP(y^\prime\vert
X_j)\ln \left(\frac{1}{q_{\hat \theta_N}(y^\prime \vert
X_j)}\right),(\#eq:ConditionalTICRegressors)
\]&lt;/span&gt; where the outer integral is a conditional expectation on the
sample responses, while the inner integrals are expectations with
respect to a new response associated to a sample regressor &lt;span
class="math inline"&gt;\(X_i\)&lt;/span&gt;. If we now average over &lt;span
class="math inline"&gt;\(\mathbf X\)&lt;/span&gt;, we&lt;br /&gt;
find:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E (\text{TIC})=\intop \prod_{i=1}^N\text
dP(x_i,y_i)\,\,\frac{1}{N}\sum_{j=1}^N\intop \text dP(y^\prime\vert
x_j)\ln \left(\frac{1}{q_{\hat \theta_N}(y^\prime \vert
x_i)}\right)=\mathbb
E(\text{CE}_\text{in}).(\#eq:UnonditionalTICRegressors)
\]&lt;/span&gt; The right-hand side is the expected in-sample cross-entropy,
which is in general different from the extra-sample cross-entropy:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E(\text{CE}) =\intop \prod_{i=1}^N\text dP(x_i,y_i)\intop \text
dP(x^\prime,y^\prime)\ln \left(\frac{1}{q_{\hat \theta_N}(y^\prime \vert
x^\prime)}\right).
(\#eq:ExtraSampleCE)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="citation"&gt;(Shalizi 2024)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="citation"&gt;(Claeskens and Hjort 2008)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="citation"&gt;(Freedman 2006)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="citation"&gt;(White 1982)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references csl-bib-body hanging-indent"&gt;
&lt;div id="ref-claeskens2008model" class="csl-entry"&gt;
Claeskens, Gerda, and Nils Lid Hjort. 2008. &lt;span&gt;“Model Selection and
Model Averaging.”&lt;/span&gt; &lt;em&gt;Cambridge Books&lt;/em&gt;.
&lt;/div&gt;
&lt;div id="ref-freedman2006so" class="csl-entry"&gt;
Freedman, David A. 2006. &lt;span&gt;“On the so-Called &lt;span&gt;‘Huber Sandwich
Estimator’&lt;/span&gt; and &lt;span&gt;‘Robust Standard Errors’&lt;/span&gt;.”&lt;/span&gt;
&lt;em&gt;The American Statistician&lt;/em&gt; 60 (4): 299–302.
&lt;/div&gt;
&lt;div id="ref-shaliziADA" class="csl-entry"&gt;
Shalizi, Cosma. 2024. &lt;em&gt;Advanced Data Analysis from an Elementary
Point of View&lt;/em&gt;. &lt;a
href="https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/"&gt;https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-white1982maximum" class="csl-entry"&gt;
White, Halbert. 1982. &lt;span&gt;“Maximum Likelihood Estimation of
Misspecified Models.”&lt;/span&gt; &lt;em&gt;Econometrica: Journal of the
Econometric Society&lt;/em&gt;, 1–25.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes footnotes-end-of-document"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;The definition does not depend on the representations
&lt;span class="math inline"&gt;\(q_\theta = \frac{\text d Q_\theta}{\text d
\mu}\)&lt;/span&gt; chosen for the &lt;span
class="math inline"&gt;\(\mu\)&lt;/span&gt;-density of &lt;span
class="math inline"&gt;\(Q_\theta\)&lt;/span&gt; if &lt;span
class="math inline"&gt;\(P\)&lt;/span&gt; is also absolutely continuous with
respect to &lt;span class="math inline"&gt;\(\mu\)&lt;/span&gt;, which we tacitly
assume. Typically &lt;span class="math inline"&gt;\(\mu\)&lt;/span&gt; would be some
relative of Lebesgue or counting measures, in continuous and discrete
settings respectively.&lt;a href="#fnref1"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;As a random variable, &lt;span class="math inline"&gt;\(\hat
\theta _N\)&lt;/span&gt; is also independent (modulo a measure zero set) of
the specific &lt;span class="math inline"&gt;\(L_1\)&lt;/span&gt; representation
&lt;span class="math inline"&gt;\(q_\theta\)&lt;/span&gt; if &lt;span
class="math inline"&gt;\(P\)&lt;/span&gt; is absolutely continuous with respect
to &lt;span class="math inline"&gt;\(\mu\)&lt;/span&gt;.&lt;a href="#fnref2"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5>5752701da8bd0c0a178e71feaa8b8068</distill:md5>
      <guid>https://vgherard.github.io/notebooks/maximum-likelihood</guid>
      <pubDate>Wed, 13 Mar 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Exponential Dispersion Models</title>
      <dc:creator>Valerio Gherardi</dc:creator>
      <link>https://vgherard.github.io/notebooks/exponential-dispersion-models</link>
      <description>Exponential Dispersion Models</description>
      <guid>https://vgherard.github.io/notebooks/exponential-dispersion-models</guid>
      <pubDate>Thu, 07 Mar 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Bootstrap</title>
      <dc:creator>Valerio Gherardi</dc:creator>
      <link>https://vgherard.github.io/notebooks/bootstrap</link>
      <description>Bootstrap</description>
      <guid>https://vgherard.github.io/notebooks/bootstrap</guid>
      <pubDate>Wed, 07 Feb 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Ordinary Least Squares</title>
      <dc:creator>Valerio Gherardi</dc:creator>
      <link>https://vgherard.github.io/notebooks/ordinary-least-squares</link>
      <description>


&lt;h1 id="generalities"&gt;Generalities&lt;/h1&gt;
&lt;p&gt;Ordinary Least Squares (OLS) is a regression algorithm for estimating
the best linear predictor (BLP) &lt;span class="math inline"&gt;\(f^*(X) =
X\beta\)&lt;/span&gt; of a response &lt;span class="math inline"&gt;\(Y \in \mathbb
R\)&lt;/span&gt; in terms of a vector of regressors &lt;span
class="math inline"&gt;\(X\in \mathbb R ^p\)&lt;/span&gt;, which we will
frequently identify with an &lt;span class="math inline"&gt;\(1\times
p\)&lt;/span&gt; row matrix. Here, “best” is understood in terms of the &lt;span
class="math inline"&gt;\(L_2\)&lt;/span&gt; error:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\beta = \arg \min _{\beta &amp;#39;}  \mathbb E[(Y - X\beta
^\prime)^2]=\mathbb E (X^TX)^{-1}\mathbb E(X^T Y), (\#eq:BLP)
\]&lt;/span&gt; where the first equation is the defining one, while the second
one follows from elementary calculus.&lt;/p&gt;
&lt;p&gt;Given i.i.d. data &lt;span
class="math inline"&gt;\(\{(X_i,\,Y_i)\}_{i=1,\,2,\,\dots,\,N}\)&lt;/span&gt;,
and denoting by &lt;span class="math inline"&gt;\(\mathbf Y\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\mathbf X\)&lt;/span&gt; the &lt;span
class="math inline"&gt;\(N\times 1\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(N\times p\)&lt;/span&gt; matrices obtained by vertically
stacking independent observations of &lt;span
class="math inline"&gt;\(Y\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt;, respectively, the OLS estimate of
@ref(eq:BLP) is defined by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat \beta = \arg \min _{\beta &amp;#39;}  \sum _{i=1} ^N \frac{1}{N}(Y_i -
X_i\beta ^\prime)^2 = (\mathbf X ^T \mathbf X) ^{-1} \mathbf X ^T
\mathbf Y, (\#eq:OLS)
\]&lt;/span&gt; which is readily recognized to be the plugin estimate of &lt;span
class="math inline"&gt;\(\beta\)&lt;/span&gt;. Correspondingly, we define the OLS
predictor:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat Y (x) = x \hat \beta. (\#eq:OLSPredictor)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What motivates the use of an &lt;span class="math inline"&gt;\(L_2\)&lt;/span&gt;
criterion in @ref(eq:BLP)?&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Mathematical tractability.&lt;/em&gt; The fact that @ref(eq:BLP)
admits a closed form solution, which is furthermore linear in the
response variable &lt;span class="math inline"&gt;\(Y\)&lt;/span&gt;, greatly
simplifies the analysis of the properties of &lt;span
class="math inline"&gt;\(\beta\)&lt;/span&gt; and its estimators such as the OLS
one @ref(eq:OLS), making it a perfect study case.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Numerical tractability.&lt;/em&gt; A consequence of the previous
point, but worth a separate mention. Computing the plugin estimate in
@ref(eq:OLS) is just a matter of basic linear algebra manipulations,
which, with modern software libraries, is a relatively cheap
operation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Normal theory.&lt;/em&gt; Focusing on the consequence of Eq.
@ref(eq:BLP), namely the plugin estimate @ref(eq:OLS), if the
conditional distribution of &lt;span class="math inline"&gt;\(Y\)&lt;/span&gt; given
&lt;span class="math inline"&gt;\(X\)&lt;/span&gt; is normal with constant variance,
and if &lt;span class="math inline"&gt;\(\mathbb E(Y\vert X)\)&lt;/span&gt; is truly
linear, &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; coincides with the
maximum likelihood estimate of &lt;span
class="math inline"&gt;\(\beta\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="the-linear-model"&gt;The linear model&lt;/h2&gt;
&lt;p&gt;The term generally refers to a model for the conditional distribution
of &lt;span class="math inline"&gt;\(Y\vert X\)&lt;/span&gt; that requires the
conditional mean &lt;span class="math inline"&gt;\(\mathbb E(Y\vert
X)\)&lt;/span&gt; to be a linear function of &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt;. In its most parsimonious form, this is
just:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
Y = X \beta + \varepsilon, \quad \mathbb E(\varepsilon\vert X) =
0.(\#eq:LinearModel)
\]&lt;/span&gt; That said, depending on context, @ref(eq:LinearModel) is
usually supplemented with additional assumptions that further
characterise the conditional distribution of the error term&lt;a
href="#fn1" class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, typically
(with increasing strength of assumptions):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Constant Variance.&lt;/em&gt; &lt;span class="math inline"&gt;\(\mathbb
V(\varepsilon \vert X) = \sigma ^2\)&lt;/span&gt;, independently of &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;span class="math inline"&gt;\(X\)&lt;/span&gt;-Independent Errors.&lt;/em&gt;
&lt;span class="math inline"&gt;\(\varepsilon \perp X\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Normal Errors.&lt;/em&gt; &lt;span class="math inline"&gt;\(\varepsilon
\vert X \sim \mathcal N (0,\sigma ^2)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the OLS estimator is well-defined irrespective of the validity
of any of these models, it is clear that, in order for &lt;span
class="math inline"&gt;\(\hat \beta\)&lt;/span&gt; to represent a meaningful
summary of the &lt;span class="math inline"&gt;\(Y\)&lt;/span&gt;-&lt;span
class="math inline"&gt;\(X\)&lt;/span&gt; dependence, one should require at least
@ref(eq:LinearModel) to hold in some approximate sense. Correspondingly,
while some general features of &lt;span class="math inline"&gt;\(\hat
\beta\)&lt;/span&gt; can be discussed independently of linear model
assumptions, its most important properties crucially depend on Eq.
@ref(eq:LinearModel).&lt;/p&gt;
&lt;h1 id="properties"&gt;Properties&lt;/h1&gt;
&lt;h2 id="algebraic-properties-of-blp-and-ols-estimates"&gt;Algebraic
properties of BLP and OLS estimates&lt;/h2&gt;
&lt;p&gt;Consider the BLP &lt;span class="math inline"&gt;\(f^*(X) =
X\beta\)&lt;/span&gt;, with &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; defined
as in Eq. @ref(eq:BLP). We usually assume that the covariate vector
&lt;span class="math inline"&gt;\(X\)&lt;/span&gt; contains an intercept term, that
is &lt;span class="math display"&gt;\[
X=\begin{pmatrix}1 &amp;amp; Z\end{pmatrix},\quad Z\in \mathbb R^{p-1}
(\#eq:XtoZ)
\]&lt;/span&gt; for some &lt;span class="math inline"&gt;\(p-1\)&lt;/span&gt; dimensional
random vector &lt;span class="math inline"&gt;\(Z\)&lt;/span&gt;. The presence of an
intercept term leads to &lt;span class="math inline"&gt;\(f^*(X)\)&lt;/span&gt;
having a bunch of nice properties, such as being unconditionally
unbiased (&lt;span class="math inline"&gt;\(\mathbb E(Y-f^*(X))=0\)&lt;/span&gt;),
as we show below.&lt;/p&gt;
&lt;p&gt;Let us decompose &lt;span class="math inline"&gt;\(\beta = \begin{pmatrix}a
&amp;amp; b\end{pmatrix}\)&lt;/span&gt;, where &lt;span class="math inline"&gt;\(a\in
\mathbb R\)&lt;/span&gt; is the intercept term, and &lt;span
class="math inline"&gt;\(b \in \mathbb R^{p-1}\)&lt;/span&gt; is the coefficient
of &lt;span class="math inline"&gt;\(Z\)&lt;/span&gt;. We can easily prove that:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
b  &amp;amp;=
\mathbb V (Z)^{-1}\mathbb V(Z,Y),\\
a  &amp;amp;= \mathbb E(Y)-\mathbb E(Z)b,
\end{split}(\#eq:BLP0)
\]&lt;/span&gt; where we denote by &lt;span class="math inline"&gt;\(\mathbb V
(A,B)\)&lt;/span&gt; the covariance matrix of &lt;span
class="math inline"&gt;\(A\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(B\)&lt;/span&gt;, and by &lt;span
class="math inline"&gt;\(\mathbb V (A)\equiv \mathbb V (A,A)\)&lt;/span&gt;.
These expressions can be used to recast the error term as:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
Y-X\beta = (Y-\mathbb E(Y))-(Z-\mathbb E(Z))\mathbb V (Z)^{-1}\mathbb
V(Z,Y)(\#eq:BLPErrorTerm).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From this expression we can easily find the first two moments:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
\mathbb E(Y-X\beta)&amp;amp;=0,\\
\mathbb E((Y-X\beta)^2)&amp;amp;=\mathbb V(Y)-\mathbb V(Z,Y)^T\mathbb
V(Z)^{-1}\mathbb V(Z,Y),
\end{split}(\#eq:BLPExpectedError)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In particular, as anticipated the best linear predictor is
unconditionally unbiased. More generally, from Eq. @ref(eq:BLPErrorTerm)
it follows one has:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E(X^T(Y-X\beta))=0(\#eq:OrthogonalityErrorTerm),
\]&lt;/span&gt; which, since &lt;span class="math inline"&gt;\(\mathbb E(Y-X\beta) =
0\)&lt;/span&gt;, can also be interpreted as saying that the error term &lt;span
class="math inline"&gt;\(Y-X\beta\)&lt;/span&gt; and the covariate vector &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt; are uncorrelated.&lt;/p&gt;
&lt;p&gt;These properties directly translate to corresponding properties of
the empirical residuals &lt;span class="math inline"&gt;\(\mathbf Y-\mathbf
X\hat \beta\)&lt;/span&gt;. Notice that, up to now no result depends on the
particular probability measure to which expectations refer to. In
particular, we can choose this measure to be the empirical distribution
realized in a specific sample, which amounts to replace all expectations
with sample means. Thus, Eq. @ref(eq:OrthogonalityErrorTerm) translates
to:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\frac{1}{N}\mathbf X^T(\mathbf Y-\mathbf X\hat
\beta)=0(\#eq:OrthogonalitySampleResiduals),
\]&lt;/span&gt; which implies in particular that sample residuals have
vanishing sample means.&lt;/p&gt;
&lt;h2 id="distribution-of-coefficient-estimates-hat-beta"&gt;Distribution of
coefficient estimates &lt;span class="math inline"&gt;\(\hat
\beta\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;As an estimator of &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; as
defined in Eq. @ref(eq:BLP), the OLS estimator &lt;span
class="math inline"&gt;\(\hat \beta\)&lt;/span&gt; is consistent (converges in
probability to &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt;) but generally
biased (an example is provided &lt;a
href="https://vgherard.github.io/posts/2023-05-12-consistency-and-bias-of-ols-estimators/"&gt;here&lt;/a&gt;).
However, due to the plugin nature of &lt;span class="math inline"&gt;\(\hat
\beta\)&lt;/span&gt;, the bias is generally of order &lt;span
class="math inline"&gt;\(\mathcal O (N^{-1})\)&lt;/span&gt;, which makes it often
negligible in comparison to its &lt;span class="math inline"&gt;\(\mathcal
O(N^{-1/2})\)&lt;/span&gt; sampling variability (see below).&lt;/p&gt;
&lt;p&gt;Explicitly, the bias is given by: &lt;span class="math display"&gt;\[
\mathbb E(\hat \beta) - \beta = \mathbb E\lbrace((\mathbf X ^T \mathbf
X) ^{-1}-\mathbb E[(\mathbf X ^T \mathbf X) ^{-1}])\cdot \mathbf X ^T f
(\mathbf X)\rbrace, (\#eq:BiasOLS)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(f(X) \equiv \mathbb E(Y
\vert X)\)&lt;/span&gt; is the true conditional mean function. In general,
this vanishes only if &lt;span class="math inline"&gt;\(f(X)=X\beta\)&lt;/span&gt;,
as in the linear expectation model @ref(eq:LinearModel).&lt;/p&gt;
&lt;p&gt;The &lt;span class="math inline"&gt;\(\mathbf X\)&lt;/span&gt;-conditional
variance of &lt;span class="math inline"&gt;\(\hat \beta\)&lt;/span&gt; can be
derived directly from Eq. @ref(eq:OLS):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb V (\hat \beta \vert \mathbf X)=(\mathbf X ^T \mathbf X) ^{-1}
\mathbf X ^T  \mathbb V (\mathbf Y\vert \mathbf X) \mathbf X (\mathbf X
^T \mathbf X), (\#eq:VarBetaHatCond)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(\mathbb V (\mathbf Y\vert
\mathbf X)\)&lt;/span&gt; is diagonal for i.i.d. observations. For
homoskedastic errors we get:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb V (\hat \beta \vert \mathbf X)=(\mathbf X ^T \mathbf X) ^{-1}
\sigma ^2 \quad (\mathbb V(Y\vert X)=\sigma
^2).\  (\#eq:VarBetaHatCondHomo)
\]&lt;/span&gt; Under the normal linear model, this allows to obtain
finite-sample correct confidence sets for &lt;span
class="math inline"&gt;\(\beta\)&lt;/span&gt;. In the general case, confidence
sets can be derived from the Central Limit Theorem satisfied by &lt;span
class="math inline"&gt;\(\hat \beta\)&lt;/span&gt; &lt;span class="citation"&gt;(Buja
et al. 2019)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\sqrt N (\hat \beta -\beta) \to \mathcal N (0,V ) (\#eq:CLTBetaHat)
\]&lt;/span&gt; where the asymptotic variance is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
V = \mathbb E[X^TX] ^{-1} \cdot \mathbb E[X^T(Y-X\beta)^2X] \cdot
\mathbb E[X^TX] ^{-1}.(\#eq:AsyVarBetaHat)
\]&lt;/span&gt; The plugin estimate of @ref(eq:AsyVarBetaHat) leads to the so
called Sandwich variance estimator:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
V_\text{sand} \equiv  (\mathbf X^T \mathbf X)^{-1} \mathbf X^T D_\mathbf
{r^2}\mathbf X (\mathbf X^T \mathbf X)^{-1},(\#eq:SandwichEstimate)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(D_{\mathbf r^2}\)&lt;/span&gt; is
the diagonal matrix whose &lt;span class="math inline"&gt;\(i\)&lt;/span&gt;-th
entry is the squared residual &lt;span class="math inline"&gt;\(r_i ^2 =
(Y_i-X_i\beta)^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id="variance-estimates"&gt;Variance estimates&lt;/h2&gt;
&lt;p&gt;These can be based on:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
s ^2 = \frac{1}{N}(\mathbf Y-\mathbf X \hat \beta)^T(\mathbf Y-\mathbf X
\hat \beta)
\]&lt;/span&gt; which has expectation&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E\left[s^2\right]=\frac{1}{N}\text {Tr}\,\mathbb E \left[
(1-\mathbf H) \cdot \left(\mathbb E(\mathbf Y \vert \mathbf X)\mathbb
E(\mathbf Y \vert \mathbf X)^T + \mathbb V(\mathbf Y\vert\mathbf
X)\right) \right] (\#eq:ExpectedInSampleError)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the hat matrix &lt;span class="math inline"&gt;\(\mathbf H\)&lt;/span&gt;
is defined as usual:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbf H \equiv \mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf
X^T(\#eq:HatMatrix)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the general linear model @ref(eq:LinearModel) holds, so that &lt;span
class="math inline"&gt;\(E(\mathbf Y \vert \mathbf X) = \mathbf X
\beta\)&lt;/span&gt;, we have &lt;span class="math inline"&gt;\((1-\mathbf H)
\mathbb E(\mathbf Y \vert \mathbf X) = 0\)&lt;/span&gt;. If we furthermore
assume homoskedasticity, we obtain:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E\left[s^2\right]=\frac{N-p}{N}\sigma^2
\quad(\text{Homoskedastic linear model}), (\#eq:VarianceEstimateHomo)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(p = \text {Tr}(\mathbf
H)\)&lt;/span&gt; is the number of independent covariates. On the other hand,
if homoskedasticity holds, but &lt;span class="math inline"&gt;\(E(\mathbf Y
\vert \mathbf X)\)&lt;/span&gt; is not linear, the left-hand side of the
previous equation is an overestimate of &lt;span
class="math inline"&gt;\(\mathbf V \vert X\)&lt;/span&gt;.&lt;/p&gt;
&lt;h1 id="measurement-error"&gt;Measurement error&lt;/h1&gt;
&lt;p&gt;Suppose that, rather than the variables of interest &lt;span
class="math inline"&gt;\(Y\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt;, we can only observe noisy
versions:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\widetilde Y = Y+\delta_Y,\qquad \widetilde Z=Z+\delta
_Z(\#eq:NoisyYandZ)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class="math inline"&gt;\(X=\begin{pmatrix}1 &amp;amp;
Z\end{pmatrix}\)&lt;/span&gt; as in Eq. @ref(eq:XtoZ). We are truly interested
in &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; defined by Eq.
@ref(eq:BLP), but OLS will actually estimate &lt;span
class="math inline"&gt;\(\widetilde \beta =\begin{pmatrix}\widetilde a
&amp;amp; \widetilde b\end{pmatrix}\)&lt;/span&gt;, the coefficient of the BLP of
&lt;span class="math inline"&gt;\(\widetilde Y\)&lt;/span&gt; in terms of &lt;span
class="math inline"&gt;\(\widetilde X\)&lt;/span&gt;. Focusing on the slope term
&lt;span class="math inline"&gt;\(\widetilde b\)&lt;/span&gt;, a comparison with Eq.
@ref(eq:BLP0) yields:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
\widetilde b -b&amp;amp;= \Delta_{\mathbb V(Z)^{-1}}\cdot\mathbb V(Z,Y)
+\mathbb V(Z)^{-1} \cdot \Delta _{\mathbb V(Z,Y)}+\Delta_{\mathbb
V(Z)^{-1}}\cdot\Delta _{\mathbb V(Z,Y)},\\
\Delta_{\mathbb V(Z)^{-1}}&amp;amp;=\mathbb V (\widetilde Z)^{-1}-V(Z)^{-1},
\\
\Delta _{\mathbb V(Z,Y)} &amp;amp;=\mathbb V(\widetilde Z,\widetilde
Y)-\mathbb V(Z,Y).
\end{split}(\#eq:Deltab)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where, explicitly:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
\mathbb V(\widetilde Z)&amp;amp;=\mathbb V(Z)+\mathbb V(\delta _Z)+2\mathbb
V(Z,\delta _Z),\\
\mathbb V(\widetilde Z,\widetilde Y)&amp;amp;=\mathbb V(Z,Y)+\mathbb
V(\delta _Z,Y)+\mathbb V(\delta _Y,Z)+\mathbb V(\delta _Y,\delta _Z).
\end{split}(\#eq:DeltaVvsNoise)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The effect on the slope estimate generally depends on the correlation
structure of signal and noise. In the special case of “totally random”
noise, that is if &lt;span class="math inline"&gt;\(\delta _Z\)&lt;/span&gt; and
&lt;span class="math inline"&gt;\(\delta _Y\)&lt;/span&gt; are independent of each
other and of &lt;span class="math inline"&gt;\(Z\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(Y\)&lt;/span&gt;, we see that the only effect is an
increase &lt;span class="math inline"&gt;\(\mathbb V(\widetilde Z) = \mathbb
V(Z)+ \mathbb V(\delta _Z)\)&lt;/span&gt;, which shrinks the slope coefficient
&lt;span class="math inline"&gt;\(\widetilde b\)&lt;/span&gt; towards zero. In
particular, if random noise is only present in the response (&lt;span
class="math inline"&gt;\(\delta _Z = 0\)&lt;/span&gt;), the estimation target is
unaffected (although the variance of the &lt;span
class="math inline"&gt;\(\hat b\)&lt;/span&gt; estimator is, in general).&lt;/p&gt;
&lt;h1 id="proofs"&gt;Proofs&lt;/h1&gt;
&lt;h2 id="proof-of-central-limit-theorem-for-hat-beta"&gt;Proof of Central
Limit Theorem for &lt;span class="math inline"&gt;\(\hat \beta\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Convergence to the normal distribution @ref(eq:CLTBetaHat) with
variance @ref(eq:AsyVarBetaHat) can be proved using the formalism of
influence functions. From Eq. @ref(eq:BLP), we see that a small
variation &lt;span class="math inline"&gt;\(P\to P+\delta P\)&lt;/span&gt; to the
joint &lt;span class="math inline"&gt;\(XY\)&lt;/span&gt; probability measure
induces a first order shift:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\delta \beta =  \intop \mathbb E(X^TX)^{-1}X^T (Y-\beta X) \text
d(\delta P)(\#eq:BetaDifferential)
\]&lt;/span&gt; in the best linear predictor. The influence function of &lt;span
class="math inline"&gt;\(\beta\)&lt;/span&gt; is defined by the measurable
representation of &lt;span class="math inline"&gt;\(\delta \beta\)&lt;/span&gt;,
namely:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\phi _\beta = \mathbb E(X^TX)^{-1}X^T (Y-\beta X).
(\#eq:BetaInfluenceFunction)
\]&lt;/span&gt; A general result for plugin estimates then tells us that &lt;span
class="math inline"&gt;\(\sqrt N (\hat \beta -\beta) \to \mathcal N (0,
\mathbb E (\phi _\beta ^2))\)&lt;/span&gt; in distribution, and using the
explicit form of @ref(eq:BetaInfluenceFunction) we readily obtain
@ref(eq:AsyVarBetaHat).&lt;/p&gt;
&lt;h1 id="related-posts"&gt;Related posts&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a
href="https://vgherard.github.io/posts/2023-05-12-consistency-and-bias-of-ols-estimators/"&gt;Consistency
and bias of OLS estimators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a
href="https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches/"&gt;Model
misspecification and linear sandwiches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a
href="https://vgherard.github.io/posts/2023-05-20-linear-regression-with-autocorrelated-noise/"&gt;Linear
regression with autocorrelated noise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a
href="https://vgherard.github.io/posts/2023-07-11-testing-functional-specification-in-linear-regression/"&gt;Testing
functional specification in linear regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references csl-bib-body hanging-indent"&gt;
&lt;div id="ref-buja2019models" class="csl-entry"&gt;
Buja, Andreas, Richard Berk, Lawrence Brown, Edward George, Emil Pitkin,
Mikhail Traskin, Linda Zhao, and Kai Zhang. 2019. &lt;span&gt;“Models as
Approximations i: Consequences Illustrated with Linear
Regression.”&lt;/span&gt; &lt;a
href="https://arxiv.org/abs/1404.1578"&gt;https://arxiv.org/abs/1404.1578&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes footnotes-end-of-document"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;I use the notation &lt;span class="math inline"&gt;\(A\perp
B\)&lt;/span&gt; to indicate that the random variables &lt;span
class="math inline"&gt;\(A\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(B\)&lt;/span&gt; are statistically independent.&lt;a
href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5>639b544ad1bc9e40504b8cb76a6eccf1</distill:md5>
      <guid>https://vgherard.github.io/notebooks/ordinary-least-squares</guid>
      <pubDate>Wed, 07 Feb 2024 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
