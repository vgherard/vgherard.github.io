<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>vgherard</title>
    <link>https://vgherard.github.io/</link>
    <atom:link href="https://vgherard.github.io/notebooks.xml" rel="self" type="application/rss+xml"/>
    <description>Valerio Gherardi's Personal Website
</description>
    <generator>Distill</generator>
    <lastBuildDate>Thu, 14 Mar 2024 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Maximum Likelihood</title>
      <dc:creator>Valerio Gherardi</dc:creator>
      <link>https://vgherard.github.io/notebooks/maximum-likelihood</link>
      <description>


&lt;p&gt;We start off with a functional description of Maximum Likelihood (ML)
estimation. Let &lt;span class="math inline"&gt;\(\mathcal Q \equiv\{\text d
Q_{\theta} = q_\theta \,\text d \mu\}_{\theta \in \Theta}\)&lt;/span&gt; be a
parametric family of probability measures dominated by some common
measure &lt;span class="math inline"&gt;\(\mu\)&lt;/span&gt;. Consider the
functional&lt;a href="#fn1" class="footnote-ref"
id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\theta ^* (P) = \arg \min_{\theta \in \Theta} \intop \text dP\,\ln
(\frac{1}{q_\theta}) (\#eq:FunctionalThethaStar).
\]&lt;/span&gt; This is the parameter of the best (in the cross-entropy sense)
approximation of &lt;span class="math inline"&gt;\(P\)&lt;/span&gt; within &lt;span
class="math inline"&gt;\(\mathcal Q\)&lt;/span&gt;, which we assume to be
unique.&lt;/p&gt;
&lt;p&gt;If &lt;span class="math inline"&gt;\(P\)&lt;/span&gt; represents the true
probability distribution of the data under study, &lt;span
class="math inline"&gt;\(\theta ^*(P)\)&lt;/span&gt; is the target of ML
estimation, in the general case in which &lt;span
class="math inline"&gt;\(P\)&lt;/span&gt; is not necessarily in &lt;span
class="math inline"&gt;\(\mathcal Q\)&lt;/span&gt;. The ML estimate &lt;span
class="math inline"&gt;\(\hat \theta _N\)&lt;/span&gt; of &lt;span
class="math inline"&gt;\(\theta^*\)&lt;/span&gt; from an i.i.d. sample of &lt;span
class="math inline"&gt;\(N\)&lt;/span&gt; observations is&lt;a href="#fn2"
class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat \theta _N \equiv \theta ^*(\hat P _N)=\arg \max_{\theta \in \Theta}
\sum_{i=1}^N \ln ({q_\theta(Z_i)}), (\#eq:ThetaMLE)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(\hat P _N\)&lt;/span&gt; is the
empirical distribution of the sample.&lt;/p&gt;
&lt;p&gt;Denoting:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
c_{P}(\theta) = \intop \text dP\,\ln
(\frac{1}{q_\theta}),(\#eq:CrossEntIntegral)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;we see that &lt;span class="math inline"&gt;\(\theta^*\)&lt;/span&gt; is
determined by the condition &lt;span
class="math inline"&gt;\(c_{P}&amp;#39;(\theta^*)=0\)&lt;/span&gt;. From this, we can
easily derive the first order variation of &lt;span
class="math inline"&gt;\(\theta ^*\)&lt;/span&gt; under a variation &lt;span
class="math inline"&gt;\(P \to P + \delta P\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\delta \theta ^* =\left(\intop \text dP\,I_{\theta ^*}
\right)^{-1}\left(\intop \text d(\delta P)u_{\theta
^*}\right)(\#eq:DifferentialThetaStar)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where we have defined:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
u_\theta = \frac{\partial }{\partial \theta} \ln q_\theta,\quad I_\theta
= -\frac{\partial^2 }{\partial \theta ^2}  \ln
q_\theta.(\#eq:ScoreFisherInfo)
\]&lt;/span&gt; From @ref(eq:DifferentialThetaStar) we can identify the
influence function of the &lt;span class="math inline"&gt;\(\theta ^*\)&lt;/span&gt;
functional:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\psi_P(z)=\left(\intop \text dP\,I_{\theta ^*} \right)^{-1}u_{\theta
^*}(z)(\#eq:InfluenceFunction)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then, from the standard theory of influence functions, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat \theta _N \approx \theta ^*+J_{\theta ^*} ^{-1}
U_{\theta^*}(\#eq:ThetaNFirstOrder)
\]&lt;/span&gt; where we have defined:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
J_{\theta ^*}\equiv \intop \text dP\,I_{\theta ^*},\quad U_{\theta ^*
}\equiv\frac{1}{N}\sum _{i=1}^Nu_{\theta
^*}(Z_i)(\#eq:ScoreFisherInfo2).
\]&lt;/span&gt; In particular, we obtain the Central Limit Theorem (CLT)&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\sqrt N(\hat \theta _N - \theta ^*) \overset{d}{\to} \mathcal N(0,
J_{\theta^*}^{-1}K_{\theta ^*}J_{\theta ^*}^{-1}),(\#eq:ThetaCLT)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
K_{\theta ^*} = \mathbb V(u_{\theta ^*}(Z)). (\#eq:ScoreVariance)
\]&lt;/span&gt; The matrices &lt;span class="math inline"&gt;\(K_{\theta
^*}\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(J_{\theta ^*}\)&lt;/span&gt;
depend on the unknown value &lt;span class="math inline"&gt;\(\theta
^*\)&lt;/span&gt;, but we can readily construct plugin estimators:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat J_N = -\frac{1}{N}\sum _{i=1}^NI_{\hat \theta _N}(z_i),\quad\hat
K_N = \frac{1}{N}\sum _{i=1}^Nu_{\hat \theta _N}(z_i)u_{\hat \theta
_N}(z_i)^T,(\#eq:PluginJK)
\]&lt;/span&gt; and estimate the variance of &lt;span class="math inline"&gt;\(\hat
\theta _N\)&lt;/span&gt; as:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\widehat {\mathbb V}(\hat \theta _N) = \frac{\hat J _N ^{-1}\hat K_N\hat
J_N ^{-1}}{N}(\#eq:SandwichEstimator),
\]&lt;/span&gt; which is the usual Sandwich estimator. Finally, if &lt;span
class="math inline"&gt;\(P = Q_{\theta^*}\)&lt;/span&gt;, then &lt;span
class="math inline"&gt;\(J _{\theta^*} = K _{\theta^*}\)&lt;/span&gt;, and the
CLT @ref(eq:ThetaCLT) becomes simply &lt;span class="math inline"&gt;\(\sqrt
N(\hat \theta _N - \theta ^*) \overset{d}{\to} \mathcal N(0,
J_{\theta^*}^{-1})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let us now consider the following expansion of &lt;span
class="math inline"&gt;\(c_P(\hat \theta _N)\)&lt;/span&gt; which, we recall, is
the cross-entropy of the ML model on the true distribution &lt;span
class="math inline"&gt;\(P\)&lt;/span&gt; (&lt;em&gt;cf.&lt;/em&gt;
@ref(eq:CrossEntIntegral)):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
c_P(\hat \theta _N)
    &amp;amp;= -\intop \text d P(z&amp;#39;)\,\ln (q_{\hat \theta}(z&amp;#39;))\\
    &amp;amp; \approx -\mathbb E&amp;#39;(\ln q_{\theta^*})+\frac{1}{2}(\hat
\theta-\theta ^*)^TJ_{\theta ^*} (\hat \theta-\theta ^*)\\
    &amp;amp; \approx -\mathbb E&amp;#39;(\ln q_{\theta^*})+\frac{1}{2}U_{\theta
^*}^TJ_{\theta ^*}^{-1}U_{\theta ^*}
\end{split}
\]&lt;/span&gt; Taking the expectation with respect to the training dataset,
noting that &lt;span class="math inline"&gt;\(\mathbb E(U_{\theta ^*}U_{\theta
^*}^T)=K_{\theta ^*}\)&lt;/span&gt;, we get:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E (c_P(\hat \theta _N))\approx -\mathbb E&amp;#39;(\ln
q_{\theta^*})+\frac{1}{2N}\text {Tr}(J_{\theta ^*}^{-1}K_{\theta^*})
(\#eq:CrossEntExp)
\]&lt;/span&gt; Now consider the in-sample estimate:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
c_{\hat P _N}(\hat \theta _N) &amp;amp;= -\frac{1}{N}\sum _{i=1}^N\ln
q_{\hat \theta}(Z_i)\\
&amp;amp; \approx - \frac{1}{N}\sum _{i=1} ^N \ln q_{\theta^*}(Z_i)-
U_{\theta ^*}^T(\hat \theta _N-\theta^*)+ \frac{1}{2}(\hat \theta
_N-\theta^*)^TJ_{\theta ^*}(\hat \theta _N-\theta^*)\\
&amp;amp; \approx - \frac{1}{N}\sum _{i=1} ^N \ln q_{\theta^*}(Z_i)-
U_{\theta ^*}^TJ_{\theta ^*} ^{-1} U_{\theta^*}+ \frac{1}{2}U_{\theta
^{*}}^TJ_{\theta ^*} ^{-1}\hat J_N J_{\theta ^*} ^{-1}U_{\theta ^{*}}\\
&amp;amp; \approx - \frac{1}{N}\sum _{i=1} ^N \ln q_{\theta^*}(Z_i)-
\frac{1}{2}U_{\theta ^*}^TJ_{\theta ^*} ^{-1} U_{\theta^*}.
\end{split}
\]&lt;/span&gt; Taking the expectation :&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E (c_{\hat P _N}(\hat \theta _N)) = -\mathbb E&amp;#39;(\ln
q_{\theta^*})-\frac{1}{2N}\text{Tr}(J_{\theta
^*}^{-1}K_{\theta^*})(\#eq:InSampleCrossEntExp)
\]&lt;/span&gt; Comparing Eqs. @ref(eq:InSampleCrossEntExp) and
@ref(eq:CrossEntExp) we see that:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text{TIC}\equiv -\frac{1}{N}\sum _{i=1}^N\ln q_{\hat
\theta}(Z_i)+\frac{1}{N}\text{Tr}(J_{\theta
^*}^{-1}K_{\theta^*})(\#eq:TIC)
\]&lt;/span&gt; provides an asymptotically unbiased estimate of &lt;span
class="math inline"&gt;\(\mathbb E (c_P(\hat \theta _N))\)&lt;/span&gt;, the
expected cross-entropy of a model from family &lt;span
class="math inline"&gt;\(\mathcal Q\)&lt;/span&gt; estimated on a sample of &lt;span
class="math inline"&gt;\(N\)&lt;/span&gt; observations.&lt;/p&gt;
&lt;p&gt;In the previous derivation, we could take each &lt;span
class="math inline"&gt;\(Z_i\)&lt;/span&gt; to be a pair &lt;span
class="math inline"&gt;\((X_i,\,Y_i)\)&lt;/span&gt; drawn from a joint &lt;span
class="math inline"&gt;\(X-Y\)&lt;/span&gt; distribution. If we replace the model
family &lt;span class="math inline"&gt;\(\mathcal Q\)&lt;/span&gt; with a parametric
family of conditional densities &lt;span class="math inline"&gt;\(\mathcal Q
\equiv\{\text d Q_{\theta}(\cdot\vert X) = q_{\theta}(\cdot\vert X)
\,\text d \mu\}_{\theta \in \Theta}\)&lt;/span&gt;, and change the objective
function to:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
c_P(\theta) = \intop \text dP(x,y)\,\ln (\frac{1}{q_{\theta}(y\vert
x)}),(\#eq:CondCrossEntIntegral)
\]&lt;/span&gt; we can repeat our above argument without any further change.
This provides a quick and dirty derivation of the CLT @ref(eq:ThetaCLT)
and of the information criterion @ref(eq:TIC) in a regression setting
with random regressors.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="citation"&gt;(Shalizi 2024)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="citation"&gt;(Claeskens and Hjort 2008)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="citation"&gt;(Freedman 2006)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="citation"&gt;(White 1982)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references csl-bib-body hanging-indent"&gt;
&lt;div id="ref-claeskens2008model" class="csl-entry"&gt;
Claeskens, Gerda, and Nils Lid Hjort. 2008. &lt;span&gt;“Model Selection and
Model Averaging.”&lt;/span&gt; &lt;em&gt;Cambridge Books&lt;/em&gt;.
&lt;/div&gt;
&lt;div id="ref-freedman2006so" class="csl-entry"&gt;
Freedman, David A. 2006. &lt;span&gt;“On the so-Called &lt;span&gt;‘Huber Sandwich
Estimator’&lt;/span&gt; and &lt;span&gt;‘Robust Standard Errors’&lt;/span&gt;.”&lt;/span&gt;
&lt;em&gt;The American Statistician&lt;/em&gt; 60 (4): 299–302.
&lt;/div&gt;
&lt;div id="ref-shaliziADA" class="csl-entry"&gt;
Shalizi, Cosma. 2024. &lt;em&gt;Advanced Data Analysis from an Elementary
Point of View&lt;/em&gt;. &lt;a
href="https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/"&gt;https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-white1982maximum" class="csl-entry"&gt;
White, Halbert. 1982. &lt;span&gt;“Maximum Likelihood Estimation of
Misspecified Models.”&lt;/span&gt; &lt;em&gt;Econometrica: Journal of the
Econometric Society&lt;/em&gt;, 1–25.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes footnotes-end-of-document"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;The definition does not depend on the representations
&lt;span class="math inline"&gt;\(q_\theta = \frac{\text d Q_\theta}{\text d
\mu}\)&lt;/span&gt; chosen for the &lt;span
class="math inline"&gt;\(\mu\)&lt;/span&gt;-density of &lt;span
class="math inline"&gt;\(Q_\theta\)&lt;/span&gt; if &lt;span
class="math inline"&gt;\(P\)&lt;/span&gt; is also absolutely continuous with
respect to &lt;span class="math inline"&gt;\(\mu\)&lt;/span&gt;, which we tacitly
assume. Typically &lt;span class="math inline"&gt;\(\mu\)&lt;/span&gt; would be some
relative of Lebesgue or counting measures, in continuous and discrete
settings respectively.&lt;a href="#fnref1"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;As a random variable, &lt;span class="math inline"&gt;\(\hat
\theta _N\)&lt;/span&gt; is also independent (modulo a measure zero set) of
the specific representer &lt;span class="math inline"&gt;\(q_\theta\)&lt;/span&gt;
if &lt;span class="math inline"&gt;\(P\)&lt;/span&gt; is absolutely continuous with
respect to &lt;span class="math inline"&gt;\(\mu\)&lt;/span&gt;.&lt;a href="#fnref2"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5>5752701da8bd0c0a178e71feaa8b8068</distill:md5>
      <guid>https://vgherard.github.io/notebooks/maximum-likelihood</guid>
      <pubDate>Thu, 14 Mar 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Exponential Dispersion Models</title>
      <dc:creator>Valerio Gherardi</dc:creator>
      <link>https://vgherard.github.io/notebooks/exponential-dispersion-models</link>
      <description>


&lt;h2 id="intro"&gt;Intro&lt;/h2&gt;
&lt;p&gt;Exponential Dispersion Models (EDMs) provide a natural generalization
of the normal distribution, in which the modeled variable &lt;span
class="math inline"&gt;\(Y\)&lt;/span&gt; is assumed to follow a probability
density:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text d P _{\lambda,\,\mu}(y)=e^{-\frac{\lambda}{2}d(y,\,\mu)}\text d
\nu _{\lambda}(y)
\]&lt;/span&gt; with respect to a certain dominating measure &lt;span
class="math inline"&gt;\(\nu _{\lambda}\)&lt;/span&gt;. Here &lt;span
class="math inline"&gt;\(\mu = \intop y\,\text d
P_{\lambda,\,\mu}(y)\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(d(y,\,\mu)
\geq 0\)&lt;/span&gt;, with equality only for &lt;span class="math inline"&gt;\(y =
\mu\)&lt;/span&gt;. The function &lt;span
class="math inline"&gt;\(d(y,\,\mu)\)&lt;/span&gt; is called the &lt;em&gt;unit
deviance&lt;/em&gt;, and plays for EDMs the same role of squared distance
&lt;span class="math inline"&gt;\((y-\mu)^2\)&lt;/span&gt; for the normal model. Not
surprisingly, EDMs provide a sound framework for the maximum-likelihood
based formulation of generalized linear models, additive models, and
similar beasts.&lt;/p&gt;
&lt;h2 id="exponential-dispersion-models"&gt;Exponential Dispersion
Models&lt;/h2&gt;
&lt;p&gt;We start with a probability measure on &lt;span
class="math inline"&gt;\(\mathbb R ^n\)&lt;/span&gt; in the form of an
&lt;em&gt;additive EDM&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text d P ^* _{\lambda, \theta} (z) = e^{\theta ^T
z-\lambda\kappa(\theta)}\text dQ^*_\lambda (z) (\#eq:EDstar)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(\lambda &amp;gt; 0\)&lt;/span&gt;,
&lt;span class="math inline"&gt;\(\text Q^*_\lambda\)&lt;/span&gt; is a Borelian
probability measure on &lt;span class="math inline"&gt;\(\mathbb R\)&lt;/span&gt;,
and &lt;span class="math inline"&gt;\(\kappa(\theta)\)&lt;/span&gt; is a
differentiable strictly convex function, with &lt;span
class="math inline"&gt;\(\kappa&amp;#39;&amp;#39;(\theta) &amp;gt; 0\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\kappa(0) =0\)&lt;/span&gt;. For a random variable &lt;span
class="math inline"&gt;\(Z\)&lt;/span&gt; distributed according to @ref(eq:ED) we
write &lt;span class="math inline"&gt;\(Z\sim \text{ED}^*(\lambda,
\,\theta,\,\kappa)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For any given &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt;,
normalization of @ref(eq:EDstar) requires: &lt;span class="math display"&gt;\[
e^{\lambda \kappa(\theta)}=\intop e^{\theta ^Ty}\text
dQ^*_\lambda(z)(\#eq:NormCond)
\]&lt;/span&gt; to hold for all &lt;span class="math inline"&gt;\(\theta\)&lt;/span&gt;
and &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt;. In other words, &lt;span
class="math inline"&gt;\(M_\lambda(\theta) \equiv e^{\lambda
\kappa(\theta)}\)&lt;/span&gt; must be the moment generating function of the
measure &lt;span class="math inline"&gt;\(Q^* _\lambda(y)\)&lt;/span&gt; for a given
&lt;span class="math inline"&gt;\(\theta\)&lt;/span&gt;, which we assume to be
uniquely determined by its moments&lt;a href="#fn1" class="footnote-ref"
id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, so that we can omit the mention of the
measure &lt;span class="math inline"&gt;\(Q^*_\lambda\)&lt;/span&gt; in the notation
&lt;span class="math inline"&gt;\(\text{ED}^*(\lambda,
\,\theta,\,\kappa)\)&lt;/span&gt;^. This requires, in particular &lt;span
class="math inline"&gt;\(\kappa (0) = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A closely related parametrization is the so-called &lt;em&gt;reproductive
EDM&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text d P _{\lambda, \theta} (y) = e^{\lambda(\theta ^T
y-\kappa(\theta))}\text dQ_\lambda (y)(\#eq:ED).
\]&lt;/span&gt; For a random variable &lt;span class="math inline"&gt;\(Y\)&lt;/span&gt;
distributed according to @ref(eq:ED) we write &lt;span
class="math inline"&gt;\(Y\sim \text{ED}(\lambda,
\,\theta,\,\kappa)\)&lt;/span&gt;. The link between @ref(eq:ED) and
@ref(eq:EDstar) is that &lt;span class="math inline"&gt;\(Y\sim
\text{ED}(\lambda, \,\theta,\,\kappa)\)&lt;/span&gt; if and only if &lt;span
class="math inline"&gt;\(Z=\lambda Y\sim \text{ED}^*(\lambda,
\,\theta,\,\kappa)\)&lt;/span&gt;, so that reproductive and additive EDMs can
be interchanged whenever convenient, at least for theoretical
considerations. The probability measures &lt;span
class="math inline"&gt;\(\text d Q_\lambda\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\text d Q ^*_\lambda\)&lt;/span&gt;, which are uniquely
determined by normalization, are related by push-forward:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
Q_\lambda ^* = (m_\lambda )_*(Q_\lambda),(\#eq:PushForward)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(m_\lambda\)&lt;/span&gt; denotes
multiplication by &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt;,
&lt;em&gt;i.e.&lt;/em&gt; &lt;span class="math inline"&gt;\(m_\lambda(y)=\lambda
y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In cases of practical interest (see the examples below), &lt;span
class="math inline"&gt;\(Q_\lambda\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(Q_\lambda^*\)&lt;/span&gt; are absolutely continuous
either with respect to the Lebesgue measure, or with respect some
measure concentrated on &lt;span class="math inline"&gt;\(c \cdot \mathbb
N\)&lt;/span&gt; for some &lt;span class="math inline"&gt;\(c&amp;gt;0\)&lt;/span&gt;. The two
cases are referred to as the “continuous” and “discrete” case,
respectively, for obvious reasons.&lt;/p&gt;
&lt;h2 id="general-properties"&gt;General Properties&lt;/h2&gt;
&lt;h3 id="moment-generating-function"&gt;Moment generating function&lt;/h3&gt;
&lt;p&gt;Consider first the reproductive EDM @ref(eq:ED). If &lt;span
class="math inline"&gt;\(Y\sim \text
{ED}(\lambda,\,\theta,\,\kappa)\)&lt;/span&gt;, its moment generating function
is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
M_Y(s)=\mathbb E(e^{sY})=\exp\left[\lambda\left(\kappa(\theta
+\frac{s}{\lambda})-\kappa(\theta)\right)\right],(\#eq:MGF)
\]&lt;/span&gt; from which we can derive, in particular:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
\mathbb E(Y) &amp;amp;= \frac{\text d}{\text ds}\vert_{s=0}\log M(s)
=\kappa&amp;#39;(\theta),\\
\mathbb V(Y) &amp;amp;= \frac{\text d^2}{\text ds ^2}\vert_{s=0}\log M(s)
=\frac{\kappa&amp;#39;&amp;#39;(\theta)}{\lambda}.\\
\end{split}(\#eq:ExpVar)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the additive EDM @ref(eq:EDstar), the corresponding results for
&lt;span class="math inline"&gt;\(Z\sim
\text{ED}^*(\lambda,\,\theta,\,\kappa)\)&lt;/span&gt; are:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
M_Z(s)&amp;amp;=\exp\left[\lambda\left(\kappa(\theta +
s)-\kappa(\theta)\right)\right],\\
\mathbb E(Z) &amp;amp;= \lambda\kappa&amp;#39;(\theta),\\
\mathbb V(Z) &amp;amp;= \lambda \kappa&amp;#39;&amp;#39;(\theta).
\end{split}(\#eq:MGFResultsEDstar)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id="legendre-transform-of-kappa-theta"&gt;Legendre Transform of &lt;span
class="math inline"&gt;\(\kappa (\theta)\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Since &lt;span class="math inline"&gt;\(\kappa\)&lt;/span&gt; is strictly convex,
the mapping:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mu = \frac{\partial\kappa}{\partial\theta}(\#eq:MuThetaMapping)
\]&lt;/span&gt; is invertible, and we may equivalently parametrize the
reproductive EDM in terms of &lt;span class="math inline"&gt;\(\mu\)&lt;/span&gt;
and &lt;span class="math inline"&gt;\(\lambda\)&lt;/span&gt; as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text d P _{\lambda, \mu} (y) = e^{\lambda(\theta(\mu) ^T
(y-\mu)+\tau(\mu))}\text dQ_\lambda (y)(\#eq:EDmu).
\]&lt;/span&gt; where:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\tau(\mu) = \theta(\mu)^T\mu - \kappa(\theta(\mu))
(\#eq:LegendreTransform).
\]&lt;/span&gt; is the Legendre transform of &lt;span
class="math inline"&gt;\(\kappa\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id="deviance"&gt;Deviance&lt;/h2&gt;
&lt;p&gt;Consider two reproductive EDMs &lt;span class="math inline"&gt;\(P
_{\lambda,\mu _1}\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(P
_{\lambda,\mu _2}\)&lt;/span&gt; with the same dispersion parameter &lt;span
class="math inline"&gt;\(\lambda\)&lt;/span&gt; (the function &lt;span
class="math inline"&gt;\(\kappa\)&lt;/span&gt; is assumed to be fixed
throughout). The likelihood ratio at a given &lt;span
class="math inline"&gt;\(Y=y\)&lt;/span&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\ln (\frac{\text d P_{\lambda,\mu_1}}{\text d
P_{\lambda,\mu_2}}(y))=\lambda\cdot\left[(\theta_1-\theta_2)y-(\kappa_1-\kappa_2)\right],(\#eq:LogLikelihood)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(\theta _1 =
\theta(\mu_1)\)&lt;/span&gt;, &lt;span class="math inline"&gt;\(\kappa_1=
\kappa(\theta(\mu _1))\)&lt;/span&gt;, &lt;em&gt;etc.&lt;/em&gt;. Setting &lt;span
class="math inline"&gt;\(\mu _1 = y\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\mu _2 = \mu\)&lt;/span&gt; in this expression and
multiplying by a convenient factor, we obtain the so called &lt;em&gt;unit
scaled deviance&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
d_\lambda(y,\mu) &amp;amp;\equiv 2\left.\ln (\frac{\text d
P_{\lambda,\mu_0}}{\text d P_{\lambda,\mu}}(y)) \right \vert
_{\mu_0=y}\\&amp;amp;=2\lambda\cdot\left[(\theta(y)-\theta(\mu))y-\kappa(\theta(y))+\kappa(\theta(\mu))\right].
\end{split}(\#eq:UnitScaledDeviance)
\]&lt;/span&gt; The &lt;em&gt;unit deviance&lt;/em&gt; is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
d(y,\mu) &amp;amp;\equiv
d_1(y,\mu)\\&amp;amp;=2\cdot\left[(\theta(y)-\theta(\mu))y-\kappa(\theta(y))+\kappa(\theta(\mu))\right]
\end{split}(\#eq:UnitDeviance)
\]&lt;/span&gt; It is also useful to express &lt;span
class="math inline"&gt;\(d_\lambda\)&lt;/span&gt; in terms of the Legendre
transform &lt;span class="math inline"&gt;\(\tau\)&lt;/span&gt; of &lt;span
class="math inline"&gt;\(\kappa\)&lt;/span&gt;, as defined in
@ref(eq:LegendreTransform):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
d_\lambda(y,\mu)
=2\lambda\cdot\left[-\theta(\mu)^T(y-\mu)+\tau(y)-\tau(\mu)\right](\#eq:UnitDevianceLegendre)
\]&lt;/span&gt; Using the convexity of &lt;span
class="math inline"&gt;\(\kappa\)&lt;/span&gt;, it is easy to show that &lt;span
class="math inline"&gt;\(d_\lambda(y,\mu) \geq 0\)&lt;/span&gt; for all &lt;span
class="math inline"&gt;\(y\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\mu\)&lt;/span&gt;, and that &lt;span
class="math inline"&gt;\(d_\lambda(y,\mu) = 0\)&lt;/span&gt; requires &lt;span
class="math inline"&gt;\(\mu = y\)&lt;/span&gt;. The probability measure can be
expressed in terms of the unit deviance as:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text d P _{\lambda, \mu} (y) =
e^{-\frac{\lambda}{2}d(y,\,\mu)}e^{\lambda \tau(y)}\text dQ_\lambda
(y)(\#eq:EDvsDeviance).
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id="maximum-likelihood-estimation"&gt;Maximum Likelihood
Estimation&lt;/h2&gt;
&lt;p&gt;Let &lt;span class="math inline"&gt;\(Y_i\sim
\text{ED}(\lambda,\,\mu^{(0)}_ i ,\,\kappa)\)&lt;/span&gt; be independent for
&lt;span class="math inline"&gt;\(i=1,\,2,\,\dots,\,N\)&lt;/span&gt; and let &lt;span
class="math inline"&gt;\(M\subseteq \mathbb R ^N\)&lt;/span&gt; be a family of
models for the mean &lt;span class="math inline"&gt;\(\boldsymbol \mu ^{(0)} =
(\mu _1^{(0)},\,\mu_2^{(0)},\dots,\,\mu_N^{(0)})^T\)&lt;/span&gt; (in a GLM
context, &lt;span class="math inline"&gt;\(M\)&lt;/span&gt; would be the linear
subspace spanned by the covariates, &lt;span
class="math inline"&gt;\(\boldsymbol \mu _\beta = \mathbf X
\beta\)&lt;/span&gt;). From Eq. @ref(eq:EDvsDeviance), we see that the
likelihood of a model &lt;span class="math inline"&gt;\(\boldsymbol \mu \in
M\)&lt;/span&gt; is, modulo a &lt;span class="math inline"&gt;\(\boldsymbol
\mu\)&lt;/span&gt;-independent term, equal to its total deviance:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\log \mathcal L (\boldsymbol \mu,\,\lambda;\mathbf Y) =
-\frac{\lambda}{2}\mathcal D(\mathbf Y,\boldsymbol
\mu)+g_\lambda(\mathbf Y),(\#eq:LikelihoodVsDeviance)
\]&lt;/span&gt; with:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathcal D (\mathbf Y,\boldsymbol \mu)\equiv\sum_{i=1}^Nd(Y_i,\mu_i)
(\#eq:TotalDeviance)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence, the Maximum Likelihood Estimate (MLE) of &lt;span
class="math inline"&gt;\(\boldsymbol \mu\)&lt;/span&gt; corresponds to the
minimum deviance estimate:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat {\boldsymbol \mu}\equiv \arg \max _{\boldsymbol \mu \in M} \mathcal
L (\boldsymbol \mu,\,\lambda;\,\mathbf Y)=\arg \min _{\boldsymbol \mu
\in M} \mathcal D (\mathbf Y;\boldsymbol \mu).(\#eq:MLEisMDE)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In particular, the MLE &lt;span class="math inline"&gt;\(\hat {\boldsymbol
\mu}\)&lt;/span&gt; is obtained by minimizing a function of &lt;span
class="math inline"&gt;\(\boldsymbol \mu\)&lt;/span&gt; only, and is independent
on whether the dispersion parameter &lt;span
class="math inline"&gt;\(\lambda\)&lt;/span&gt; is being estimated itself or
not.&lt;/p&gt;
&lt;p&gt;These results are sometimes formulated in terms of a “saturated”
model &lt;span class="math inline"&gt;\(\boldsymbol \mu _\text{s} = \mathbf
Y\)&lt;/span&gt;. From Eq. @ref(eq:LikelihoodVsDeviance) we see that such a
model has likelihood equal to &lt;span
class="math inline"&gt;\(g_{\lambda}(\boldsymbol \mu)\)&lt;/span&gt;, implying
that:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\lambda\mathcal D(\mathbf Y,\boldsymbol \mu) = -2\log
\left(\frac{\mathcal L (\boldsymbol \mu,\lambda;\mathbf Y)}{\mathcal L
(\mathbf Y,\lambda;\mathbf Y)}\right) (\#eq:DevianceLogLik).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We note the asymptotic results &lt;span class="citation"&gt;(B. Jørgensen
1992, sec. 3.6)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\lambda \mathcal D(\mathbf Y,\,\hat {\boldsymbol
\mu})\overset{d}{\to}  \chi ^2 _{N-p} \qquad (\lambda \to
\infty)(\#eq:DevianceAsymptotics),\\
\]&lt;/span&gt; for a correctly specified model family &lt;span
class="math inline"&gt;\(M\)&lt;/span&gt; with &lt;span class="math inline"&gt;\(\dim
(M) = p\)&lt;/span&gt;, and:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\lambda \mathcal D(\mathbf Y,\,\hat {\boldsymbol \mu}_1)-\lambda
D(\mathbf Y,\,\hat {\boldsymbol \mu}_2)\overset{d}{\to} \chi ^2
_{p_2-p_1} \qquad (\lambda \to \infty \text { or } N\to
\infty)(\#eq:DevianceDiffAsymptotics),
\]&lt;/span&gt; for a correctly specified model family &lt;span
class="math inline"&gt;\(M_1\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(M_2
\supseteq M_1\)&lt;/span&gt;, with &lt;span class="math inline"&gt;\(p_i =\dim
(M_i)\)&lt;/span&gt;. Eq. @ref(eq:DevianceAsymptotics) can be seen as the
limiting case of @ref(eq:DevianceDiffAsymptotics) when &lt;span
class="math inline"&gt;\(M_2 = \mathbb R ^N\)&lt;/span&gt;, as in the saturated
model. The manifolds &lt;span class="math inline"&gt;\(M\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(M_i\)&lt;/span&gt; are not strictly required to be
linear subspaces of &lt;span class="math inline"&gt;\(\mathbb R^N\)&lt;/span&gt;,
because in the limits and under the null hypotheses implied by Eqs.
@ref(eq:DevianceAsymptotics) and @ref(eq:DevianceDiffAsymptotics) the
distributions of MLEs are concentrated around the true value &lt;span
class="math inline"&gt;\(\boldsymbol \mu ^{(0)}\)&lt;/span&gt;, so that the
manifolds &lt;span class="math inline"&gt;\(M\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(M_i\)&lt;/span&gt; can be effectivley approximated by
their tangent spaces.&lt;/p&gt;
&lt;p&gt;Noteworthy, limit @ref(eq:DevianceAsymptotics) holds in the small
dispersion limit &lt;span class="math inline"&gt;\(\lambda \to \infty\)&lt;/span&gt;
only, whereas limit @ref(eq:DevianceDiffAsymptotics) is also valid in
the large sample limit, essentially due to Wilks’ theorem.&lt;/p&gt;
&lt;h2 id="examples-of-edms"&gt;Examples of EDMs&lt;/h2&gt;
&lt;h4 id="univariate-gaussian"&gt;Univariate Gaussian&lt;/h4&gt;
&lt;p&gt;The univariate gaussian family &lt;span class="math inline"&gt;\(N (\mu,
\sigma^2)\)&lt;/span&gt;, with probability density function (PDF):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
f_{\mu,\sigma}(y)=\frac{1}{\sqrt {2 \pi \sigma
^2}}\exp\left[-\frac{(y-\mu)^2}{2\sigma ^2}\right] (\#eq:GaussianPDF)
\]&lt;/span&gt; corresponds to the reproductive EDM &lt;span
class="math inline"&gt;\(\text{ED}(\lambda, \,\theta,\,\kappa)\)&lt;/span&gt;
with:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\kappa (\theta) = \frac{\theta ^2}{2},\quad \theta \in \mathbb R,\quad
\lambda \in \mathbb R^+.(\#eq:GaussianEDM)
\]&lt;/span&gt; The correspondence is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\theta = \mu,\quad \lambda =
\frac{1}{\sigma^2}.(\#eq:GaussianIdentification)
\]&lt;/span&gt; The base probability measure is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text d Q_\lambda (y)=\sqrt{\frac \lambda {2\pi}}e^{-\lambda y^2/2}
\text dy(\#eq:GaussianBaseMeasure)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Legendre transform of &lt;span class="math inline"&gt;\(\kappa\)&lt;/span&gt;
is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\tau(\mu) = \frac{\mu ^2}{2} (\#eq:GaussianTau)
\]&lt;/span&gt; and the unit deviance reads:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
d(y, \hat \mu) =(y-\hat \mu)^2.(\#eq:GaussianDeviance)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id="binomial"&gt;Binomial&lt;/h4&gt;
&lt;p&gt;The binomial family &lt;span class="math inline"&gt;\(\mathcal
B(p,N)\)&lt;/span&gt; with probability mass function (PMF):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
f_{p,N}(z) = \binom{N}{z} p^z(1-p)^{N-z}(\#eq:BinomialPMF)
\]&lt;/span&gt; corresponds to the additive EDM &lt;span
class="math inline"&gt;\(\text{ED}^*(\lambda, \,\theta;\,\kappa)\)&lt;/span&gt;
with:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\kappa(\theta) = \ln (\dfrac{1+e^\theta}{2}),\quad \theta\in \mathbb R
,\quad \lambda \in \mathbb N.(\#eq:BinomialEDM)
\]&lt;/span&gt; The correspondence is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\theta = \ln\frac{p}{1-p},\quad\lambda =N.(\#eq:BinomialIdentification)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The base probability measure reads:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\frac{\text d Q_\lambda^*(z)}{\text d z} =2^{-\lambda}\sum_{i=0}
^\lambda \binom{\lambda}{i} \delta (z-i) (\#eq:BinomialBaseMeasure).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Legendre transform of &lt;span class="math inline"&gt;\(\kappa\)&lt;/span&gt;
(using &lt;span class="math inline"&gt;\(p\)&lt;/span&gt; for the mean parameter)
is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\tau(p)= \ln2+p\ln p+(1-p)\ln(1-p)(\#eq:BinomialTau)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and the unit deviance for the &lt;em&gt;reproductive&lt;/em&gt; EDM:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
d(y,\hat p)=-2y\ln (\dfrac{\hat p}{y})-2(1-y)\ln (\dfrac{1-\hat
p}{1-y}).(\#eq:BinomialDeviance)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the additive EDM, appropriate to an integer valued binomial
variable, this is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
d(z,\hat p)=-2\frac{z}{N}\ln (\dfrac{N\hat p}{z})-2(1-\frac{z}{N})\ln
(\dfrac{N-N\hat p}{N-z}).(\#eq:BinomialDevianceBis)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id="multinomial"&gt;Multinomial&lt;/h4&gt;
&lt;p&gt;The multinomial family &lt;span class="math inline"&gt;\(\text{Mult}
_{K+1}(p_1,\,p_2,\dots ,p_{K+1},\,N)\)&lt;/span&gt; for &lt;span
class="math inline"&gt;\(K+1\)&lt;/span&gt; categories is given by the PMF:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
f_{\boldsymbol p ,N}(z) = \binom{N}{z_1,\,z_2,\,\dots,\,z_{K+1}}\prod
_{k=1}^{K+1}p_k^{z_k}(\#eq:MultPMF),
\]&lt;/span&gt; In order to identify this with an EDM, we use the constraints
&lt;span class="math inline"&gt;\(\sum _{i=1}^{M+1}z_i =1\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\sum _{i=1}^{M+1}p_i =1\)&lt;/span&gt; to eliminate one
dependent variable and parameter, say &lt;span
class="math inline"&gt;\(z_{M+1}\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(p_{M+1}\)&lt;/span&gt;, respectively. The family of
densities for the resulting &lt;span
class="math inline"&gt;\(M\)&lt;/span&gt;-dimensional vector &lt;span
class="math inline"&gt;\(\boldsymbol z=(z_1\,z_2\,\dots\,z_M)^T\)&lt;/span&gt;
corresponds to the additive EDM &lt;span
class="math inline"&gt;\(\text{ED}^*(\lambda, \,\theta;\,\kappa)\)&lt;/span&gt;
with:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\quad \kappa(\theta) = \ln(\dfrac{1+\sum_{i=1}^K e^{\theta _k}}{K+1}),
\quad \theta \in \mathbb R^K,\quad \lambda \in \mathbb N(\#eq:MultEDM),
\]&lt;/span&gt; the correspondence being given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\theta _i = \ln \frac{p_i}{p_{K+1}},\quad \lambda =
N.(\#eq:MultIdentification)
\]&lt;/span&gt; The base measure:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\frac{\text d Q_\lambda^*(z)}{\text d \boldsymbol z}
=(K+1)^{-\lambda}\sum_{\boldsymbol i\in \mathbb N ^{K}\,\colon \,\sum
_{k=1}^{K}i_k\leq\lambda} \binom{\lambda}{i_1,\,i_2,\dots,i_{K+1}}
\delta (\boldsymbol z-\boldsymbol i), (\#eq:MultBaseMeasure)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(i_{K+1} = N - \sum _{k=1}
^{K} i_k\)&lt;/span&gt;. The Legendre transform of &lt;span
class="math inline"&gt;\(\kappa\)&lt;/span&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\tau(\boldsymbol p)= \ln (K+1)+\sum _{k=1}^{K+1}p_k\ln p_k(\#eq:MultTau)
\]&lt;/span&gt; and deviance is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
d(y,\hat {\boldsymbol p}) = -2\sum _{k=1}^{K+1}y_k\ln (\frac{\hat
p_k}{y_k}).(\#eq:MultDeviance)
\]&lt;/span&gt; For the additive EDM, appropriate to an integer valued
multinomial variable, this is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
d(z,\hat p)=-2\sum _{k=1}^{K+1}\frac{z_k}{N}\ln (\dfrac{N\hat
p}{z_k}).(\#eq:MultDevianceBis)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id="poisson"&gt;Poisson&lt;/h4&gt;
&lt;p&gt;The Poisson PMF is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
f(y) = \frac {\nu ^z} {z!} e^{-\nu}(\#eq:PoissonPMF)
\]&lt;/span&gt; This can be interpreted as coming from an additive EDM
with:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\kappa(\theta)=e^\theta-1,\quad \theta \in \mathbb R,\quad \lambda \in
\mathbb R^+(\#eq:PoissonEDM).
\]&lt;/span&gt; However, the correspondence is not unique, being given by the
single relation:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\lambda e^\theta = \nu (\#eq:PoissonIdentification)
\]&lt;/span&gt; which describes a curve in the &lt;span
class="math inline"&gt;\(\Theta \times \Lambda\)&lt;/span&gt; space. The
corresponding base measure is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\dfrac{\text d Q _\lambda (z)}{\text d z} = e^{-\lambda}\sum
_{k=0}^{\infty}\frac{\lambda ^k\delta(z-k)}{k!}(\#eq:PoissonBaseMeasure)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is nothing but the Poisson measure itself.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;I have mostly followed &lt;span class="citation"&gt;(Bent Jørgensen
1987)&lt;/span&gt;. References &lt;span class="citation"&gt;(B. Jørgensen
1992)&lt;/span&gt; &lt;span class="citation"&gt;(Jorgensen 1997)&lt;/span&gt; from the
same author provide more extensive expositions. A good reference for
GLMs is &lt;span class="citation"&gt;(McCullagh 2019)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id="refs" class="references csl-bib-body hanging-indent"&gt;
&lt;div id="ref-jorgensen1997theory" class="csl-entry"&gt;
Jorgensen, Bent. 1997. &lt;em&gt;The Theory of Dispersion Models&lt;/em&gt;. CRC
Press.
&lt;/div&gt;
&lt;div id="ref-jorgensen1992theory" class="csl-entry"&gt;
Jørgensen, B. 1992. &lt;em&gt;The Theory of Exponential Dispersion Models and
Analysis of Deviance&lt;/em&gt;. Monografias de Matem&lt;span&gt;á&lt;/span&gt;tica. IMPA.
&lt;a
href="https://books.google.es/books?id=twN3vgAACAAJ"&gt;https://books.google.es/books?id=twN3vgAACAAJ&lt;/a&gt;.
&lt;/div&gt;
&lt;div id="ref-jorgensen1987exponential" class="csl-entry"&gt;
Jørgensen, Bent. 1987. &lt;span&gt;“Exponential Dispersion Models.”&lt;/span&gt;
&lt;em&gt;Journal of the Royal Statistical Society: Series B
(Methodological)&lt;/em&gt; 49 (2): 127–45.
&lt;/div&gt;
&lt;div id="ref-mccullagh2019generalized" class="csl-entry"&gt;
McCullagh, Peter. 2019. &lt;em&gt;Generalized Linear Models&lt;/em&gt;. Routledge.
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes footnotes-end-of-document"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Whether a set of moments determines a unique probability
measure is called the &lt;a
href="https://en.wikipedia.org/wiki/Hamburger_moment_problem"&gt;Hamburger
moment problem&lt;/a&gt;.&lt;a href="#fnref1"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5>b23eab3a239ec49cf932769538e2531d</distill:md5>
      <guid>https://vgherard.github.io/notebooks/exponential-dispersion-models</guid>
      <pubDate>Wed, 06 Mar 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Bootstrap</title>
      <dc:creator>Valerio Gherardi</dc:creator>
      <link>https://vgherard.github.io/notebooks/bootstrap</link>
      <description>Bootstrap</description>
      <guid>https://vgherard.github.io/notebooks/bootstrap</guid>
      <pubDate>Wed, 07 Feb 2024 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Ordinary Least Squares</title>
      <dc:creator>Valerio Gherardi</dc:creator>
      <link>https://vgherard.github.io/notebooks/ordinary-least-squares</link>
      <description>Ordinary Least Squares</description>
      <guid>https://vgherard.github.io/notebooks/ordinary-least-squares</guid>
      <pubDate>Wed, 07 Feb 2024 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
