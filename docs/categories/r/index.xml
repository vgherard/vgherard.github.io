<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>Valerio Gherardi</title>
    <link>https://vgherard.github.io/</link>
    <atom:link href="https://vgherard.github.io/index.xml" rel="self" type="application/rss+xml"/>
    <description>Valerio Gherardi
</description>
    <generator>Distill</generator>
    <lastBuildDate>2023-07-26</lastBuildDate>
    <item>
      <title>AB tests and repeated checks</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2023-07-24-ab-tests-and-repeated-checks</link>
      <description>


&lt;h2 id="intro"&gt;Intro&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;“How is the experiment going?”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Also:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Do we already see something?”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And my favorite one:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Did we already hit significance, or do we need more data?”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you have dealt with experiments with relatively high outcome
expectations, you will likely have received (or perhaps asked yourself)
similar questions from time to time.&lt;/p&gt;
&lt;p&gt;In many data analysis contexts, including but not limited to
for-profit ones, researchers are always trying to come up with positive
results as fast as they can. Therefore, it is not at all surprising to
see questions such as the ones above regularly arise during the course
of an experiment. This is natural and not a problem &lt;em&gt;per se&lt;/em&gt;.
What I want to highlight and quantify in this post is how, if not done
carefully, such “real-time” monitoring schedules can seriously
invalidate data analysis - by inflating false positive and false
negative rates.&lt;/p&gt;
&lt;p&gt;Generally speaking, repeated and ad-hoc checks lead to problems of
selective/simultaneous inference (a topic which I have touched in &lt;a
href="https://vgherard.github.io/#category:Selective_Inference"&gt;other
places in this blog&lt;/a&gt;). Avoiding them is not the only valid solution -
if you want to learn about some proper method you may give a look into
&lt;a href="https://en.wikipedia.org/wiki/Sequential_analysis"&gt;Sequential
Hypothesis Testing&lt;/a&gt;, a topic that I may explore in future posts. Here
my goal is to understand the &lt;em&gt;consequences of naive repeated
checking&lt;/em&gt;, which can be easily found out through simulation.&lt;/p&gt;
&lt;h2 id="whats-the-matter-with-repeated-checks"&gt;What’s the matter with
repeated checks?&lt;/h2&gt;
&lt;p&gt;To understand why problems can arise, recall that the classical
Frequentist framework &lt;a href="#fn1" class="footnote-ref"
id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; operates by providing &lt;em&gt;a priori&lt;/em&gt;
guarantees (bounds) on the probabilities of &lt;a href="#fn2"
class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A false positive outcome in the absence of any signal: rejecting the
null hypothesis when this is actually true.&lt;/li&gt;
&lt;li&gt;A false negative outcome in the presence of some (well-defined)
signal.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;a priori&lt;/em&gt; nature of these guarantees means that they are
stipulated before running the experiment and assuming a certain
experimental &lt;em&gt;schedule&lt;/em&gt; &lt;a href="#fn3" class="footnote-ref"
id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. This implies that any departure from the
original schedule can in principle invalidate the claimed False Positive
Rate (FPR) and False Negative Rate (FNR).&lt;/p&gt;
&lt;p&gt;For instance, the most basic experimental schedule (actually the one
implicitly assumed by virtually all &lt;a
href="https://www.google.com/search?q=sample+size+calculator"&gt;sample
size calculators&lt;/a&gt; ) is:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;Collect data until reaching a prefixed sample size.&lt;/li&gt;
&lt;li&gt;Run an hypothesis test (with a prefixed significance threshold for
claiming a signal).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Common examples of departures from the original schedule include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Running several tests on partial data (before reaching the
established sample size), to look for an early signal.&lt;/li&gt;
&lt;li&gt;Stopping the experiment beforehand, because partial data doesn’t
show any signal.&lt;/li&gt;
&lt;li&gt;Prolonging the experiment after reaching the established sample
size, because there’s a “hint” to a signal, but the significance
threshold was not reached.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In what follows, I will focus on the first behavior, whose result is
to inflate the FPR. Again, there are various ways to perform repeated
checks while keeping the FPR under control, but that’s not the focus of
this post. Instead, I want to understand how FPR is affected when
&lt;em&gt;the same test is repeated several times&lt;/em&gt; on partial data.&lt;/p&gt;
&lt;h2 id="example"&gt;Example&lt;/h2&gt;
&lt;p&gt;Let me illustrate the idea with an imaginary marketing experiment.
Suppose you are optimizing an advertising campaign, say you want to test
whether a new ad design performs better than the existing one in terms
of click through rates. You start sending batches of two thousands ads&lt;a
href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; to
randomized users, half using the new design and half using the old
one.&lt;/p&gt;
&lt;p&gt;If the new design does actually perform better, you want to fully
switch to it as soon as possible, so that after each batch send, you
compare the click through rates of all ads sent so far, with the idea of
switching &lt;em&gt;as soon as a statistically significant improvement is
observed&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Concretely, you propose to do the following:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;At each step, calculate the click through rates for the new and old
designs.&lt;/li&gt;
&lt;li&gt;Compute a &lt;span class="math inline"&gt;\(p\)&lt;/span&gt;-value for the
hypothesis test&lt;a href="#fn5" class="footnote-ref"
id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; that tests whether the new design leads to
an higher click through rate than the old one.&lt;/li&gt;
&lt;li&gt;If the &lt;span class="math inline"&gt;\(p\)&lt;/span&gt;-value is smaller than
a certain fixed threshold &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt;,
stop the experiment and declare the new design as the winner.&lt;/li&gt;
&lt;li&gt;If no &lt;span class="math inline"&gt;\(p\)&lt;/span&gt;-value smaller than
&lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt; is observed after a certain
number &lt;span class="math inline"&gt;\(n\)&lt;/span&gt; of iterations, stop the
experiment and declare the old design as the winner.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, the question is: how often would the above procedure declare the
new design as the winner, if it doesn’t truly perform better than the
old one? (&lt;em&gt;i.e.&lt;/em&gt; what is the FPR of the whole procedure?)&lt;/p&gt;
&lt;h2 id="simulation"&gt;Simulation&lt;/h2&gt;
&lt;p&gt;To compute the FPR, we assume that both the new and old designs have
in fact the &lt;em&gt;same&lt;/em&gt; click through rate &lt;span
class="math inline"&gt;\(p = 10 \%\)&lt;/span&gt;. The following function
generates a sequence of &lt;span class="math inline"&gt;\(n\)&lt;/span&gt;
consecutive &lt;span class="math inline"&gt;\(p\)&lt;/span&gt;-values, computed as
described above, that one could observe under these circumstances:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;generate_p_values &amp;lt;- function(n = 28,      # maximum number of iterations
                                                            size = 1e3,  # ad sends per batch
                                                            p = 0.1      # true common click through rate
                                                            ) 
    {
    successes_a &amp;lt;- cumsum( rbinom(n = n, size = size, prob = p) )  # clicks old ad
    successes_b &amp;lt;- cumsum( rbinom(n = n, size = size, prob = p) )  # clicks new ad
    
    sapply(1:n, \(k) {
        prop.test(
            x = c(successes_a[k], successes_b[k]), 
            n = k * size * c(1, 1), 
            alternative = &amp;quot;greater&amp;quot;,
            )$p.value
    })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For instance:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(999)
( p_example &amp;lt;- generate_p_values(n = 5) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.4704229 0.3932333 0.1669308 0.2219066 0.2592812&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function below evaluates such a sequence of &lt;span
class="math inline"&gt;\(p\)&lt;/span&gt;-values with a fixed threshold &lt;span
class="math inline"&gt;\(\alpha\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;evaluate_p_values &amp;lt;- function(p, alpha = 0.05, checkpoints = seq_along(p)) {
    p &amp;lt;- p[checkpoints]
    as.logical(cumsum(p &amp;lt; alpha))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For instance, with &lt;span class="math inline"&gt;\(\alpha =
20\%\)&lt;/span&gt;, the sequence above would lead to a (false) positive
result, which would be claimed at the third check. Output looks as
follows:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;evaluate_p_values(p_example, alpha = 0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] FALSE FALSE  TRUE  TRUE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me now simulate a large number of such “experiments”. I will fix
&lt;span class="math inline"&gt;\(\alpha = 5\%\)&lt;/span&gt;, a popular choice:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(840)
sim_data &amp;lt;- replicate(1e4, generate_p_values(n = 100) |&amp;gt; evaluate_p_values())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result is a matrix whose columns are logical vectors such as the
one above:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;sim_data[,1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [11] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [21] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [31] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [41] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [51] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [71] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [81] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [91] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(a true negative result). Hence, the averages of this matrix rows
provide the false positive rates after &lt;span
class="math inline"&gt;\(n\)&lt;/span&gt; checks:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;fpr &amp;lt;- rowMeans(sim_data)
plot(fpr, type = &amp;quot;l&amp;quot;, xlab = &amp;quot;Checks&amp;quot;, ylab = &amp;quot;False Positive Rate&amp;quot;)
abline(h = 0.05, col = &amp;quot;red&amp;quot;, lty = &amp;quot;dashed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file5590970284_files/figure-html/unnamed-chunk-7-1.png" width="672" /&gt;
The curve above shows how the FPR depends on the number of checks
performed, according to the procedure described in the previous section.
For a single check, this coincides with FPR of an individual binomial
test&lt;a href="#fn6" class="footnote-ref" id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;.
However, allowing for repeated checks, we see that the overall FPR
steadily increases with number of checks. With &lt;span
class="math inline"&gt;\(n = 3\)&lt;/span&gt; checks, the FPR is already close to
&lt;span class="math inline"&gt;\(10 \%\)&lt;/span&gt;, almost twice the nominal FPR
of each individual test:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;fpr[3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.0929&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With &lt;span class="math inline"&gt;\(n \approx 40\)&lt;/span&gt; checks, the
FPR is about &lt;span class="math inline"&gt;\(25 \%\)&lt;/span&gt;, the same FPR of
an experiment that involves tossing a coin twice, declaring it biased if
the result is two consecutive “tails”.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;fpr[40]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.2471&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we are assuming that data is re-checked after the arrival of
every single batch, but there are of course infinite alternative
possibilities. For instance, the plot below shows what happens when
checks are performed after the collection of &lt;span
class="math inline"&gt;\(n = 1, \,4, \,16, \,64\)&lt;/span&gt; batches of data
(at each checkpoint, the expected size of statistical fluctuations is
reduced by a factor of &lt;span class="math inline"&gt;\(2\)&lt;/span&gt;).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;checkpoints &amp;lt;- c(1, 4, 16, 64)

set.seed(840)
fpr2 &amp;lt;- replicate(1e4, 
                    generate_p_values(n = 64) |&amp;gt; 
                        evaluate_p_values(checkpoints = checkpoints)
                    ) |&amp;gt;
    rowMeans()

plot(fpr2, 
         type = &amp;quot;b&amp;quot;, 
         xlab = &amp;quot;Checks&amp;quot;, 
         ylab = &amp;quot;False Positive Rate&amp;quot;, 
         xaxt = &amp;quot;n&amp;quot;
         )

abline(h = 0.05, col = &amp;quot;red&amp;quot;, lty = &amp;quot;dashed&amp;quot;)
axis(1, at = seq_along(checkpoints))
axis(3, at = seq_along(checkpoints), labels = paste(checkpoints, &amp;quot;K&amp;quot;))
mtext(&amp;quot;Sample Size&amp;quot;, side = 3, line = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file5590970284_files/figure-html/unnamed-chunk-10-1.png" width="672" /&gt;
As a third possible variation, we may think of applying different &lt;span
class="math inline"&gt;\(p\)&lt;/span&gt;-value thresholds at different checks (a
scheme that can be actually made to work in practice, see for instance
the Wikipedia article on the &lt;a
href="https://en.wikipedia.org/wiki/Haybittle%E2%80%93Peto_boundary"&gt;Haybittle–Peto
boundary&lt;/a&gt;). The following plot illustrates this, assuming three
(equally spaced) checks after the collection of &lt;span
class="math inline"&gt;\(n = 1,\,2,\,3\)&lt;/span&gt; data batches, using the
significance thresholds &lt;span class="math inline"&gt;\(\alpha = 0.01,
\,0.025, \,0.05\)&lt;/span&gt;, respectively.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(840)

alpha &amp;lt;- c(0.01, 0.025, 0.05)

fpr3 &amp;lt;- replicate(1e5, 
                    generate_p_values(n = 3) |&amp;gt; 
                        evaluate_p_values(alpha = alpha)
                    ) |&amp;gt;
    rowMeans()

plot(fpr3, 
         type = &amp;quot;b&amp;quot;, 
         xlab = &amp;quot;Checks&amp;quot;, 
         ylab = &amp;quot;False Positive Rate&amp;quot;, 
         xaxt = &amp;quot;n&amp;quot;
         )

abline(h = alpha[3], col = &amp;quot;red&amp;quot;, lty = &amp;quot;dashed&amp;quot;)
abline(h = alpha[2], col = &amp;quot;blue&amp;quot;, lty = &amp;quot;dashed&amp;quot;)
axis(1, at = seq_along(fpr3))
axis(3, at = seq_along(fpr3), labels = alpha)
mtext(&amp;quot;p-value threshold&amp;quot;, side = 3, line = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file5590970284_files/figure-html/unnamed-chunk-11-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h2 id="conclusions"&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;This post illustrated quantitatively how the performance of repeated
checks during the process of data collection can affect the overall
False Positive Rate of an experimental analysis. The code provided above
can be easily adapted to simulate other types of experiments and schemes
for interim checks.&lt;/p&gt;
&lt;p&gt;A question that may possibly arise is: &lt;em&gt;should I really care?&lt;/em&gt;
You could argue that what I’ve shown here represents a simple trade-off
between FPR on one side, FNR and efficiency (speed) in detection of a
signal on the other.&lt;/p&gt;
&lt;p&gt;My answer is a resounding &lt;em&gt;yes&lt;/em&gt;, irrespective of whether you
are running experiments for purely scientific or utilitaristic purposes.
If you are unable to characterize (at least approximately) the FPR and
FNR of your analysis, the whole point of running a formal test looks
very dubious to me. You may as well simply collect some data and draw an
educated guess.&lt;/p&gt;
&lt;p&gt;Other story is if you are able to tell &lt;em&gt;in advance&lt;/em&gt; how
interim checks affect FPR/FNR, and use this knowledge to optimize your
analysis strategy. This note provides some clues on how to do so.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes footnotes-end-of-document"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;I move within this framework because it is the only one
I’m reasonably comfortable with, and for which I have a decent
understanding of the decision dynamics that follow from it. That said, I
suspect that also Bayesian hypothesis testing can be affected by the
kind of issues discussed here, although perhaps in a less transparent
way, due to working with formal a posteriori probabilities.&lt;a
href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;The statistical jargon used to indicate these two types
of errors, and the corresponding a priori guarantees on their
probabilities, sounds very mysterious to me (Type I/II errors, size and
power…). I like to think in terms of “False Positive” and “False
Negative” rates, which is the same thing.&lt;a href="#fnref2"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;This is generally true, also in the aforementioned
sequential settings. In that case, the difference is that the schedule
takes into account that continuous and/or interim checks will be
performed.&lt;a href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;The actual numbers in this example may be totally
unrealistic, but that’s beside the point.&lt;a href="#fnref4"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;Technically, this would be a two-sample, one-sided
binomial test.&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;The fact that this is not exactly equal to &lt;span
class="math inline"&gt;\(\alpha\)&lt;/span&gt;, but in fact slightly smaller, is
due to the discreteness of the underlying binomial distributions. The
&lt;span class="math inline"&gt;\(p\)&lt;/span&gt;-value of the binomial test is
defined in such a way to satisfy &lt;span class="math inline"&gt;\(\text{Pr}(p
&amp;lt; \alpha)\leq \alpha\)&lt;/span&gt;.&lt;a href="#fnref6"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">cbd97ea700f2603c624c83adfd111a4b</distill:md5>
      <category>AB testing</category>
      <category>Sequential Hypothesis Testing</category>
      <category>Frequentist Methods</category>
      <category>Statistics</category>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2023-07-24-ab-tests-and-repeated-checks</guid>
      <pubDate>2023-07-26</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://vgherard.github.io/posts/2023-07-24-ab-tests-and-repeated-checks/ab-tests-and-repeated-checks_files/figure-html5/unnamed-chunk-7-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Testing functional specification in linear regression</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2023-07-11-testing-functional-specification-in-linear-regression</link>
      <description>Some options in R, using the `{lmtest}` package.</description>
      <category>Statistics</category>
      <category>Model Misspecification</category>
      <category>Regression</category>
      <category>Linear Models</category>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2023-07-11-testing-functional-specification-in-linear-regression</guid>
      <pubDate>2023-07-11</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://vgherard.github.io/posts/2023-07-11-testing-functional-specification-in-linear-regression/testing-functional-misspecification-in-linear-regression_files/figure-html5/unnamed-chunk-1-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Linear regression with autocorrelated noise</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2023-05-20-linear-regression-with-autocorrelated-noise</link>
      <description>Effects of noise autocorrelation on linear regression. Explicit formulae and a simple simulation.</description>
      <category>Statistics</category>
      <category>Regression</category>
      <category>Time Series</category>
      <category>Linear Models</category>
      <category>Model Misspecification</category>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2023-05-20-linear-regression-with-autocorrelated-noise</guid>
      <pubDate>2023-05-25</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://vgherard.github.io/posts/2023-05-20-linear-regression-with-autocorrelated-noise/linear-regression-with-autocorrelated-noise_files/figure-html5/unnamed-chunk-2-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Model Misspecification and Linear Sandwiches</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches</link>
      <description>Being wrong in the right way. With R excerpts.</description>
      <category>Statistics</category>
      <category>Regression</category>
      <category>Linear Models</category>
      <category>Model Misspecification</category>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches</guid>
      <pubDate>2023-05-14</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches/misspecification-and-linear-sandwiches_files/figure-html5/unnamed-chunk-7-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>How to get away with selection. Part I: Introduction</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2022-10-18-posi</link>
      <description>Introducing the problem of Selective Inference, illustrated through a simple simulation in R.</description>
      <category>Statistics</category>
      <category>Selective Inference</category>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2022-10-18-posi</guid>
      <pubDate>2022-11-14</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://vgherard.github.io/posts/2022-10-18-posi/posi_files/figure-html5/unnamed-chunk-3-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>kgrams v0.1.2 on CRAN</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2021-11-13-kgrams-v012-released</link>
      <description>kgrams: Classical k-gram Language Models in R.</description>
      <category>Natural Language Processing</category>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2021-11-13-kgrams-v012-released</guid>
      <pubDate>2021-11-13</pubDate>
    </item>
    <item>
      <title>R Client for R-universe APIs</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2021-07-25-r-client-for-r-universe-apis</link>
      <description>Introducing W.I.P. {runiv}, an R package to interact with R-universe 
repository APIs</description>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2021-07-25-r-client-for-r-universe-apis</guid>
      <pubDate>2021-07-25</pubDate>
    </item>
    <item>
      <title>Automatic resumes of your R-developer portfolio from your R-Universe</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2021-07-21-automatically-resume-your-r-package-portfolio-using-the-r-universe-api</link>
      <description>Create automatic resumes of your R packages using the R-Universe API.</description>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2021-07-21-automatically-resume-your-r-package-portfolio-using-the-r-universe-api</guid>
      <pubDate>2021-07-21</pubDate>
    </item>
    <item>
      <title>{r2r} now on CRAN</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2021-07-06-r2r</link>
      <description>Introducing {r2r}, an R implementation of hash tables.</description>
      <category>Data Structures</category>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2021-07-06-r2r</guid>
      <pubDate>2021-07-06</pubDate>
    </item>
  </channel>
</rss>
