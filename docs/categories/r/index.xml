<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>Valerio Gherardi</title>
    <link>https://vgherard.github.io/</link>
    <atom:link href="https://vgherard.github.io/index.xml" rel="self" type="application/rss+xml"/>
    <description>Valerio Gherardi
</description>
    <generator>Distill</generator>
    <lastBuildDate>2023-05-19</lastBuildDate>
    <item>
      <title>Linear regression with autocorrelated noise</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2023-05-20-linear-regression-with-autocorrelated-noise</link>
      <description>


&lt;p&gt;Consider two time series &lt;span class="math inline"&gt;\(Y_t\)&lt;/span&gt; and
&lt;span class="math inline"&gt;\(X_t\)&lt;/span&gt; such that:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
Y_t =  X_t \cdot \beta+\eta_t                               (\#eq:YvsX)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(\eta_t\)&lt;/span&gt; is &lt;span
class="math inline"&gt;\(\text{AR}(1)\)&lt;/span&gt; noise:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\eta_{t+1} = \alpha \eta_t + \epsilon_t, \qquad \epsilon _t \sim
\mathcal
N(0,\sigma^2_0)                                                               (\#eq:AR1)
\]&lt;/span&gt; By iteration of @ref(eq:AR1), we see that &lt;span
class="math inline"&gt;\(\eta_t\)&lt;/span&gt; has gaussian &lt;em&gt;unconditional
distribution&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\eta_t \sim \mathcal N (0, \sigma ^2),\qquad \sigma^2 \equiv
\frac{\sigma^2_0}{1-\alpha
^2}                             (\#eq:AR1Unconditional)
\]&lt;/span&gt; so that individual observations of &lt;span
class="math inline"&gt;\((X_t,\,Y_t)\)&lt;/span&gt; are distributed according to
a perfectly specified linear model.&lt;/p&gt;
&lt;p&gt;This does &lt;em&gt;not&lt;/em&gt; mean that, given observational data &lt;span
class="math inline"&gt;\(\{(X_t,\,Y_t)\}_{t = 1,\,2,\,\dots,\,T}\)&lt;/span&gt;,
we are allowed to make standard linear model assumptions to perform
valid inference on the parameters &lt;span
class="math inline"&gt;\(\beta\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\sigma\)&lt;/span&gt; of Eqs. @ref(eq:YvsX) and
@ref(eq:AR1Unconditional). Since the noise terms &lt;span
class="math inline"&gt;\(\eta _t\)&lt;/span&gt; are not independent draws from a
single distribution, but are rather autocorrelated, the usual OLS
variance estimate under linear model assumptions will be biased, as we
show below &lt;a href="#fn1" class="footnote-ref"
id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It is fairly easy to work out the consequences of autocorrelation.
Suppose, more generally, that the error term &lt;span
class="math inline"&gt;\(\eta _t\)&lt;/span&gt; is a stationary time series with
unconditional mean &lt;span class="math inline"&gt;\(\mathbb
E(\eta_t)=0\)&lt;/span&gt; and unconditional variance &lt;span
class="math inline"&gt;\(\text{Var}(\eta _t)=\sigma ^2\)&lt;/span&gt;. The OLS
estimate of &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; is&lt;a href="#fn2"
class="footnote-ref" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat \beta =(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\mathbf Y=\beta +
(\mathbf X^T\mathbf X)^{-1} \mathbf X^T \mathbf{η}, (\#eq:OLSBeta)
\]&lt;/span&gt; which is unbiased since &lt;span class="math inline"&gt;\(\mathbb E
(\mathbf{η}) = 0\)&lt;/span&gt;. The estimate of the noise variance &lt;span
class="math inline"&gt;\(\sigma ^2\)&lt;/span&gt;, on the other hand:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
\hat \sigma ^2  &amp;amp; = \frac{(\mathbf Y - \mathbf X\hat
\beta)^T(\mathbf Y - \mathbf X\hat \beta)}{N-p}=
\frac{\mathbf{η}^T(\mathbf 1-\mathbf H)\mathbf{η} }{N-p} \\
\mathbb E (\hat \sigma ^2) &amp;amp; = \dfrac{\text {Tr}\left[(\mathbf 1-
\mathbb E(\mathbf H))\cdot  \text {Cor}(\mathbf{η})\right]}{N-p}\sigma
^2                     
\end{split}
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(\mathbf H = \mathbf
X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\)&lt;/span&gt; as usual, and we have
used the fact that &lt;span class="math inline"&gt;\(\mathbb {V}( \mathbf{η} )
= \sigma ^2 \cdot \text {Cor}(\mathbf{η})\)&lt;/span&gt; (since each &lt;span
class="math inline"&gt;\(\eta_t\)&lt;/span&gt; has the same unconditional
variance &lt;span class="math inline"&gt;\(\sigma ^2\)&lt;/span&gt;). Hence the
&lt;span class="math inline"&gt;\(\hat \sigma ^2\)&lt;/span&gt; OLS estimate is
biased if &lt;span class="math inline"&gt;\(\text{Cor}(\mathbf{η})\neq \mathbf
1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Similarly, the variance-covariance matrix of the OLS &lt;span
class="math inline"&gt;\(\hat \beta\)&lt;/span&gt; estimator is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb V (\hat \beta) = \mathbb E\left[(\mathbf X^T\mathbf
X)^{-1}\mathbf X^T\text {Cor}(\mathbf{η})\mathbf X (\mathbf X^T\mathbf
X)^{-1} \right]\sigma^2
\]&lt;/span&gt; whereas its OLS estimate is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat {\mathbb V} (\hat \beta) = (\mathbf X^T\mathbf X)^{-1} \hat \sigma
^2
\]&lt;/span&gt; which is biased for &lt;span
class="math inline"&gt;\(\text{Cor}(\mathbf{η})\neq \mathbf 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Even though the variance estimators are themselves biased, the biases
could still vanish in the asymptotic limit. This is the case for &lt;span
class="math inline"&gt;\(\hat \sigma ^2\)&lt;/span&gt;, as we can see by
rewriting:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\dfrac{\mathbb E (\hat \sigma ^2)}{\sigma ^2}-1 = -\dfrac{1}{{N-p}}\text
{Tr}\left[\mathbb E(\mathbf H)^T\cdot(\text {Cor}(\mathbf{η})-\mathbf
1)\cdot \mathbb E(\mathbf H)\right]                      
\]&lt;/span&gt; where we have used the projector properties of &lt;span
class="math inline"&gt;\(\mathbf H\)&lt;/span&gt; to recast the trace in terms of
a symmetric operator. In principle, nothing prevents the operator above
to have &lt;span class="math inline"&gt;\(O(N)\)&lt;/span&gt; eigenvalues, which
would make the &lt;span class="math inline"&gt;\(\hat \sigma ^2\)&lt;/span&gt;
estimator asymptotically biased&lt;a href="#fn3" class="footnote-ref"
id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. In realistic cases, one expects the
correlations &lt;span
class="math inline"&gt;\(\text{Cor}(\eta_t,\eta_{t&amp;#39;})\)&lt;/span&gt; to decay
exponentially with &lt;span class="math inline"&gt;\(\vert t -
t&amp;#39;\vert\)&lt;/span&gt; &lt;a href="#fn4" class="footnote-ref"
id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; , in which case the trace is bounded to be
of &lt;span class="math inline"&gt;\(O(p)\)&lt;/span&gt;, and &lt;span
class="math inline"&gt;\(\mathbb E(\hat \sigma ^2) \to \sigma ^2\)&lt;/span&gt;
as &lt;span class="math inline"&gt;\(N\to \infty\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For &lt;span class="math inline"&gt;\(\hat {\mathbb V} (\hat
\beta)\)&lt;/span&gt; things are not so favorable. It is enough to consider a
special case of a plain intercept term: &lt;span
class="math inline"&gt;\(X=1\)&lt;/span&gt;. In this case, we find with some
manipulations:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
\mathbb V (\hat \beta) &amp;amp;= \frac{\sigma ^2}{N}\left(1+\frac{1}{N}\sum
_{t\neq t&amp;#39;} \text{Cor}(\eta_t,\eta_{t&amp;#39;})\right),\\
\mathbb E(\hat {\mathbb V} (\hat \beta)) &amp;amp; = \frac{\sigma
^2}{N}\left(1-\frac{1}{N(N-1)}\sum _{t\neq t&amp;#39;}
\text{Cor}(\eta_t,\eta_{t&amp;#39;})\right)
\end{split}
\]&lt;/span&gt; Since &lt;span class="math inline"&gt;\(\sum _{t\neq
t&amp;#39;}\text{Cor}(\eta_t,\eta_{t&amp;#39;})=O(N)\)&lt;/span&gt;, we see that:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\lim _{N\to \infty} \dfrac{\mathbb E(\hat {\mathbb V} (\hat
\beta))}{\mathbb V(\hat \beta)}\neq 1
\]&lt;/span&gt; which amounts to say that &lt;span class="math inline"&gt;\(\hat
{\mathbb V} (\hat \beta)\)&lt;/span&gt; is asymptotically biased&lt;a href="#fn5"
class="footnote-ref" id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="illustration"&gt;Illustration&lt;/h3&gt;
&lt;p&gt;The (foldable) block below defines helpers to simulate the results of
linear regression on data generated according to &lt;span
class="math inline"&gt;\(Y_t = f(X_t) + \eta _t\)&lt;/span&gt;. These are the
same functions used in my previous post on &lt;a
href="https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches/"&gt;misspecification
and sandwich estimators&lt;/a&gt; - slightly adapted to the current case.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(dplyr)
library(ggplot2)

rxy_fun &amp;lt;- function(rx, f, reps) {
    res &amp;lt;- function(n) {
        x &amp;lt;- rx(n)  # X has marginal distribution &amp;#39;rx&amp;#39;
        y &amp;lt;- f(x) + reps(x)  # Y has conditional mean &amp;#39;f(x)&amp;#39; and noise &amp;#39;reps(x)&amp;#39;
        return(tibble(x = x, y = y))  
    }
    return(structure(res, class = &amp;quot;rxy&amp;quot;))
}

plot.rxy &amp;lt;- function(x, N = 1000, seed = 840) {
    set.seed(seed)
    
    ggplot(data = x(N), aes(x = x, y = y)) +
        geom_point(alpha = 0.3) + 
        geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)
}

lmsim &amp;lt;- function(rxy, N = 100, vcov = stats::vcov, B = 1e3, seed = 840) 
{ 
    set.seed(seed)
    
    res &amp;lt;- list(
        coef = matrix(nrow = B, ncol = 2), 
        vcov = vector(&amp;quot;list&amp;quot;, B),
        sigma2 = numeric(B)
        )
    colnames(res$coef) &amp;lt;- c(&amp;quot;(Intercept)&amp;quot;, &amp;quot;x&amp;quot;)
    class(res) &amp;lt;- &amp;quot;lmsim&amp;quot;
                                
    for (b in 1:B) {
        .fit &amp;lt;- lm(y ~ ., data = rxy(N))
        res$coef[b, ] &amp;lt;- coef(.fit)  # Store intercept and slope in B x 2 matrix
        res$vcov[[b]] &amp;lt;- vcov(.fit)  # Store vcov estimates in length B list.
        res$sigma2[[b]] &amp;lt;- sigma(.fit) ^ 2
    }
    
    return(res)
}

print.lmsim &amp;lt;- function(x) 
{
    cat(&amp;quot;Simulation results:\n\n&amp;quot;)
    cat(&amp;quot;* Model-trusting noise variance:\n &amp;quot;)
    print( mean(x$sigma2) )
    cat(&amp;quot;* Model-trusting vcov of coefficient estimates:\n&amp;quot;)
    print( avg_est_vcov &amp;lt;- Reduce(&amp;quot;+&amp;quot;, x$vcov) / length(x$vcov) )
    cat(&amp;quot;\n* Simulation-based vcov of coefficient estimates:\n&amp;quot;)
    print( emp_vcov &amp;lt;- cov(x$coef))
    cat(&amp;quot;\n* Ratio (Model-trusting / Simulation):\n&amp;quot;)
    print( avg_est_vcov / emp_vcov )
    return(invisible(x))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We simulate linear regression on data generated according to:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
Y_t &amp;amp;= 1 + X_t+\eta_t,\\
X_{t+1} &amp;amp;= 0.4 \cdot X_t+Z^X_t,\\
\eta _{t+1} &amp;amp;= \frac{1}{\sqrt 2}\eta _t +Z^\eta_t\\
\end{split}
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(Z^{X,\eta}_t\sim \mathcal
N(0,1)\)&lt;/span&gt;. The noise &lt;span class="math inline"&gt;\(\eta_t\)&lt;/span&gt;
is &lt;span class="math inline"&gt;\(\text{AR}(1)\)&lt;/span&gt;, and results in the
unconditional variance of the corresponding linear model &lt;span
class="math inline"&gt;\(\text{Var} (\eta _t) = 2\)&lt;/span&gt;, twice the
conditional variance &lt;span class="math inline"&gt;\(\text{Var}(\eta
_{t+1}\vert \eta _t)=\mathbb E(Z_t ^2)=1\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;rxy_01 &amp;lt;- rxy_fun(
    rx = \(n) 1 + arima.sim(list(order = c(1,0,0), ar = 0.4), n = n),
    f = \(x) 1 + x,
    reps = \(x) arima.sim(
        list(order = c(1,0,0), ar = 1/sqrt(2)), 
        n = length(x) 
        )
)

plot(rxy_01)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file270744faa3fe_files/figure-html/unnamed-chunk-2-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;From the simulation below, we see that with &lt;span
class="math inline"&gt;\(N=100\)&lt;/span&gt; serial observations, &lt;span
class="math inline"&gt;\(\mathbb E(\hat \sigma ^2)\)&lt;/span&gt; is relatively
close to &lt;span class="math inline"&gt;\(\sigma ^2 = 2\)&lt;/span&gt;, but the
&lt;span class="math inline"&gt;\(\mathbb E(\hat {\mathbb V} (\hat
\beta))\)&lt;/span&gt; grossly underestimates all entries (as can be seen from
the last line of the output of &lt;code&gt;lmsim()&lt;/code&gt; below).&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;lmsim(rxy_01, N = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Simulation results:

* Model-trusting noise variance:
 [1] 1.870606
* Model-trusting vcov of coefficient estimates:
            (Intercept)           x
(Intercept)  0.03583159 -0.01663739
x           -0.01663739  0.01659708

* Simulation-based vcov of coefficient estimates:
            (Intercept)           x
(Intercept)  0.15486131 -0.02665435
x           -0.02665435  0.02978162

* Ratio (Model-trusting / Simulation):
            (Intercept)         x
(Intercept)   0.2313786 0.6241905
x             0.6241905 0.5572928&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To correctly estimate &lt;span class="math inline"&gt;\(\mathbb V (\hat
\beta)\)&lt;/span&gt;, we could try using the “autocorrelation-consistent”
sandwich estimator &lt;code&gt;sandwich::vcovHAC()&lt;/code&gt; &lt;a href="#fn6"
class="footnote-ref" id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;. It turns out that,
even with a relatively simple example like the present one, the sample
size required for the HAC estimator’s bias to die out is unreasonably
large (see below). With such large samples, one can probably obtain much
better results by leaving out some data for model building, performing
inference on the remaining data with a proper time-series model.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;lmsim(rxy_01, vcov = sandwich::vcovHAC, N = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Simulation results:

* Model-trusting noise variance:
 [1] 1.870606
* Model-trusting vcov of coefficient estimates:
            (Intercept)           x
(Intercept)  0.08787795 -0.02323242
x           -0.02323242  0.02339146

* Simulation-based vcov of coefficient estimates:
            (Intercept)           x
(Intercept)  0.15486131 -0.02665435
x           -0.02665435  0.02978162

* Ratio (Model-trusting / Simulation):
            (Intercept)         x
(Intercept)   0.5674623 0.8716182
x             0.8716182 0.7854329&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;lmsim(rxy_01, vcov = sandwich::vcovHAC, N = 500)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Simulation results:

* Model-trusting noise variance:
 [1] 1.974131
* Model-trusting vcov of coefficient estimates:
             (Intercept)            x
(Intercept)  0.023032270 -0.005723968
x           -0.005723968  0.005684149

* Simulation-based vcov of coefficient estimates:
             (Intercept)            x
(Intercept)  0.029600757 -0.005993161
x           -0.005993161  0.006152216

* Ratio (Model-trusting / Simulation):
            (Intercept)         x
(Intercept)   0.7780973 0.9550834
x             0.9550834 0.9239189&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;lmsim(rxy_01, vcov = sandwich::vcovHAC, N = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Simulation results:

* Model-trusting noise variance:
 [1] 1.98033
* Model-trusting vcov of coefficient estimates:
             (Intercept)            x
(Intercept)  0.011878079 -0.002771484
x           -0.002771484  0.002849089

* Simulation-based vcov of coefficient estimates:
             (Intercept)            x
(Intercept)  0.015085291 -0.002844716
x           -0.002844716  0.002855522

* Ratio (Model-trusting / Simulation):
            (Intercept)         x
(Intercept)   0.7873948 0.9742566
x             0.9742566 0.9977471&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes footnotes-end-of-document"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;For the linear model assumptions to hold, the &lt;span
class="math inline"&gt;\((X_t,\,Y_t)\)&lt;/span&gt; pairs should come from
&lt;em&gt;independent realizations&lt;/em&gt; of the same time series, which is of
course not the type of data we are usually presented with.&lt;a
href="#fnref1" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;As usual we stack observations vertically in the &lt;span
class="math inline"&gt;\(\mathbf X\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\mathbf Y\)&lt;/span&gt; matrices.&lt;a href="#fnref2"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;For an extreme case, suppose that &lt;span
class="math inline"&gt;\(\mathbf X = \mathbf e\)&lt;/span&gt; (no covariate
except for an intercept term), and let the noise term be &lt;span
class="math inline"&gt;\(\eta _t = Z_0 + Z_t\)&lt;/span&gt;, where &lt;span
class="math inline"&gt;\(Z_0\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\{Z_t\}_{t=1,2,\dots,T}\)&lt;/span&gt; are independent
&lt;span class="math inline"&gt;\(Z\)&lt;/span&gt;-scores. One can easily see that,
in this setting, &lt;span class="math inline"&gt;\(\text {Cor}(\eta) =
\frac{1}{2}(\mathbf 1+\mathbf e \mathbf e^T )\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\text{Tr}(\cdots) \approx \frac{N}{2}\)&lt;/span&gt;.&lt;a
href="#fnref3" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;For instance, for the &lt;span
class="math inline"&gt;\(\text{AR}(1)\)&lt;/span&gt; noise of Eq. @ref(eq:AR1),
we have &lt;span class="math inline"&gt;\(\text{Cor}(\eta_t, \eta_{t&amp;#39;})=
\alpha ^{\vert t - t&amp;#39;\vert}\)&lt;/span&gt;.&lt;a href="#fnref4"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;The difference &lt;span class="math inline"&gt;\(\mathbb
E(\hat {\mathbb V} (\hat \beta))-\mathbb V(\hat \beta)\)&lt;/span&gt; decays
as &lt;span class="math inline"&gt;\(O(N^{-1})\)&lt;/span&gt;, which is of the same
order of the estimation target &lt;span class="math inline"&gt;\(\mathbb V
(\hat \beta)\)&lt;/span&gt;. Not sure I’m using standard terminology here.&lt;a
href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;Disclaimer: I haven’t read any theory about the HAC
estimator, so I may be misusing it here, but I would have expected it to
work relatively well on such an “easy” example. For illustrations on how
to use sandwich estimators for first- and second-order linear model
misspecification, you can read &lt;a
href="https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches/"&gt;this
post of mine&lt;/a&gt;.&lt;a href="#fnref6" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5 xmlns:distill="https://distill.pub/journal/">b539717c0543de8c5787acda9bbbd811</distill:md5>
      <category>Statistics</category>
      <category>Regression</category>
      <category>Time Series</category>
      <category>Linear Models</category>
      <category>Model Misspecification</category>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2023-05-20-linear-regression-with-autocorrelated-noise</guid>
      <pubDate>2023-05-19</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://vgherard.github.io/posts/2023-05-20-linear-regression-with-autocorrelated-noise/linear-regression-with-autocorrelated-noise_files/figure-html5/unnamed-chunk-2-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Model Misspecification and Linear Sandwiches</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches</link>
      <description>Being wrong in the right way. With R excerpts.</description>
      <category>Statistics</category>
      <category>Regression</category>
      <category>Linear Models</category>
      <category>Model Misspecification</category>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches</guid>
      <pubDate>2023-05-14</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches/misspecification-and-linear-sandwiches_files/figure-html5/unnamed-chunk-7-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>How to get away with selection. Part I: Introduction</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2022-10-18-posi</link>
      <description>Introducing the problem of Selective Inference, illustrated through a simple simulation in R.</description>
      <category>Statistics</category>
      <category>Selective Inference</category>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2022-10-18-posi</guid>
      <pubDate>2022-11-14</pubDate>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://vgherard.github.io/posts/2022-10-18-posi/posi_files/figure-html5/unnamed-chunk-3-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>kgrams v0.1.2 on CRAN</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2021-11-13-kgrams-v012-released</link>
      <description>kgrams: Classical k-gram Language Models in R.</description>
      <category>Natural Language Processing</category>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2021-11-13-kgrams-v012-released</guid>
      <pubDate>2021-11-13</pubDate>
    </item>
    <item>
      <title>R Client for R-universe APIs</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2021-07-25-r-client-for-r-universe-apis</link>
      <description>Introducing W.I.P. {runiv}, an R package to interact with R-universe 
repository APIs</description>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2021-07-25-r-client-for-r-universe-apis</guid>
      <pubDate>2021-07-25</pubDate>
    </item>
    <item>
      <title>Automatic resumes of your R-developer portfolio from your R-Universe</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2021-07-21-automatically-resume-your-r-package-portfolio-using-the-r-universe-api</link>
      <description>Create automatic resumes of your R packages using the R-Universe API.</description>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2021-07-21-automatically-resume-your-r-package-portfolio-using-the-r-universe-api</guid>
      <pubDate>2021-07-21</pubDate>
    </item>
    <item>
      <title>{r2r} now on CRAN</title>
      <dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2021-07-06-r2r</link>
      <description>Introducing {r2r}, an R implementation of hash tables.</description>
      <category>Data Structures</category>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2021-07-06-r2r</guid>
      <pubDate>2021-07-06</pubDate>
    </item>
  </channel>
</rss>
