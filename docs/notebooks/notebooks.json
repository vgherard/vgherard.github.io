[
  {
    "path": "notebooks/ordinary-least-squares/",
    "title": "Ordinary Least Squares",
    "description": {},
    "author": [
      {
        "name": "Valerio Gherardi",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2024-02-07",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nGeneralities\r\nThe linear model\r\nProperties of OLS estimates\r\nDistributional properties of \\(\\hat \\beta\\)\r\nVariance estimates\r\nMore on residuals\r\n\r\nRelated posts\r\n\r\nGeneralities\r\nOrdinary Least Squares (OLS) is a regression algorithm for estimating the best linear predictor \\(\\hat Y = X\\beta\\) of a response \\(Y \\in \\mathbb R\\) in terms of a vector of regressors \\(X\\in \\mathbb R ^p\\), which we will frequently identify with an \\(1\\times p\\) row matrix. Here, “best” is understood in terms of the \\(L_2\\) error:\r\n\\[\r\n\\beta = \\arg \\min _{\\beta '}  \\mathbb E[(Y - X\\beta ^\\prime)^2]=\\mathbb E (X^TX)^{-1}\\mathbb E(X^T Y), \\tag{1}\r\n\\]\r\nwhere the first equation is the defining one, while the second one follows from elementary calculus.\r\nGiven i.i.d. data \\(\\{(X_i,\\,Y_i)\\}_{i=1,\\,2,\\,\\dots,\\,N}\\), and denoting by \\(\\mathbf Y\\) and \\(\\mathbf X\\) the \\(N\\times 1\\) and \\(N\\times p\\) matrices obtained by vertically stacking independent observations of \\(Y\\) and \\(X\\), respectively, the OLS estimate of (1) is defined by:\r\n\\[\r\n\\hat \\beta = \\arg \\min _{\\beta '}  \\sum _{i=1} ^N \\frac{1}{N}(Y_i - X_i\\beta ^\\prime)^2 = (\\mathbf X ^T \\mathbf X) ^{-1} \\mathbf X ^T \\mathbf Y, \\tag{2}\r\n\\]\r\nwhich is readily recognized to be the plugin estimate of \\(\\beta\\). Correspondingly, we define the OLS predictor:\r\n\\[\r\n\\hat Y (x) = x \\hat \\beta. \\tag{3}\r\n\\]\r\nWhat motivates the use of an \\(L_2\\) criterion in (1)?\r\nMathematical tractability. The fact that (1) admits a closed form solution, which is furthermore linear in the response variable \\(Y\\), greatly simplifies the analysis of the properties of \\(\\beta\\) and its estimators such as the OLS one (2), making it a perfect study case.\r\nNumerical tractability. A consequence of the previous point, but worth a separate mention. Computing the plugin estimate in (2) is just a matter of basic linear algebra manipulations, which, with modern software libraries, is a relatively cheap operation.\r\nNormal theory. Focusing on the consequence of Eq. (1), namely the plugin estimate (2), if the conditional distribution of \\(Y\\) given \\(X\\) is normal with constant variance, and if \\(\\mathbb E(Y\\vert X)\\) is truly linear, \\(\\beta\\) coincides with the maximum likelihood estimate of \\(\\beta\\).\r\nThe linear model\r\nThe term generally refers to a model for the conditional distribution of \\(Y\\vert X\\) that requires the conditional mean \\(\\mathbb E(Y\\vert X)\\) to be a linear function of \\(X\\). In its most parsimonious form, this is just:\r\n\\[\r\nY = X \\beta + \\varepsilon, \\quad \\mathbb E(\\varepsilon\\vert X) = 0.\\tag{4}\r\n\\]\r\nThat said, depending on context, (4) is usually supplemented with additional assumptions that further characterise the conditional distribution of the error term1, typically (with increasing strength of assumptions):\r\nConstant Variance. \\(\\mathbb V(\\varepsilon \\vert X) = \\sigma ^2\\), independently of \\(X\\).\r\n\\(X\\)-Independent Errors. \\(\\varepsilon \\perp X\\).\r\nNormal Errors. \\(\\varepsilon \\vert X \\sim \\mathcal N (0,\\sigma ^2)\\).\r\nWhile the OLS estimator is well-defined irrespective of the validity of any of these models, it is clear that, in order for \\(\\hat \\beta\\) to represent a meaningful summary of the \\(Y\\)-\\(X\\) dependence, one should require at least (4) to hold in some approximate sense. Correspondingly, while some general features of \\(\\hat \\beta\\) can be discussed independently of linear model assumptions, its most important properties crucially depend on Eq. (4).\r\nProperties of OLS estimates\r\nDistributional properties of \\(\\hat \\beta\\)\r\nAs an estimator of \\(\\beta\\) as defined in Eq. (1), the OLS estimator \\(\\hat \\beta\\) is consistent (converges in probability to \\(\\beta\\)) but generally biased (an example is provided here). However, due to the plugin nature of \\(\\hat \\beta\\), the bias is generally of order \\(\\mathcal O (N^{-1})\\), which makes it often negligible in comparison to its \\(\\mathcal O(N^{-1/2})\\) sampling variability (see below).\r\nExplicitly, the bias is given by:\r\n\\[\r\n\\mathbb E(\\hat \\beta) - \\beta = \\mathbb E\\lbrace((\\mathbf X ^T \\mathbf X) ^{-1}-\\mathbb E[(\\mathbf X ^T \\mathbf X) ^{-1}])\\cdot \\mathbf X ^T f (\\mathbf X)\\rbrace, \\tag{5}\r\n\\]\r\nwhere \\(f(X) \\equiv \\mathbb E(Y \\vert X)\\) is the true conditional mean function. In general, this vanishes only if \\(f(X)=X\\beta\\), as in the linear expectation model (4).\r\nThe \\(\\mathbf X\\)-conditional variance of \\(\\hat \\beta\\) can be derived directly from Eq. (2):\r\n\\[\r\n\\mathbb V (\\hat \\beta \\vert \\mathbf X)=(\\mathbf X ^T \\mathbf X) ^{-1} \\mathbf X ^T  \\mathbb V (\\mathbf Y\\vert \\mathbf X) \\mathbf X (\\mathbf X ^T \\mathbf X), \\tag{6}\r\n\\]\r\nwhere \\(\\mathbb V (\\mathbf Y\\vert \\mathbf X)\\) is diagonal for i.i.d. observations. For homoskedastic errors we get:\r\n\\[\r\n\\mathbb V (\\hat \\beta \\vert \\mathbf X)=(\\mathbf X ^T \\mathbf X) ^{-1} \\sigma ^2 \\quad (\\mathbb V(Y\\vert X)=\\sigma ^2).\\  \\tag{7}\r\n\\]\r\nUnder the normal linear model, this allows to obtain finite-sample correct confidence sets for \\(\\beta\\). In the general case, confidence sets can be derived from the Central Limit Theorem satisfied by \\(\\hat \\beta\\) (Buja et al. 2019):\r\n\\[\r\n\\sqrt N (\\hat \\beta -\\beta) \\to \\mathcal N (0,V ) \\tag{8}\r\n\\]\r\nwhere the asymptotic variance is given by:\r\n\\[\r\nV = \\mathbb E[X^TX] ^{-1} \\cdot \\mathbb E[X^T(Y-X\\beta)^2X] \\cdot \\mathbb E[X^TX] ^{-1}.\\tag{9}\r\n\\]\r\nThe plugin estimate of (9) leads to the so called Sandwich variance estimator:\r\n\\[\r\nV_\\text{sand} \\equiv  (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T D_\\mathbf {r^2}\\mathbf X (\\mathbf X^T \\mathbf X)^{-1},\\tag{10}\r\n\\]\r\nwhere \\(D_{\\mathbf r^2}\\) is the diagonal matrix whose \\(i\\)-th entry is the squared residual \\(r_i ^2 = (Y_i-X_i\\beta)^2\\).\r\nProof of Central Limit Theorem for \\(\\hat \\beta\\)\r\nConvergence to the normal distribution (8) with variance (9) can be proved using the formalism of influence functions. From Eq. (1), we see that a small variation \\(P\\to P+\\delta P\\) to the joint \\(XY\\) probability measure induces a first order shift:\r\n\\[\r\n\\delta \\beta =  \\intop \\mathbb E(X^TX)^{-1}X^T (Y-\\beta X) \\text d(\\delta P)\\tag{11}\r\n\\]\r\nin the best linear predictor. The influence function of \\(\\beta\\) is defined by the measurable representation of \\(\\delta \\beta\\), namely:\r\n\\[\r\n\\phi _\\beta = \\mathbb E(X^TX)^{-1}X^T (Y-\\beta X). \\tag{12}\r\n\\]\r\nA general result for plugin estimates then tells us that \\(\\sqrt N (\\hat \\beta -\\beta) \\to \\mathcal N (0, \\mathbb E (\\phi _\\beta ^2))\\) in distribution, and using the explicit form of (12) we readily obtain (9).\r\nVariance estimates\r\nThese can be based on:\r\n\\[\r\ns ^2 = \\frac{1}{N}(\\mathbf Y-\\mathbf X \\hat \\beta)^T(\\mathbf Y-\\mathbf X \\hat \\beta)\r\n\\]\r\nwhich has expectation\r\n\\[\r\n\\mathbb E\\left[s^2\\right]=\\frac{1}{N}\\text {Tr}\\,\\mathbb E \\left[ (1-\\mathbf H) \\cdot \\left(\\mathbb E(\\mathbf Y \\vert \\mathbf X)\\mathbb E(\\mathbf Y \\vert \\mathbf X)^T + \\mathbb V(\\mathbf Y\\vert\\mathbf X)\\right) \\right] \\tag{13}\r\n\\]\r\nwhere the hat matrix \\(\\mathbf H\\) is defined as usual:\r\n\\[\r\n\\mathbf H \\equiv \\mathbf X(\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T\\tag{14}\r\n\\]\r\nIf the general linear model (4) holds, so that \\(E(\\mathbf Y \\vert \\mathbf X) = \\mathbf X \\beta\\), we have \\((1-\\mathbf H) \\mathbb E(\\mathbf Y \\vert \\mathbf X) = 0\\). If we furthermore assume homoskedasticity, we obtain:\r\n\\[\r\n\\mathbb E\\left[s^2\\right]=\\frac{N-p}{N}\\sigma^2 \\quad(\\text{Homoskedastic linear model}), \\tag{15}\r\n\\]\r\nwhere \\(p = \\text {Tr}(\\mathbf H)\\) is the number of independent covariates. On the other hand, if homoskedasticity holds, but \\(E(\\mathbf Y \\vert \\mathbf X)\\) is not linear, the left-hand side of the previous equation is an overestimate of \\(\\mathbf V \\vert X\\).\r\nMore on residuals\r\nIn order to study the properties of residuals, it is convenient to define the functional:\r\n\\[\r\n\\beta (Q) \\equiv \\arg \\min _{\\beta '}  \\mathbb E_Q[(Y - X\\beta ^\\prime)^2]=\\mathbb E_Q (X^TX)^{-1}\\mathbb E_Q(X^T Y), \\tag{16}\r\n\\]\r\nwhich depends on the arbitrary measure \\(Q\\). If \\(Q=P\\) is the original joint \\(XY\\) probability measure, we get \\(\\beta(P) \\equiv \\beta\\) as defined by Eq. (1), whereas if \\(Q = \\hat P\\) is the empirical \\(XY\\) distribution, we get \\(\\beta (\\hat P) \\equiv \\hat \\beta\\) of Eq. (2).\r\nFrom the definition it easily follows that:\r\n\\[\r\n\\mathbb E _Q [X^T (Y-X\\beta(Q))]=0. \\tag{17}\r\n\\]\r\nWith \\(Q=P\\), the previous equation yields:\r\n\\[\r\n\\mathbb E(X^T(Y-X\\beta))=0, \\tag{18}\r\n\\]\r\nwhile setting \\(Q=\\hat P\\) we obtain:\r\n\\[\r\n\\mathbf X^T(\\mathbf Y - \\mathbf X \\hat \\beta)=0. \\tag{19}\r\n\\]\r\nThese orthogonality properties, satisfied by the sample and population residuals respectively, are independent of the actual distribution of \\(X\\) and \\(Y\\), and are simple consequences of the definition of \\(\\beta\\). In particular, if \\(X\\) contains an intercept term, for example \\(X^1 = 1\\), Eqs. (18) and (19) imply that population (sample) residuals have vanishing expectation (sample mean).\r\nRelated posts\r\nConsistency and bias of OLS estimators\r\nModel misspecification and linear sandwiches\r\nLinear regression with autocorrelated noise\r\nTesting functional specification in linear regression\r\n\r\n\r\n\r\nBuja, Andreas, Richard Berk, Lawrence Brown, Edward George, Emil Pitkin, Mikhail Traskin, Linda Zhao, and Kai Zhang. 2019. “Models as Approximations i: Consequences Illustrated with Linear Regression.” https://arxiv.org/abs/1404.1578.\r\n\r\n\r\nI use the notation \\(A\\perp B\\) to indicate that the random variables \\(A\\) and \\(B\\) are statistically independent.↩︎\r\n",
    "preview": {},
    "last_modified": "2024-02-11T08:42:19+01:00",
    "input_file": {}
  },
  {
    "path": "notebooks/bootstrap/",
    "title": "Bootstrap",
    "description": {},
    "author": [
      {
        "name": "Valerio Gherardi",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2024-02-07",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nThe plugin principle\r\nThe role of simulation\r\n[TODO] Standard bootstrap estimates\r\nVariance of an estimator\r\nConfidence intervals\r\n\\(p\\)-values\r\nPrediction error\r\n\r\n[TODO] Bootstrap for time series data\r\n[TODO] Bootstrap for text data\r\n\r\nIntroduction\r\nThe Bootstrap (B. Efron 1979; Bradley Efron and Tibshirani 1994) is a set of computational techniques for statistical inference that generally operate by approximating the distribution of a population of interest with an empirical estimate obtained from a finite sample. Such methods find practical use in all those situations when the true distribution is either unknown, due to limited knowledge of the data generating process, or impossible to compute in practice.\r\nIn general, bootstrap algorithms consist of two ingredients:\r\nA plugin principle, i.e. a substitution rule \\(P\\to\\hat P\\) that replaces the true data distribution \\(P\\) with an empirical estimate \\(\\hat P\\) obtained from a finite sample.\r\nA calculation scheme for computing functionals of the plugin distribution \\(\\hat P\\), usually involving simulation.\r\nThe plugin principle\r\nThe main theoretical idea behind the bootstrap can be sketched with a non-parametric example. Consider a functional \\(t=t(P)\\) of a probability measure \\(P\\) which admits a first order expansion :\r\n\\[\r\nt(Q) \\approx t(P) + L(P; Q - P),\\tag{1}\r\n\\]\r\nwhere \\(L(P;\\nu)\\) is assumed to be a linear functional of \\(\\nu\\). If \\(Q\\) has finite support, linearity implies that:\r\n\\[\r\nL(P,Q-P) = \\intop \\psi _P\\,\\text dQ\\tag{2},\r\n\\]\r\nand we shall further assume that such a representation is valid for any \\(Q\\)1. Notice in particular, that from this definition we have:\r\n\\[\r\n\\mathbb E(\\psi _P) = L(P,P-P) = 0\\tag{3}\r\n\\]\r\nSuppose now that we have a sample of \\(N\\) i.i.d. observations \\(\\{X_1,\\,X_2,\\,\\dots,\\,X_N\\}\\) coming from \\(P\\), and let \\(Q=\\hat P _N\\) in the previous expression, where \\(\\hat P _N = \\frac{1}{N}\\sum _{i=1}^N\\delta _{X_i}\\) stands for the empirical distribution. Then:\r\n\\[\r\nt(\\hat P _N) \\approx t(P) + L(P; \\hat P _N - P) = t(P) + \\frac{1}{N}\\sum _{i = 1} ^N \\psi(X_i).\\tag{4}\r\n\\]\r\nIt follows that \\(t(\\hat P _N)\\), i.e. the so-called “plugin” estimate, is a consistent estimate of \\(t(P)\\), with:\r\n\\[\r\n\\mathbb E(t(\\hat P _N)) \\approx t(P),\\quad \\mathbb V(t(\\hat P_N)) \\approx \\frac{1}{N}\\mathbb V(\\psi(X)),\\tag{5}\r\n\\]\r\nwhere the first equation follows from (3), while the second one follows from the i.i.d. nature of the sample.\r\nThe main idea behind the non-parametric bootstrap is to estimate \\(t(P)\\) with \\(t(\\hat P _N)\\), which is justified by Eq. (5) whenever the \\(\\mathcal O (N^{-1})\\) variance can be considered negligible (as is often the case in concrete bootstrap applications). In a parametric setting, the role of \\(\\hat P_N\\) could be played by some parametric estimate of \\(P\\). Similarly, sampling schemes other than i.i.d. (e.g. if dealing with time series data) require different empirical estimates of \\(P\\), but the basic principle - estimating \\(t(P)\\) with \\(t(\\hat P_N)\\) - remains the same.\r\nEstimates such as \\(t(\\hat P_N)\\) are usually referred to as ideal bootstrap estimates. As implied by the name, they can rarely be computed exactly in practice, because no analytic formula exists, and exact numerical calculations rapidly become prohibitive with growing \\(N\\). This leads to \\(t(\\hat P_N)\\) being estimated through simulation from \\(\\hat P _N\\), as explained below.\r\nA technical refinement\r\nIn many practical applications, the functional of interest \\(t(P)\\) would itself depend on \\(N\\), so that we should actually write \\(t_N(P)\\). A relevant example would be the variance of a plugin estimate \\(v_N(P)\\equiv\\mathbb V (t(\\hat P _N))\\). In this and similar cases, in which \\(v_N=\\mathcal O (N^{-\\alpha})\\) the argument can be repeated mutatis mutandis for the functional \\(V_N = N^\\alpha \\cdot v_N\\), which has a finite limit \\(V_N \\to V\\), assumed to be different from zero. Specifically, if we let \\(V_N = V+\\Delta _N\\) we have:\r\n\\[\r\nV_N(\\hat P_N) -V_N(P) = V(\\hat P_N)-V(P)+\\Delta _N (\\hat P_N)-\\Delta_N(P).\r\n\\]\r\nSince both terms in the right hand side have vanishing (to first order) expectation and \\(\\mathcal O (N^{-1})\\) variance, this shows that we can use \\(V_N(\\hat P _N)\\) to approximate the target quantity \\(V_N(P)\\), or equivalently \\(v_N(\\hat P _N)\\) to approximate \\(v_N(P)\\), since \\(\\frac{\\mathbb E(v_N(\\hat P_N))}{v_N(P)}\\approx 1\\) and \\(\\frac{\\sqrt {\\mathbb V(v_N(\\hat P_N))}}{v_N(P)}=\\mathcal O (N^{-1/2})\\).\r\nThe role of simulation\r\nThe second, more case specific, ingredient of a practical bootstrap estimate is an approximation scheme for effectively computing \\(t(\\hat P _N)\\). These methods typically involve simulating from \\(\\hat P_N\\), the reason being that the functional \\(t(Q)\\) usually involves expectations and/or quantiles of random variables of samples from \\(Q\\). When \\(Q = \\hat P_N\\), simulation allows to obtain \\(t(\\hat P_N)\\) by brute force.\r\nThis is best clarified with an example. Given a functional \\(\\theta(P)\\), we let:\r\n\\[\r\nv_N (P) = \\mathbb V_P(\\theta (\\hat P_N))\\tag{6}\r\n\\]\r\ndenote the variance (with respect to the original measure \\(P\\)) of its plugin estimate from a dataset of \\(N\\) i.i.d. observations. The ideal bootstrap estimate is given by:\r\n\\[\r\nv_N(\\hat P_N) = \\mathbb V_{\\hat P _N}(\\theta(\\hat P_N^*)),\\tag{7}\r\n\\]\r\nwhere \\(\\hat P _N ^*\\) denotes the empirical distribution of an i.i.d. sample of \\(N\\) elements from \\(\\hat P_N\\) - obviously, i.i.d. sampling from \\(\\hat P _N\\) is the same as sampling with replacement from the original dataset. Suppose now we generate \\(B\\) synthetic datasets of size \\(N\\) by sampling with replacement, and let \\(\\hat P_N ^{(b)*}\\) denote the corresponding empirical distributions. We can then estimate (7) by:\r\n\\[\r\n\\widetilde {v_N(\\hat P_N)} = \\dfrac{1}{B-1}\\sum_{b=1}^{B}(\\theta(\\hat P_N ^{(b)*})- \\overline \\theta^*)^2,\\tag{8}\r\n\\]\r\nwhere \\(\\overline \\theta^* = \\frac{1}{B}\\sum_{b=1}^{B}\\theta(\\hat P_N ^{(b)*})\\). This would be our practical (as opposed to ideal) bootstrap estimate of \\(v_N(P)\\).\r\nStrictly speaking, the practical bootstrap estimate involves again an application of the plugin principle mentioned in the previous section, in which however the role of the true distribution \\(P\\) is played by the empirical estimate \\(\\hat P_N\\), from which we can sample without any limits. This means that (re)sampling variability associated with (8) can be always made arbitrarily small, at least in principle, simply by increasing \\(B\\).\r\n[TODO] Standard bootstrap estimates\r\nVariance of an estimator\r\nConfidence intervals\r\n\\(p\\)-values\r\nPrediction error\r\n[TODO] Bootstrap for time series data\r\n[TODO] Bootstrap for text data\r\n\r\n\r\n\r\nEfron, B. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26. https://doi.org/10.1214/aos/1176344552.\r\n\r\n\r\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to the Bootstrap. CRC press.\r\n\r\n\r\nHuber, Peter J. 2004. Robust Statistics. Vol. 523. John Wiley & Sons.\r\n\r\n\r\nThe integral representation (2) with bounded \\(\\psi _P\\) automatically follows if \\(L\\) is (weak-star) continuous and Frechét differentiable (Huber 2004). In the most general case, the sense in which Eq. (1) is assumed to hold is that of a directional (Gateaux) derivative, and we assume Eq. (2) to hold for some measurable (not necessarily bounded) function \\(\\psi _P\\), which is usually easy to verify in concrete cases.↩︎\r\n",
    "preview": {},
    "last_modified": "2024-02-15T15:03:43+01:00",
    "input_file": "bootstrap.knit.md"
  }
]
