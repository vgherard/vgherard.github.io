[
  {
    "path": "notebooks/ordinary-least-squares/",
    "title": "Ordinary Least Squares",
    "description": {},
    "author": [
      {
        "name": "Valerio Gherardi",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2024-02-07",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nGeneralities\r\nThe linear model\r\nProperties of OLS estimates\r\nDistributional properties of \\(\\hat \\beta\\)\r\nVariance estimates\r\nMore on residuals\r\n\r\n\r\nGeneralities\r\nOrdinary Least Squares (OLS) is a regression algorithm for estimating the best linear predictor \\(\\hat Y = X\\beta\\) of a response \\(Y \\in \\mathbb R\\) in terms of a vector of regressors \\(X\\in \\mathbb R ^p\\), which we will frequently identify with an \\(1\\times p\\) row matrix. Here, “best” is understood in terms of the \\(L_2\\) error:\r\n\\[\r\n\\beta = \\arg \\min _{\\beta '}  \\mathbb E[(Y - X\\beta ^\\prime)^2]=\\mathbb E (X^TX)^{-1}\\mathbb E(X^T Y), \\tag{1}\r\n\\]\r\nwhere the first equation is the defining one, while the second one follows from elementary calculus.\r\nGiven i.i.d. data \\(\\{(X_i,\\,Y_i)\\}_{i=1,\\,2,\\,\\dots,\\,N}\\), and denoting by \\(\\mathbf Y\\) and \\(\\mathbf X\\) the \\(N\\times 1\\) and \\(N\\times p\\) matrices obtained by vertically stacking independent observations of \\(Y\\) and \\(X\\), respectively, the OLS estimate of (1) is defined by:\r\n\\[\r\n\\hat \\beta = \\arg \\min _{\\beta '}  \\sum _{i=1} ^N \\frac{1}{N}(Y_i - X_i\\beta ^\\prime)^2 = (\\mathbf X ^T \\mathbf X) ^{-1} \\mathbf X ^T \\mathbf Y, \\tag{2}\r\n\\]\r\nwhich is readily recognized to be the plugin estimate of \\(\\beta\\). Correspondingly, we define the OLS predictor:\r\n\\[\r\n\\hat Y (x) = x \\hat \\beta. \\tag{3}\r\n\\]\r\nWhat motivates the use of an \\(L_2\\) criterion in (1)?\r\nMathematical tractability. The fact that (1) admits a closed form solution, which is furthermore linear in the response variable \\(Y\\), greatly simplifies the analysis of the properties of \\(\\beta\\) and its estimators such as the OLS one (2), making it a perfect study case.\r\nNumerical tractability. A consequence of the previous point, but worth a separate mention. Computing the plugin estimate in (2) is just a matter of basic linear algebra manipulations, which, with modern software libraries, is a relatively cheap operation.\r\nNormal theory. Focusing on the consequence of Eq. (1), namely the plugin estimate (2), if the conditional distribution of \\(Y\\) given \\(X\\) is normal with constant variance, and if \\(\\mathbb E(Y\\vert X)\\) is truly linear, \\(\\beta\\) coincides with the maximum likelihood estimate of \\(\\beta\\).\r\nThe linear model\r\nThe term generally refers to a model for the conditional distribution of \\(Y\\vert X\\) that requires the conditional mean \\(\\mathbb E(Y\\vert X)\\) to be a linear function of \\(X\\). In its most parsimonious form, this is just:\r\n\\[\r\nY = X \\beta + \\varepsilon, \\quad \\mathbb E(\\varepsilon\\vert X) = 0.\\tag{4}\r\n\\]\r\nThat said, depending on context, (4) is usually supplemented with additional assumptions that further characterise the conditional distribution of the error term1, typically (with increasing strength of assumptions):\r\nConstant Variance. \\(\\mathbb V(\\varepsilon \\vert X) = \\sigma ^2\\), independently of \\(X\\).\r\n\\(X\\)-Independent Errors. \\(\\varepsilon \\perp X\\).\r\nNormal Errors. \\(\\varepsilon \\vert X \\sim \\mathcal N (0,\\sigma ^2)\\).\r\nWhile the OLS estimator is well-defined irrespective of the validity of any of these models, it is clear that, in order for \\(\\hat \\beta\\) to represent a meaningful summary of the \\(Y\\)-\\(X\\) dependence, one should require at least (4) to hold in some approximate sense. Correspondingly, while some general features of \\(\\hat \\beta\\) can be discussed independently of linear model assumptions, its most important properties crucially depend on Eq. (4).\r\nProperties of OLS estimates\r\nDistributional properties of \\(\\hat \\beta\\)\r\nAs an estimator of \\(\\beta\\) as defined in Eq. (1), the OLS estimator \\(\\hat \\beta\\) is consistent (converges in probability to \\(\\beta\\)) but generally biased (an example is provided here). However, due to the plugin nature of \\(\\hat \\beta\\), the bias is generally of order \\(\\mathcal O (N^{-1})\\), which makes it often negligible in comparison to its \\(\\mathcal O(N^{-1/2})\\) sampling variability (see below).\r\nExplicitly, the bias is given by:\r\n\\[\r\n\\mathbb E(\\hat \\beta) - \\beta = \\mathbb E\\lbrace((\\mathbf X ^T \\mathbf X) ^{-1}-\\mathbb E[(\\mathbf X ^T \\mathbf X) ^{-1}])\\cdot \\mathbf X ^T f (\\mathbf X)\\rbrace, \\tag{5}\r\n\\]\r\nwhere \\(f(X) \\equiv \\mathbb E(Y \\vert X)\\) is the true conditional mean function. In general, this vanishes only if \\(f(X)=X\\beta\\), as in the linear expectation model (4).\r\nThe \\(\\mathbf X\\)-conditional variance of \\(\\hat \\beta\\) can be derived directly from Eq. (2):\r\n\\[\r\n\\mathbb V (\\hat \\beta \\vert \\mathbf X)=(\\mathbf X ^T \\mathbf X) ^{-1} \\mathbf X ^T  \\mathbb V (\\mathbf Y\\vert \\mathbf X) \\mathbf X (\\mathbf X ^T \\mathbf X), \\tag{6}\r\n\\]\r\nwhere \\(\\mathbb V (\\mathbf Y\\vert \\mathbf X)\\) is diagonal for i.i.d. observations. For homoskedastic errors we get:\r\n\\[\r\n\\mathbb V (\\hat \\beta \\vert \\mathbf X)=(\\mathbf X ^T \\mathbf X) ^{-1} \\sigma ^2 \\quad (\\mathbb V(Y\\vert X)=\\sigma ^2).\\  \\tag{7}\r\n\\]\r\nUnder the normal linear model, this allows to obtain finite-sample correct confidence sets for \\(\\beta\\). In the general case, confidence sets can be derived from the Central Limit Theorem satisfied by \\(\\hat \\beta\\) (Buja et al. 2019):\r\n\\[\r\n\\sqrt N (\\hat \\beta -\\beta) \\to \\mathcal N (0,V ) \\tag{8}\r\n\\]\r\nwhere the asymptotic variance is given by:\r\n\\[\r\nV = \\mathbb E[X^TX] ^{-1} \\cdot \\mathbb E[X^T(Y-X\\beta)^2X] \\cdot \\mathbb E[X^TX] ^{-1}.\\tag{9}\r\n\\]\r\nThe plugin estimate of (9) leads to the so called Sandwich variance estimator:\r\n\\[\r\nV_\\text{sand} \\equiv  (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T D_\\mathbf {r^2}\\mathbf X (\\mathbf X^T \\mathbf X)^{-1},\\tag{10}\r\n\\]\r\nwhere \\(D_{\\mathbf r^2}\\) is the diagonal matrix whose \\(i\\)-th entry is the squared residual \\(r_i ^2 = (Y_i-X_i\\beta)^2\\).\r\nProof of Central Limit Theorem for \\(\\hat \\beta\\)\r\nConvergence to the normal distribution (8) with variance (9) can be proved using the formalism of influence functions. From Eq. (1), we see that a small variation \\(P\\to P+\\delta P\\) to the joint \\(XY\\) probability measure induces a first order shift:\r\n\\[\r\n\\delta \\beta =  \\intop \\mathbb E(X^TX)^{-1}X^T (Y-\\beta X) \\text d(\\delta P)\\tag{11}\r\n\\]\r\nin the best linear predictor. The influence function of \\(\\beta\\) is defined by the measurable representation of \\(\\delta \\beta\\), namely:\r\n\\[\r\n\\phi _\\beta = \\mathbb E(X^TX)^{-1}X^T (Y-\\beta X). \\tag{12}\r\n\\]\r\nA general result for plugin estimates then tells us that \\(\\sqrt N (\\hat \\beta -\\beta) \\to \\mathcal N (0, \\mathbb E (\\phi _\\beta ^2))\\) in distribution, and using the explicit form of (12) we readily obtain (9).\r\nVariance estimates\r\nThese can be based on:\r\n\\[\r\ns ^2 = \\frac{1}{N}(\\mathbf Y-\\mathbf X \\hat \\beta)^T(\\mathbf Y-\\mathbf X \\hat \\beta)\r\n\\]\r\nwhich has expectation\r\n\\[\r\n\\mathbb E\\left[s^2\\right]=\\frac{1}{N}\\text {Tr}\\,\\mathbb E \\left[ (1-\\mathbf H) \\cdot \\left(\\mathbb E(\\mathbf Y \\vert \\mathbf X)\\mathbb E(\\mathbf Y \\vert \\mathbf X)^T + \\mathbb V(\\mathbf Y\\vert\\mathbf X)\\right) \\right] \\tag{13}\r\n\\]\r\nwhere the hat matrix \\(\\mathbf H\\) is defined as usual:\r\n\\[\r\n\\mathbf H \\equiv \\mathbf X(\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T\\tag{14}\r\n\\]\r\nIf the general linear model (4) holds, so that \\(E(\\mathbf Y \\vert \\mathbf X) = \\mathbf X \\beta\\), we have \\((1-\\mathbf H) \\mathbb E(\\mathbf Y \\vert \\mathbf X) = 0\\). If we furthermore assume homoskedasticity, we obtain:\r\n\\[\r\n\\mathbb E\\left[s^2\\right]=\\frac{N-p}{N}\\sigma^2 \\quad(\\text{Homoskedastic linear model}), \\tag{15}\r\n\\]\r\nwhere \\(p = \\text {Tr}(\\mathbf H)\\) is the number of independent covariates. On the other hand, if homoskedasticity holds, but \\(E(\\mathbf Y \\vert \\mathbf X)\\) is not linear, the left-hand side of the previous equation is an overestimate of \\(\\mathbf V \\vert X\\).\r\nMore on residuals\r\nIn order to study the properties of residuals, it is convenient to define the functional:\r\n\\[\r\n\\beta (Q) \\equiv \\arg \\min _{\\beta '}  \\mathbb E_Q[(Y - X\\beta ^\\prime)^2]=\\mathbb E_Q (X^TX)^{-1}\\mathbb E_Q(X^T Y), \\tag{16}\r\n\\]\r\nwhich depends on the arbitrary measure \\(Q\\). If \\(Q=P\\) is the original joint \\(XY\\) probability measure, we get \\(\\beta(P) \\equiv \\beta\\) as defined by Eq. (1), whereas if \\(Q = \\hat P\\) is the empirical \\(XY\\) distribution, we get \\(\\beta (\\hat P) \\equiv \\hat \\beta\\) of Eq. (2).\r\nFrom the definition it easily follows that:\r\n\\[\r\n\\mathbb E _Q [X^T (Y-X\\beta(Q))]=0. \\tag{17}\r\n\\]\r\nWith \\(Q=P\\), the previous equation yields:\r\n\\[\r\n\\mathbb E(X^T(Y-X\\beta))=0, \\tag{18}\r\n\\]\r\nwhile setting \\(Q=\\hat P\\) we obtain:\r\n\\[\r\n\\mathbf X^T(\\mathbf Y - \\mathbf X \\hat \\beta)=0. \\tag{19}\r\n\\]\r\nThese orthogonality properties, satisfied by the sample and population residuals respectively, are independent of the actual distribution of \\(X\\) and \\(Y\\), and are simple consequences of the definition of \\(\\beta\\). In particular, if \\(X\\) contains an intercept term, for example \\(X^1 = 1\\), Eqs. (18) and (19) imply that population (sample) residuals have vanishing expectation (sample mean).\r\n\r\n\r\n\r\nBuja, Andreas, Richard Berk, Lawrence Brown, Edward George, Emil Pitkin, Mikhail Traskin, Linda Zhao, and Kai Zhang. 2019. “Models as Approximations i: Consequences Illustrated with Linear Regression.” https://arxiv.org/abs/1404.1578.\r\n\r\n\r\nI use the notation \\(A\\perp B\\) to indicate that the random variables \\(A\\) and \\(B\\) are statistically independent.↩︎\r\n",
    "preview": {},
    "last_modified": "2024-02-09T15:07:48+01:00",
    "input_file": "ordinary-least-squares.knit.md"
  }
]
