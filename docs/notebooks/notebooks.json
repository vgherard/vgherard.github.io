[
  {
    "path": "notebooks/maximum-likelihood/",
    "title": "Maximum Likelihood",
    "description": {},
    "author": [
      {
        "name": "Valerio Gherardi",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2024-03-14",
    "categories": [],
    "contents": "\r\nWe start off with a functional description of Maximum Likelihood (ML)\r\nestimation. Let \\(\\mathcal Q \\equiv\\{\\text d Q_{\\theta} = q_\\theta \\,\\text d \\mu\\}_{\\theta \\in \\Theta}\\) be a parametric family\r\nof probability measures dominated by some common measure \\(\\mu\\). Consider the functional1:\r\n\\[\r\n\\theta ^* (P) = \\arg \\min_{\\theta \\in \\Theta} \\intop \\text dP\\,\\ln (\\frac{1}{q_\\theta}) \\tag{1}.\r\n\\]\r\nThis is the parameter of the best (in the cross-entropy sense) approximation of\r\n\\(P\\) within \\(\\mathcal Q\\), which we assume to be unique.\r\nIf \\(P\\) represents the true probability distribution of the data under study, \\(\\theta ^*(P)\\) is the target of ML estimation, in the general case in which \\(P\\)\r\nis not necessarily in \\(\\mathcal Q\\). The ML estimate \\(\\hat \\theta _N\\) of \\(\\theta^*\\) from an i.i.d. sample of \\(N\\) observations is2:\r\n\\[\r\n\\hat \\theta _N \\equiv \\theta ^*(\\hat P _N)=\\arg \\max_{\\theta \\in \\Theta} \\sum_{i=1}^N \\ln ({q_\\theta(Z_i)}), \\tag{2}\r\n\\]\r\nwhere \\(\\hat P _N\\) is the empirical distribution of the sample.\r\nDenoting:\r\n\\[\r\nc_{P}(\\theta) = \\intop \\text dP\\,\\ln (\\frac{1}{q_\\theta}),\\tag{3}\r\n\\]\r\nwe see that \\(\\theta^*\\) is determined by the condition\r\n\\(c_{P}'(\\theta^*)=0\\). From this, we can easily derive\r\nthe first order variation of \\(\\theta ^*\\) under a variation\r\n\\(P \\to P + \\delta P\\):\r\n\\[\r\n\\delta \\theta ^* =\\left(\\intop \\text dP\\,I_{\\theta ^*} \\right)^{-1}\\left(\\intop \\text d(\\delta P)u_{\\theta ^*}\\right)\\tag{4}\r\n\\]\r\nwhere we have defined:\r\n\\[\r\nu_\\theta = \\frac{\\partial }{\\partial \\theta} \\ln q_\\theta,\\quad I_\\theta = -\\frac{\\partial^2 }{\\partial \\theta ^2}  \\ln q_\\theta.\\tag{5}\r\n\\]\r\nFrom (4) we can identify the influence function of\r\nthe \\(\\theta ^*\\) functional:\r\n\\[\r\n\\psi_P(z)=\\left(\\intop \\text dP\\,I_{\\theta ^*} \\right)^{-1}u_{\\theta ^*}(z)\\tag{6}\r\n\\]\r\nThen, from the standard theory of influence functions, we have:\r\n\\[\r\n\\hat \\theta _N \\approx \\theta ^*+J_{\\theta ^*} ^{-1} U_{\\theta^*}\\tag{7}\r\n\\]\r\nwhere we have defined:\r\n\\[\r\nJ_{\\theta ^*}\\equiv \\intop \\text dP\\,I_{\\theta ^*},\\quad U_{\\theta ^* }\\equiv\\frac{1}{N}\\sum _{i=1}^Nu_{\\theta ^*}(Z_i)\\tag{8}.\r\n\\]\r\nIn particular, we obtain the Central Limit Theorem (CLT)\r\n\\[\r\n\\sqrt N(\\hat \\theta _N - \\theta ^*) \\overset{d}{\\to} \\mathcal N(0, J_{\\theta^*}^{-1}K_{\\theta ^*}J_{\\theta ^*}^{-1}),\\tag{9}\r\n\\]\r\nwith:\r\n\\[\r\nK_{\\theta ^*} = \\mathbb V(u_{\\theta ^*}(Z)). \\tag{10}\r\n\\]\r\nThe matrices \\(K_{\\theta ^*}\\) and \\(J_{\\theta ^*}\\) depend on the unknown value \\(\\theta ^*\\), but we can readily construct plugin estimators:\r\n\\[\r\n\\hat J_N = -\\frac{1}{N}\\sum _{i=1}^NI_{\\hat \\theta _N}(z_i),\\quad\\hat K_N = \\frac{1}{N}\\sum _{i=1}^Nu_{\\hat \\theta _N}(z_i)u_{\\hat \\theta _N}(z_i)^T,\\tag{11}\r\n\\]\r\nand estimate the variance of \\(\\hat \\theta _N\\) as:\r\n\\[\r\n\\widehat {\\mathbb V}(\\hat \\theta _N) = \\frac{\\hat J _N ^{-1}\\hat K_N\\hat J_N ^{-1}}{N}\\tag{12},\r\n\\]\r\nwhich is the usual Sandwich estimator. Finally, if \\(P = Q_{\\theta^*}\\), then \\(J _{\\theta^*} = K _{\\theta^*}\\), and the CLT (9) becomes simply\r\n\\(\\sqrt N(\\hat \\theta _N - \\theta ^*) \\overset{d}{\\to} \\mathcal N(0, J_{\\theta^*}^{-1})\\).\r\nLet us now consider the following expansion of \\(c_P(\\hat \\theta _N)\\) which, we recall, is the cross-entropy of the ML model on the true distribution \\(P\\) (cf. (3)):\r\n\\[\r\n\\begin{split}\r\nc_P(\\hat \\theta _N)\r\n    &= -\\intop \\text d P(z')\\,\\ln (q_{\\hat \\theta}(z'))\\\\\r\n    & \\approx -\\mathbb E'(\\ln q_{\\theta^*})+\\frac{1}{2}(\\hat \\theta-\\theta ^*)^TJ_{\\theta ^*} (\\hat \\theta-\\theta ^*)\\\\\r\n    & \\approx -\\mathbb E'(\\ln q_{\\theta^*})+\\frac{1}{2}U_{\\theta ^*}^TJ_{\\theta ^*}^{-1}U_{\\theta ^*}\r\n\\end{split}\r\n\\]\r\nTaking the expectation with respect to the training dataset, noting that\r\n\\(\\mathbb E(U_{\\theta ^*}U_{\\theta ^*}^T)=K_{\\theta ^*}\\), we get:\r\n\\[\r\n\\mathbb E (c_P(\\hat \\theta _N))\\approx -\\mathbb E'(\\ln q_{\\theta^*})+\\frac{1}{2N}\\text {Tr}(J_{\\theta ^*}^{-1}K_{\\theta^*}) \\tag{13}\r\n\\]\r\nNow consider the in-sample estimate:\r\n\\[\r\n\\begin{split}\r\nc_{\\hat P _N}(\\hat \\theta _N) &= -\\frac{1}{N}\\sum _{i=1}^N\\ln q_{\\hat \\theta}(Z_i)\\\\\r\n& \\approx - \\frac{1}{N}\\sum _{i=1} ^N \\ln q_{\\theta^*}(Z_i)- U_{\\theta ^*}^T(\\hat \\theta _N-\\theta^*)+ \\frac{1}{2}(\\hat \\theta _N-\\theta^*)^TJ_{\\theta ^*}(\\hat \\theta _N-\\theta^*)\\\\\r\n& \\approx - \\frac{1}{N}\\sum _{i=1} ^N \\ln q_{\\theta^*}(Z_i)- U_{\\theta ^*}^TJ_{\\theta ^*} ^{-1} U_{\\theta^*}+ \\frac{1}{2}U_{\\theta ^{*}}^TJ_{\\theta ^*} ^{-1}\\hat J_N J_{\\theta ^*} ^{-1}U_{\\theta ^{*}}\\\\\r\n& \\approx - \\frac{1}{N}\\sum _{i=1} ^N \\ln q_{\\theta^*}(Z_i)- \\frac{1}{2}U_{\\theta ^*}^TJ_{\\theta ^*} ^{-1} U_{\\theta^*}.\r\n\\end{split}\r\n\\]\r\nTaking the expectation :\r\n\\[\r\n\\mathbb E (c_{\\hat P _N}(\\hat \\theta _N)) = -\\mathbb E'(\\ln q_{\\theta^*})-\\frac{1}{2N}\\text{Tr}(J_{\\theta ^*}^{-1}K_{\\theta^*})\\tag{14}\r\n\\]\r\nComparing Eqs. (14) and (13) we see\r\nthat:\r\n\\[\r\n\\text{TIC}\\equiv -\\frac{1}{N}\\sum _{i=1}^N\\ln q_{\\hat \\theta}(Z_i)+\\frac{1}{N}\\text{Tr}(J_{\\theta ^*}^{-1}K_{\\theta^*})\\tag{15}\r\n\\]\r\nprovides an asymptotically unbiased estimate of\r\n\\(\\mathbb E (c_P(\\hat \\theta _N))\\), the expected cross-entropy of a model from\r\nfamily \\(\\mathcal Q\\) estimated on a sample of \\(N\\) observations.\r\nIn the previous derivation, we could take each \\(Z_i\\) to be a pair \\((X_i,\\,Y_i)\\)\r\ndrawn from a joint \\(X-Y\\) distribution. If we replace the model family \\(\\mathcal Q\\)\r\nwith a parametric family of conditional densities \\(\\mathcal Q \\equiv\\{\\text d Q_{\\theta}(\\cdot\\vert X) = q_{\\theta}(\\cdot\\vert X) \\,\\text d \\mu\\}_{\\theta \\in \\Theta}\\), and change the objective function to:\r\n\\[\r\nc_P(\\theta) = \\intop \\text dP(x,y)\\,\\ln (\\frac{1}{q_{\\theta}(y\\vert x)}),\\tag{16}\r\n\\]\r\nwe can repeat our above argument without any further change. This\r\nprovides a quick and dirty derivation of the CLT (9) and of the\r\ninformation criterion (15) in a regression setting with random\r\nregressors.\r\nReferences\r\n(Shalizi 2024)\r\n(Claeskens and Hjort 2008)\r\n(Freedman 2006)\r\n\r\n\r\n\r\nClaeskens, Gerda, and Nils Lid Hjort. 2008. “Model Selection and Model Averaging.” Cambridge Books.\r\n\r\n\r\nFreedman, David A. 2006. “On the so-Called ‘Huber Sandwich Estimator’ and ‘Robust Standard Errors’.” The American Statistician 60 (4): 299–302.\r\n\r\n\r\nShalizi, Cosma. 2024. Advanced Data Analysis from an Elementary Point of View. https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/.\r\n\r\n\r\nThe definition\r\ndoes not depend on the representations \\(q_\\theta = \\frac{\\text d Q_\\theta}{\\text d \\mu}\\) chosen for the \\(\\mu\\)-density of \\(Q_\\theta\\) if \\(P\\) is also absolutely continuous with respect to \\(\\mu\\), which we tacitly assume. Typically \\(\\mu\\) would be some relative of Lebesgue or counting measures, in continuous and discrete\r\nsettings respectively.↩︎\r\nAs a random variable, \\(\\hat \\theta _N\\) is also independent (modulo a measure zero set) of the specific representer \\(q_\\theta\\) if \\(P\\) is absolutely continuous with respect to \\(\\mu\\).↩︎\r\n",
    "preview": {},
    "last_modified": "2024-03-14T21:13:23+01:00",
    "input_file": "maximum-likelihood.knit.md"
  },
  {
    "path": "notebooks/exponential-dispersion-models/",
    "title": "Exponential Dispersion Models",
    "description": {},
    "author": [
      {
        "name": "Valerio Gherardi",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2024-03-07",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntro\r\nExponential Dispersion Models\r\nGeneral Properties\r\nMoment generating function\r\nLegendre Transform of \\(\\kappa (\\theta)\\)\r\n\r\nDeviance\r\nMaximum Likelihood Estimation\r\nExamples of EDMs\r\nReferences\r\n\r\nIntro\r\nExponential Dispersion Models (EDMs) provide a natural generalization of the\r\nnormal distribution, in which the modeled variable \\(Y\\) is assumed to follow a\r\nprobability density:\r\n\\[\r\n\\text d P _{\\lambda,\\,\\mu}(y)=e^{-\\frac{\\lambda}{2}d(y,\\,\\mu)}\\text d \\nu _{\\lambda}(y)\r\n\\]\r\nwith respect to a certain dominating measure \\(\\nu _{\\lambda}\\). Here\r\n\\(\\mu = \\intop y\\,\\text d P_{\\lambda,\\,\\mu}(y)\\) and \\(d(y,\\,\\mu) \\geq 0\\), with equality only for \\(y = \\mu\\). The function \\(d(y,\\,\\mu)\\) is called the\r\nunit deviance, and plays for EDMs the same role of squared distance\r\n\\((y-\\mu)^2\\) for the normal model. Not surprisingly, EDMs provide a sound\r\nframework for the maximum-likelihood based formulation of generalized linear models, additive models, and similar beasts.\r\nExponential Dispersion Models\r\nWe start with a probability measure on \\(\\mathbb R ^n\\) in the form of an additive EDM:\r\n\\[\r\n\\text d P ^* _{\\lambda, \\theta} (z) = e^{\\theta ^T z-\\lambda\\kappa(\\theta)}\\text dQ^*_\\lambda (z) \\tag{1}\r\n\\]\r\nwhere \\(\\lambda > 0\\), \\(\\text Q^*_\\lambda\\) is a Borelian probability measure on \\(\\mathbb R\\), and \\(\\kappa(\\theta)\\) is a differentiable strictly convex function, with \\(\\kappa''(\\theta) > 0\\) and \\(\\kappa(0) =0\\). For a random variable \\(Z\\) distributed according to (3) we write \\(Z\\sim \\text{ED}^*(\\lambda, \\,\\theta,\\,\\kappa)\\).\r\nFor any given \\(\\lambda\\), normalization of (1) requires:\r\n\\[\r\ne^{\\lambda \\kappa(\\theta)}=\\intop e^{\\theta ^Ty}\\text dQ^*_\\lambda(z)\\tag{2}\r\n\\]\r\nto hold for all \\(\\theta\\) and \\(\\lambda\\). In other words, \\(M_\\lambda(\\theta) \\equiv e^{\\lambda \\kappa(\\theta)}\\) must be the moment generating function of the measure \\(Q^* _\\lambda(y)\\) for a given \\(\\theta\\), which we assume to be uniquely determined by its moments1, so that we can omit the mention of the measure \\(Q^*_\\lambda\\) in the notation \\(\\text{ED}^*(\\lambda, \\,\\theta,\\,\\kappa)\\)^. This requires, in particular \\(\\kappa (0) = 0\\).\r\nA closely related parametrization is the so-called reproductive EDM:\r\n\\[\r\n\\text d P _{\\lambda, \\theta} (y) = e^{\\lambda(\\theta ^T y-\\kappa(\\theta))}\\text dQ_\\lambda (y)\\tag{3}.\r\n\\]\r\nFor a random variable \\(Y\\) distributed according to (3) we write \\(Y\\sim \\text{ED}(\\lambda, \\,\\theta,\\,\\kappa)\\). The link between (3) and (1) is that \\(Y\\sim \\text{ED}(\\lambda, \\,\\theta,\\,\\kappa)\\) if and\r\nonly if \\(Z=\\lambda Y\\sim \\text{ED}^*(\\lambda, \\,\\theta,\\,\\kappa)\\), so that reproductive and additive EDMs can be interchanged whenever convenient, at\r\nleast for theoretical considerations. The probability measures\r\n\\(\\text d Q_\\lambda\\) and \\(\\text d Q ^*_\\lambda\\), which are uniquely determined by normalization, are related by push-forward:\r\n\\[\r\nQ_\\lambda ^* = (m_\\lambda )_*(Q_\\lambda),\\tag{4}\r\n\\]\r\nwhere \\(m_\\lambda\\) denotes multiplication by \\(\\lambda\\), i.e. \\(m_\\lambda(y)=\\lambda y\\).\r\nIn cases of practical interest (see the examples below), \\(Q_\\lambda\\) and \\(Q_\\lambda^*\\) are absolutely continuous either with respect to the Lebesgue measure, or with respect some measure concentrated on \\(c \\cdot \\mathbb N\\) for some \\(c>0\\). The two cases are referred to as the “continuous” and “discrete” case, respectively, for obvious reasons.\r\nGeneral Properties\r\nMoment generating function\r\nConsider first the reproductive EDM (3). If \\(Y\\sim \\text {ED}(\\lambda,\\,\\theta,\\,\\kappa)\\), its moment generating function is:\r\n\\[\r\nM_Y(s)=\\mathbb E(e^{sY})=\\exp\\left[\\lambda\\left(\\kappa(\\theta +\\frac{s}{\\lambda})-\\kappa(\\theta)\\right)\\right],\\tag{5}\r\n\\]\r\nfrom which we can derive, in particular:\r\n\\[\r\n\\begin{split}\r\n\\mathbb E(Y) &= \\frac{\\text d}{\\text ds}\\vert_{s=0}\\log M(s) =\\kappa'(\\theta),\\\\\r\n\\mathbb V(Y) &= \\frac{\\text d^2}{\\text ds ^2}\\vert_{s=0}\\log M(s) =\\frac{\\kappa''(\\theta)}{\\lambda}.\\\\\r\n\\end{split}\\tag{6}\r\n\\]\r\nFor the additive EDM (1), the corresponding results for \\(Z\\sim \\text{ED}^*(\\lambda,\\,\\theta,\\,\\kappa)\\) are:\r\n\\[\r\n\\begin{split}\r\nM_Z(s)&=\\exp\\left[\\lambda\\left(\\kappa(\\theta + s)-\\kappa(\\theta)\\right)\\right],\\\\\r\n\\mathbb E(Z) &= \\lambda\\kappa'(\\theta),\\\\\r\n\\mathbb V(Z) &= \\lambda \\kappa''(\\theta).\r\n\\end{split}\\tag{7}\r\n\\]\r\nLegendre Transform of \\(\\kappa (\\theta)\\)\r\nSince \\(\\kappa\\) is strictly convex, the mapping:\r\n\\[\r\n\\mu = \\frac{\\partial\\kappa}{\\partial\\theta}\\tag{8}\r\n\\]\r\nis invertible, and we may equivalently parametrize the reproductive EDM in terms\r\nof \\(\\mu\\) and \\(\\lambda\\) as follows:\r\n\\[\r\n\\text d P _{\\lambda, \\mu} (y) = e^{\\lambda(\\theta(\\mu) ^T (y-\\mu)+\\tau(\\mu))}\\text dQ_\\lambda (y)\\tag{9}.\r\n\\]\r\nwhere:\r\n\\[\r\n\\tau(\\mu) = \\theta(\\mu)^T\\mu - \\kappa(\\theta(\\mu)) \\tag{10}.\r\n\\]\r\nis the Legendre transform of \\(\\kappa\\).\r\nDeviance\r\nConsider two reproductive EDMs \\(P _{\\lambda,\\mu _1}\\) and \\(P _{\\lambda,\\mu _2}\\)\r\nwith the same dispersion parameter \\(\\lambda\\) (the function \\(\\kappa\\) is assumed\r\nto be fixed throughout). The likelihood ratio at a given \\(Y=y\\) is:\r\n\\[\r\n\\ln (\\frac{\\text d P_{\\lambda,\\mu_1}}{\\text d P_{\\lambda,\\mu_2}}(y))=\\lambda\\cdot\\left[(\\theta_1-\\theta_2)y-(\\kappa_1-\\kappa_2)\\right],\\tag{11}\r\n\\]\r\nwhere \\(\\theta _1 = \\theta(\\mu_1)\\), \\(\\kappa_1= \\kappa(\\theta(\\mu _1))\\), etc..\r\nSetting \\(\\mu _1 = y\\) and \\(\\mu _2 = \\mu\\) in this expression and multiplying by a convenient factor, we obtain the so called unit scaled deviance:\r\n\\[\r\n\\begin{split}\r\nd_\\lambda(y,\\mu) &\\equiv 2\\left.\\ln (\\frac{\\text d P_{\\lambda,\\mu_0}}{\\text d P_{\\lambda,\\mu}}(y)) \\right \\vert _{\\mu_0=y}\\\\&=2\\lambda\\cdot\\left[(\\theta(y)-\\theta(\\mu))y-\\kappa(\\theta(y))+\\kappa(\\theta(\\mu))\\right].\r\n\\end{split}\\tag{12}\r\n\\]\r\nThe unit deviance is defined as:\r\n\\[\r\n\\begin{split}\r\nd(y,\\mu) &\\equiv d_1(y,\\mu)\\\\&=2\\cdot\\left[(\\theta(y)-\\theta(\\mu))y-\\kappa(\\theta(y))+\\kappa(\\theta(\\mu))\\right]\r\n\\end{split}\\tag{13}\r\n\\]\r\nIt is also useful to express \\(d_\\lambda\\) in terms of the Legendre transform \\(\\tau\\)\r\nof \\(\\kappa\\), as defined in (10):\r\n\\[\r\nd_\\lambda(y,\\mu) =2\\lambda\\cdot\\left[-\\theta(\\mu)^T(y-\\mu)+\\tau(y)-\\tau(\\mu)\\right]\\tag{14}\r\n\\]\r\nUsing the convexity of \\(\\kappa\\), it is easy to show that\r\n\\(d_\\lambda(y,\\mu) \\geq 0\\) for all \\(y\\) and \\(\\mu\\), and that \\(d_\\lambda(y,\\mu) = 0\\) requires \\(\\mu = y\\). The probability measure can be expressed in terms of the\r\nunit deviance as:\r\n\\[\r\n\\text d P _{\\lambda, \\mu} (y) = e^{-\\frac{\\lambda}{2}d(y,\\,\\mu)}e^{\\lambda \\tau(y)}\\text dQ_\\lambda (y)\\tag{15}.\r\n\\]\r\nMaximum Likelihood Estimation\r\nLet \\(Y_i\\sim \\text{ED}(\\lambda,\\,\\mu^{(0)}_ i ,\\,\\kappa)\\) be independent for \\(i=1,\\,2,\\,\\dots,\\,N\\) and let \\(M\\subseteq \\mathbb R ^N\\) be a family of models\r\nfor the mean \\(\\boldsymbol \\mu ^{(0)} = (\\mu _1^{(0)},\\,\\mu_2^{(0)},\\dots,\\,\\mu_N^{(0)})^T\\) (in a GLM context,\r\n\\(M\\) would be the linear subspace spanned by the covariates, \\(\\boldsymbol \\mu _\\beta = \\mathbf X \\beta\\)). From Eq. (15), we see that the likelihood of a model \\(\\boldsymbol \\mu \\in M\\) is, modulo a \\(\\boldsymbol \\mu\\)-independent term, equal to its total deviance:\r\n\\[\r\n\\log \\mathcal L (\\boldsymbol \\mu,\\,\\lambda;\\mathbf Y) = -\\frac{\\lambda}{2}\\mathcal D(\\mathbf Y,\\boldsymbol \\mu)+g_\\lambda(\\mathbf Y),\\tag{16}\r\n\\]\r\nwith:\r\n\\[\r\n\\mathcal D (\\mathbf Y,\\boldsymbol \\mu)\\equiv\\sum_{i=1}^Nd(Y_i,\\mu_i) \\tag{17}\r\n\\]\r\nHence, the Maximum Likelihood Estimate (MLE) of \\(\\boldsymbol \\mu\\) corresponds to the minimum deviance estimate:\r\n\\[\r\n\\hat {\\boldsymbol \\mu}\\equiv \\arg \\max _{\\boldsymbol \\mu \\in M} \\mathcal L (\\boldsymbol \\mu,\\,\\lambda;\\,\\mathbf Y)=\\arg \\min _{\\boldsymbol \\mu \\in M} \\mathcal D (\\mathbf Y;\\boldsymbol \\mu).\\tag{18}\r\n\\]\r\nIn particular, the MLE \\(\\hat {\\boldsymbol \\mu}\\) is obtained by minimizing a function of \\(\\boldsymbol \\mu\\) only, and is independent on whether the dispersion parameter \\(\\lambda\\) is being estimated itself or not.\r\nThese results are sometimes formulated in terms of a “saturated” model\r\n\\(\\boldsymbol \\mu _\\text{s} = \\mathbf Y\\). From Eq. (16)\r\nwe see that such a model has likelihood equal to \\(g_{\\lambda}(\\boldsymbol \\mu)\\),\r\nimplying that:\r\n\\[\r\n\\lambda\\mathcal D(\\mathbf Y,\\boldsymbol \\mu) = -2\\log \\left(\\frac{\\mathcal L (\\boldsymbol \\mu,\\lambda;\\mathbf Y)}{\\mathcal L (\\mathbf Y,\\lambda;\\mathbf Y)}\\right) \\tag{19}.\r\n\\]\r\nWe note the asymptotic results (B. Jørgensen 1992, sec. 3.6):\r\n\\[\r\n\\lambda \\mathcal D(\\mathbf Y,\\,\\hat {\\boldsymbol \\mu})\\overset{d}{\\to}  \\chi ^2 _{N-p} \\qquad (\\lambda \\to \\infty)\\tag{20},\\\\\r\n\\]\r\nfor a correctly specified model family \\(M\\) with \\(\\dim (M) = p\\), and:\r\n\\[\r\n\\lambda \\mathcal D(\\mathbf Y,\\,\\hat {\\boldsymbol \\mu}_1)-\\lambda D(\\mathbf Y,\\,\\hat {\\boldsymbol \\mu}_2)\\overset{d}{\\to} \\chi ^2 _{p_2-p_1} \\qquad (\\lambda \\to \\infty \\text { or } N\\to \\infty)\\tag{21},\r\n\\]\r\nfor a correctly specified model family \\(M_1\\) and \\(M_2 \\supseteq M_1\\), with\r\n\\(p_i =\\dim (M_i)\\). Eq. (20) can be seen as the limiting\r\ncase of (21) when \\(M_2 = \\mathbb R ^N\\), as in the\r\nsaturated model. The manifolds \\(M\\) and \\(M_i\\) are not strictly required to be\r\nlinear subspaces of \\(\\mathbb R^N\\), because in the limits and under the null hypotheses implied by Eqs. (20) and (21) the distributions of MLEs are concentrated\r\naround the true value \\(\\boldsymbol \\mu ^{(0)}\\), so that the manifolds \\(M\\) and\r\n\\(M_i\\) can be effectivley approximated by their tangent spaces.\r\nNoteworthy, limit (20) holds\r\nin the small dispersion limit \\(\\lambda \\to \\infty\\) only, whereas limit (21) is also valid in the large sample limit, essentially due to Wilks’ theorem.\r\nExamples of EDMs\r\nUnivariate Gaussian\r\nThe univariate gaussian family \\(N (\\mu, \\sigma^2)\\), with probability density function (PDF):\r\n\\[\r\nf_{\\mu,\\sigma}(y)=\\frac{1}{\\sqrt {2 \\pi \\sigma ^2}}\\exp\\left[-\\frac{(y-\\mu)^2}{2\\sigma ^2}\\right] \\tag{22}\r\n\\]\r\ncorresponds to the reproductive EDM \\(\\text{ED}(\\lambda, \\,\\theta,\\,\\kappa)\\)\r\nwith:\r\n\\[\r\n\\kappa (\\theta) = \\frac{\\theta ^2}{2},\\quad \\theta \\in \\mathbb R,\\quad \\lambda \\in \\mathbb R^+.\\tag{23}\r\n\\]\r\nThe correspondence is given by:\r\n\\[\r\n\\theta = \\mu,\\quad \\lambda = \\frac{1}{\\sigma^2}.\\tag{24}\r\n\\]\r\nThe base probability measure is given by:\r\n\\[\r\n\\text d Q_\\lambda (y)=\\sqrt{\\frac \\lambda {2\\pi}}e^{-\\lambda y^2/2} \\text dy\\tag{25}\r\n\\]\r\nThe Legendre transform of \\(\\kappa\\) is:\r\n\\[\r\n\\tau(\\mu) = \\frac{\\mu ^2}{2} \\tag{26}\r\n\\]\r\nand the unit deviance reads:\r\n\\[\r\nd(y, \\hat \\mu) =(y-\\hat \\mu)^2.\\tag{27}\r\n\\]\r\nBinomial\r\nThe binomial family \\(\\mathcal B(p,N)\\) with probability mass function (PMF):\r\n\\[\r\nf_{p,N}(z) = \\binom{N}{z} p^z(1-p)^{N-z}\\tag{28}\r\n\\]\r\ncorresponds to the additive EDM \\(\\text{ED}^*(\\lambda, \\,\\theta;\\,\\kappa)\\) with:\r\n\\[\r\n\\kappa(\\theta) = \\ln (\\dfrac{1+e^\\theta}{2}),\\quad \\theta\\in \\mathbb R ,\\quad \\lambda \\in \\mathbb N.\\tag{29}\r\n\\]\r\nThe correspondence is given by:\r\n\\[\r\n\\theta = \\ln\\frac{p}{1-p},\\quad\\lambda =N.\\tag{30}\r\n\\]\r\nThe base probability measure reads:\r\n\\[\r\n\\frac{\\text d Q_\\lambda^*(z)}{\\text d z} =2^{-\\lambda}\\sum_{i=0} ^\\lambda \\binom{\\lambda}{i} \\delta (z-i) \\tag{31}.\r\n\\]\r\nThe Legendre transform of \\(\\kappa\\) (using \\(p\\) for the mean parameter) is:\r\n\\[\r\n\\tau(p)= \\ln2+p\\ln p+(1-p)\\ln(1-p)\\tag{32}\r\n\\]\r\nand the unit deviance for the reproductive EDM:\r\n\\[\r\nd(y,\\hat p)=-2y\\ln \\hat p-2(1-y)\\ln (1-\\hat p).\\tag{33}\r\n\\]\r\nMultinomial\r\nThe multinomial family \\(\\text{Mult} _M(p_1,\\,p_2,\\dots ,p_{M+1},\\,N)\\) for \\(K+1\\) categories is given by the PMF:\r\n\\[\r\nf_{\\boldsymbol p ,N}(z) = \\binom{N}{z_1,\\,z_2,\\,\\dots,\\,z_{K+1}}\\prod _{k=1}^{K+1}p_k^{z_k}\\tag{34},\r\n\\]\r\nIn order to identify this with an EDM, we use the constraints\r\n\\(\\sum _{i=1}^{M+1}z_i =1\\) and \\(\\sum _{i=1}^{M+1}p_i =1\\) to eliminate one\r\ndependent variable and parameter, say \\(z_{M+1}\\) and \\(p_{M+1}\\), respectively.\r\nThe family of densities for the resulting \\(M\\)-dimensional vector \\(\\boldsymbol z=(z_1\\,z_2\\,\\dots\\,z_M)^T\\) corresponds to the additive EDM \\(\\text{ED}^*(\\lambda, \\,\\theta;\\,\\kappa)\\) with:\r\n\\[\r\n\\quad \\kappa(\\theta) = \\ln(\\dfrac{1+\\sum_{i=1}^K e^{\\theta _k}}{K+1}), \\quad \\theta \\in \\mathbb R^M,\\quad \\lambda \\in \\mathbb N\\tag{35},\r\n\\]\r\nthe correspondence being given by:\r\n\\[\r\n\\theta _i = \\ln \\frac{p_i}{p_{K+1}},\\quad \\lambda = N.\\tag{36}\r\n\\]\r\nThe base measure:\r\n\\[\r\n\\frac{\\text d Q_\\lambda^*(z)}{\\text d \\boldsymbol z} =(K+1)^{-\\lambda}\\sum_{\\boldsymbol i\\in \\mathbb N ^{M}\\,\\colon \\,\\sum _{k=1}^{K}i_k\\leq\\lambda} \\binom{\\lambda}{i_1,\\,i_2,\\dots,i_{K+1}} \\delta (\\boldsymbol z-\\boldsymbol i), \\tag{37}\r\n\\]\r\nwhere \\(i_{K+1} = N - \\sum _{k=1} ^{K} i_k\\). The Legendre transform of \\(\\kappa\\)\r\nis:\r\n\\[\r\n\\tau(\\boldsymbol p)= \\ln (K+1)+\\sum _{k=1}^{K+1}p_k\\ln p_k\\tag{38}\r\n\\]\r\nand deviance is:\r\n\\[\r\nd(y,\\hat {\\boldsymbol p}) = -2\\sum _{k=1}^{K+1}y_k\\ln \\hat p_k.\\tag{39}\r\n\\]\r\nPoisson\r\nThe Poisson PMF is:\r\n\\[\r\nf(y) = \\frac {\\nu ^z} {z!} e^{-\\nu}\\tag{40}\r\n\\]\r\nThis can be interpreted as coming from an additive EDM with:\r\n\\[\r\n\\kappa(\\theta)=e^\\theta-1,\\quad \\theta \\in \\mathbb R,\\quad \\lambda \\in \\mathbb R^+\\tag{41}.\r\n\\]\r\nHowever, the correspondence is not unique, being given by the single relation:\r\n\\[\r\n\\lambda e^\\theta = \\nu \\tag{42}\r\n\\]\r\nwhich describes a curve in the \\(\\Theta \\times \\Lambda\\) space. The corresponding base measure is:\r\n\\[\r\n\\dfrac{\\text d Q _\\lambda (z)}{\\text d z} = e^{-\\lambda}\\sum _{k=0}^{\\infty}\\frac{\\lambda ^k\\delta(z-k)}{k!}\\tag{43}\r\n\\]\r\nwhich is nothing but the Poisson measure itself.\r\nReferences\r\nI have mostly followed (Bent Jørgensen 1987). References (B. Jørgensen 1992) (Jorgensen 1997) from the same author provide\r\nmore extensive expositions. A good reference for GLMs is (McCullagh 2019).\r\n\r\n\r\n\r\nJorgensen, Bent. 1997. The Theory of Dispersion Models. CRC Press.\r\n\r\n\r\nJørgensen, B. 1992. The Theory of Exponential Dispersion Models and Analysis of Deviance. Monografias de Matemática. IMPA. https://books.google.es/books?id=twN3vgAACAAJ.\r\n\r\n\r\nJørgensen, Bent. 1987. “Exponential Dispersion Models.” Journal of the Royal Statistical Society: Series B (Methodological) 49 (2): 127–45.\r\n\r\n\r\nMcCullagh, Peter. 2019. Generalized Linear Models. Routledge.\r\n\r\n\r\nWhether a set of moments determines a unique probability measure is called the\r\nHamburger moment problem.↩︎\r\n",
    "preview": {},
    "last_modified": "2024-03-13T17:49:39+01:00",
    "input_file": {}
  },
  {
    "path": "notebooks/bootstrap/",
    "title": "Bootstrap",
    "description": {},
    "author": [
      {
        "name": "Valerio Gherardi",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2024-02-07",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nThe plugin principle\r\nThe role of simulation\r\n[TODO] Standard bootstrap estimates\r\nVariance of an estimator\r\nConfidence intervals\r\n\\(p\\)-values\r\nPrediction error\r\n\r\n[TODO] Bootstrap for time series data\r\n[TODO] Bootstrap for text data\r\n\r\nIntroduction\r\nThe Bootstrap (B. Efron 1979; Bradley Efron and Tibshirani 1994) is a set of computational techniques for statistical inference that generally operate by approximating the distribution of a population of interest with an empirical estimate obtained from a finite sample. Such methods find practical use in all those situations when the true distribution is either unknown, due to limited knowledge of the data generating process, or impossible to compute in practice.\r\nIn general, bootstrap algorithms consist of two ingredients:\r\nA plugin principle, i.e. a substitution rule \\(P\\to\\hat P\\) that replaces the true data distribution \\(P\\) with an empirical estimate \\(\\hat P\\) obtained from a finite sample.\r\nA calculation scheme for computing functionals of the plugin distribution \\(\\hat P\\), usually involving simulation.\r\nThe plugin principle\r\nThe main theoretical idea behind the bootstrap can be sketched with a non-parametric example. Consider a functional \\(t=t(P)\\) of a probability measure \\(P\\) which admits a first order expansion :\r\n\\[\r\nt(Q) \\approx t(P) + L(P; Q - P),\\tag{1}\r\n\\]\r\nwhere \\(L(P;\\nu)\\) is assumed to be a linear functional of \\(\\nu\\). If \\(Q\\) has finite support, linearity implies that:\r\n\\[\r\nL(P,Q-P) = \\intop \\psi _P\\,\\text dQ\\tag{2},\r\n\\]\r\nand we shall further assume that such a representation is valid for any \\(Q\\)1. Notice in particular, that from this definition we have:\r\n\\[\r\n\\mathbb E(\\psi _P) = L(P,P-P) = 0\\tag{3}\r\n\\]\r\nSuppose now that we have a sample of \\(N\\) i.i.d. observations \\(\\{X_1,\\,X_2,\\,\\dots,\\,X_N\\}\\) coming from \\(P\\), and let \\(Q=\\hat P _N\\) in the previous expression, where \\(\\hat P _N = \\frac{1}{N}\\sum _{i=1}^N\\delta _{X_i}\\) stands for the empirical distribution. Then:\r\n\\[\r\nt(\\hat P _N) \\approx t(P) + L(P; \\hat P _N - P) = t(P) + \\frac{1}{N}\\sum _{i = 1} ^N \\psi(X_i).\\tag{4}\r\n\\]\r\nIt follows that \\(t(\\hat P _N)\\), i.e. the so-called “plugin” estimate, is a consistent estimate of \\(t(P)\\), with:\r\n\\[\r\n\\mathbb E(t(\\hat P _N)) \\approx t(P),\\quad \\mathbb V(t(\\hat P_N)) \\approx \\frac{1}{N}\\mathbb V(\\psi(X)),\\tag{5}\r\n\\]\r\nwhere the first equation follows from (3), while the second one follows from the i.i.d. nature of the sample.\r\nThe main idea behind the non-parametric bootstrap is to estimate \\(t(P)\\) with \\(t(\\hat P _N)\\), which is justified by Eq. (5) whenever the \\(\\mathcal O (N^{-1})\\) variance can be considered negligible (as is often the case in concrete bootstrap applications). In a parametric setting, the role of \\(\\hat P_N\\) could be played by some parametric estimate of \\(P\\). Similarly, sampling schemes other than i.i.d. (e.g. if dealing with time series data) require different empirical estimates of \\(P\\), but the basic principle - estimating \\(t(P)\\) with \\(t(\\hat P_N)\\) - remains the same.\r\nEstimates such as \\(t(\\hat P_N)\\) are usually referred to as ideal bootstrap estimates. As implied by the name, they can rarely be computed exactly in practice, because no analytic formula exists, and exact numerical calculations rapidly become prohibitive with growing \\(N\\). This leads to \\(t(\\hat P_N)\\) being estimated through simulation from \\(\\hat P _N\\), as explained below.\r\nA technical refinement\r\nIn many practical applications, the functional of interest \\(t(P)\\) would itself depend on \\(N\\), so that we should actually write \\(t_N(P)\\). A relevant example would be the variance of a plugin estimate \\(v_N(P)\\equiv\\mathbb V (t(\\hat P _N))\\). In this and similar cases, in which \\(v_N=\\mathcal O (N^{-\\alpha})\\) the argument can be repeated mutatis mutandis for the functional \\(V_N = N^\\alpha \\cdot v_N\\), which has a finite limit \\(V_N \\to V\\), assumed to be different from zero. Specifically, if we let \\(V_N = V+\\Delta _N\\) we have:\r\n\\[\r\nV_N(\\hat P_N) -V_N(P) = V(\\hat P_N)-V(P)+\\Delta _N (\\hat P_N)-\\Delta_N(P).\r\n\\]\r\nSince both terms in the right hand side have vanishing (to first order) expectation and \\(\\mathcal O (N^{-1})\\) variance, this shows that we can use \\(V_N(\\hat P _N)\\) to approximate the target quantity \\(V_N(P)\\), or equivalently \\(v_N(\\hat P _N)\\) to approximate \\(v_N(P)\\), since \\(\\frac{\\mathbb E(v_N(\\hat P_N))}{v_N(P)}\\approx 1\\) and \\(\\frac{\\sqrt {\\mathbb V(v_N(\\hat P_N))}}{v_N(P)}=\\mathcal O (N^{-1/2})\\).\r\nThe role of simulation\r\nThe second, more case specific, ingredient of a practical bootstrap estimate is an approximation scheme for effectively computing \\(t(\\hat P _N)\\). These methods typically involve simulating from \\(\\hat P_N\\), the reason being that the functional \\(t(Q)\\) usually involves expectations and/or quantiles of random variables of samples from \\(Q\\). When \\(Q = \\hat P_N\\), simulation allows to obtain \\(t(\\hat P_N)\\) by brute force.\r\nThis is best clarified with an example. Given a functional \\(\\theta(P)\\), we let:\r\n\\[\r\nv_N (P) = \\mathbb V_P(\\theta (\\hat P_N))\\tag{6}\r\n\\]\r\ndenote the variance (with respect to the original measure \\(P\\)) of its plugin estimate from a dataset of \\(N\\) i.i.d. observations. The ideal bootstrap estimate is given by:\r\n\\[\r\nv_N(\\hat P_N) = \\mathbb V_{\\hat P _N}(\\theta(\\hat P_N^*)),\\tag{7}\r\n\\]\r\nwhere \\(\\hat P _N ^*\\) denotes the empirical distribution of an i.i.d. sample of \\(N\\) elements from \\(\\hat P_N\\) - obviously, i.i.d. sampling from \\(\\hat P _N\\) is the same as sampling with replacement from the original dataset. Suppose now we generate \\(B\\) synthetic datasets of size \\(N\\) by sampling with replacement, and let \\(\\hat P_N ^{(b)*}\\) denote the corresponding empirical distributions. We can then estimate (7) by:\r\n\\[\r\n\\widetilde {v_N(\\hat P_N)} = \\dfrac{1}{B-1}\\sum_{b=1}^{B}(\\theta(\\hat P_N ^{(b)*})- \\overline \\theta^*)^2,\\tag{8}\r\n\\]\r\nwhere \\(\\overline \\theta^* = \\frac{1}{B}\\sum_{b=1}^{B}\\theta(\\hat P_N ^{(b)*})\\). This would be our practical (as opposed to ideal) bootstrap estimate of \\(v_N(P)\\).\r\nStrictly speaking, the practical bootstrap estimate involves again an application of the plugin principle mentioned in the previous section, in which however the role of the true distribution \\(P\\) is played by the empirical estimate \\(\\hat P_N\\), from which we can sample without any limits. This means that (re)sampling variability associated with (8) can be always made arbitrarily small, at least in principle, simply by increasing \\(B\\).\r\n[TODO] Standard bootstrap estimates\r\nVariance of an estimator\r\nConfidence intervals\r\n\\(p\\)-values\r\nPrediction error\r\n[TODO] Bootstrap for time series data\r\n[TODO] Bootstrap for text data\r\n\r\n\r\n\r\nEfron, B. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26. https://doi.org/10.1214/aos/1176344552.\r\n\r\n\r\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to the Bootstrap. CRC press.\r\n\r\n\r\nHuber, Peter J. 2004. Robust Statistics. Vol. 523. John Wiley & Sons.\r\n\r\n\r\nThe integral representation (2) with bounded \\(\\psi _P\\) automatically follows if \\(L\\) is (weak-star) continuous and Frechét differentiable (Huber 2004). In the most general case, the sense in which Eq. (1) is assumed to hold is that of a directional (Gateaux) derivative, and we assume Eq. (2) to hold for some measurable (not necessarily bounded) function \\(\\psi _P\\), which is usually easy to verify in concrete cases.↩︎\r\n",
    "preview": {},
    "last_modified": "2024-02-15T15:03:45+01:00",
    "input_file": {}
  },
  {
    "path": "notebooks/ordinary-least-squares/",
    "title": "Ordinary Least Squares",
    "description": {},
    "author": [
      {
        "name": "Valerio Gherardi",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2024-02-07",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nGeneralities\r\nThe linear model\r\nProperties of OLS estimates\r\nDistributional properties of \\(\\hat \\beta\\)\r\nVariance estimates\r\nMore on residuals\r\n\r\nRelated posts\r\n\r\nGeneralities\r\nOrdinary Least Squares (OLS) is a regression algorithm for estimating the best linear predictor \\(\\hat Y = X\\beta\\) of a response \\(Y \\in \\mathbb R\\) in terms of a vector of regressors \\(X\\in \\mathbb R ^p\\), which we will frequently identify with an \\(1\\times p\\) row matrix. Here, “best” is understood in terms of the \\(L_2\\) error:\r\n\\[\r\n\\beta = \\arg \\min _{\\beta '}  \\mathbb E[(Y - X\\beta ^\\prime)^2]=\\mathbb E (X^TX)^{-1}\\mathbb E(X^T Y), \\tag{1}\r\n\\]\r\nwhere the first equation is the defining one, while the second one follows from elementary calculus.\r\nGiven i.i.d. data \\(\\{(X_i,\\,Y_i)\\}_{i=1,\\,2,\\,\\dots,\\,N}\\), and denoting by \\(\\mathbf Y\\) and \\(\\mathbf X\\) the \\(N\\times 1\\) and \\(N\\times p\\) matrices obtained by vertically stacking independent observations of \\(Y\\) and \\(X\\), respectively, the OLS estimate of (1) is defined by:\r\n\\[\r\n\\hat \\beta = \\arg \\min _{\\beta '}  \\sum _{i=1} ^N \\frac{1}{N}(Y_i - X_i\\beta ^\\prime)^2 = (\\mathbf X ^T \\mathbf X) ^{-1} \\mathbf X ^T \\mathbf Y, \\tag{2}\r\n\\]\r\nwhich is readily recognized to be the plugin estimate of \\(\\beta\\). Correspondingly, we define the OLS predictor:\r\n\\[\r\n\\hat Y (x) = x \\hat \\beta. \\tag{3}\r\n\\]\r\nWhat motivates the use of an \\(L_2\\) criterion in (1)?\r\nMathematical tractability. The fact that (1) admits a closed form solution, which is furthermore linear in the response variable \\(Y\\), greatly simplifies the analysis of the properties of \\(\\beta\\) and its estimators such as the OLS one (2), making it a perfect study case.\r\nNumerical tractability. A consequence of the previous point, but worth a separate mention. Computing the plugin estimate in (2) is just a matter of basic linear algebra manipulations, which, with modern software libraries, is a relatively cheap operation.\r\nNormal theory. Focusing on the consequence of Eq. (1), namely the plugin estimate (2), if the conditional distribution of \\(Y\\) given \\(X\\) is normal with constant variance, and if \\(\\mathbb E(Y\\vert X)\\) is truly linear, \\(\\beta\\) coincides with the maximum likelihood estimate of \\(\\beta\\).\r\nThe linear model\r\nThe term generally refers to a model for the conditional distribution of \\(Y\\vert X\\) that requires the conditional mean \\(\\mathbb E(Y\\vert X)\\) to be a linear function of \\(X\\). In its most parsimonious form, this is just:\r\n\\[\r\nY = X \\beta + \\varepsilon, \\quad \\mathbb E(\\varepsilon\\vert X) = 0.\\tag{4}\r\n\\]\r\nThat said, depending on context, (4) is usually supplemented with additional assumptions that further characterise the conditional distribution of the error term1, typically (with increasing strength of assumptions):\r\nConstant Variance. \\(\\mathbb V(\\varepsilon \\vert X) = \\sigma ^2\\), independently of \\(X\\).\r\n\\(X\\)-Independent Errors. \\(\\varepsilon \\perp X\\).\r\nNormal Errors. \\(\\varepsilon \\vert X \\sim \\mathcal N (0,\\sigma ^2)\\).\r\nWhile the OLS estimator is well-defined irrespective of the validity of any of these models, it is clear that, in order for \\(\\hat \\beta\\) to represent a meaningful summary of the \\(Y\\)-\\(X\\) dependence, one should require at least (4) to hold in some approximate sense. Correspondingly, while some general features of \\(\\hat \\beta\\) can be discussed independently of linear model assumptions, its most important properties crucially depend on Eq. (4).\r\nProperties of OLS estimates\r\nDistributional properties of \\(\\hat \\beta\\)\r\nAs an estimator of \\(\\beta\\) as defined in Eq. (1), the OLS estimator \\(\\hat \\beta\\) is consistent (converges in probability to \\(\\beta\\)) but generally biased (an example is provided here). However, due to the plugin nature of \\(\\hat \\beta\\), the bias is generally of order \\(\\mathcal O (N^{-1})\\), which makes it often negligible in comparison to its \\(\\mathcal O(N^{-1/2})\\) sampling variability (see below).\r\nExplicitly, the bias is given by:\r\n\\[\r\n\\mathbb E(\\hat \\beta) - \\beta = \\mathbb E\\lbrace((\\mathbf X ^T \\mathbf X) ^{-1}-\\mathbb E[(\\mathbf X ^T \\mathbf X) ^{-1}])\\cdot \\mathbf X ^T f (\\mathbf X)\\rbrace, \\tag{5}\r\n\\]\r\nwhere \\(f(X) \\equiv \\mathbb E(Y \\vert X)\\) is the true conditional mean function. In general, this vanishes only if \\(f(X)=X\\beta\\), as in the linear expectation model (4).\r\nThe \\(\\mathbf X\\)-conditional variance of \\(\\hat \\beta\\) can be derived directly from Eq. (2):\r\n\\[\r\n\\mathbb V (\\hat \\beta \\vert \\mathbf X)=(\\mathbf X ^T \\mathbf X) ^{-1} \\mathbf X ^T  \\mathbb V (\\mathbf Y\\vert \\mathbf X) \\mathbf X (\\mathbf X ^T \\mathbf X), \\tag{6}\r\n\\]\r\nwhere \\(\\mathbb V (\\mathbf Y\\vert \\mathbf X)\\) is diagonal for i.i.d. observations. For homoskedastic errors we get:\r\n\\[\r\n\\mathbb V (\\hat \\beta \\vert \\mathbf X)=(\\mathbf X ^T \\mathbf X) ^{-1} \\sigma ^2 \\quad (\\mathbb V(Y\\vert X)=\\sigma ^2).\\  \\tag{7}\r\n\\]\r\nUnder the normal linear model, this allows to obtain finite-sample correct confidence sets for \\(\\beta\\). In the general case, confidence sets can be derived from the Central Limit Theorem satisfied by \\(\\hat \\beta\\) (Buja et al. 2019):\r\n\\[\r\n\\sqrt N (\\hat \\beta -\\beta) \\to \\mathcal N (0,V ) \\tag{8}\r\n\\]\r\nwhere the asymptotic variance is given by:\r\n\\[\r\nV = \\mathbb E[X^TX] ^{-1} \\cdot \\mathbb E[X^T(Y-X\\beta)^2X] \\cdot \\mathbb E[X^TX] ^{-1}.\\tag{9}\r\n\\]\r\nThe plugin estimate of (9) leads to the so called Sandwich variance estimator:\r\n\\[\r\nV_\\text{sand} \\equiv  (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T D_\\mathbf {r^2}\\mathbf X (\\mathbf X^T \\mathbf X)^{-1},\\tag{10}\r\n\\]\r\nwhere \\(D_{\\mathbf r^2}\\) is the diagonal matrix whose \\(i\\)-th entry is the squared residual \\(r_i ^2 = (Y_i-X_i\\beta)^2\\).\r\nProof of Central Limit Theorem for \\(\\hat \\beta\\)\r\nConvergence to the normal distribution (8) with variance (9) can be proved using the formalism of influence functions. From Eq. (1), we see that a small variation \\(P\\to P+\\delta P\\) to the joint \\(XY\\) probability measure induces a first order shift:\r\n\\[\r\n\\delta \\beta =  \\intop \\mathbb E(X^TX)^{-1}X^T (Y-\\beta X) \\text d(\\delta P)\\tag{11}\r\n\\]\r\nin the best linear predictor. The influence function of \\(\\beta\\) is defined by the measurable representation of \\(\\delta \\beta\\), namely:\r\n\\[\r\n\\phi _\\beta = \\mathbb E(X^TX)^{-1}X^T (Y-\\beta X). \\tag{12}\r\n\\]\r\nA general result for plugin estimates then tells us that \\(\\sqrt N (\\hat \\beta -\\beta) \\to \\mathcal N (0, \\mathbb E (\\phi _\\beta ^2))\\) in distribution, and using the explicit form of (12) we readily obtain (9).\r\nVariance estimates\r\nThese can be based on:\r\n\\[\r\ns ^2 = \\frac{1}{N}(\\mathbf Y-\\mathbf X \\hat \\beta)^T(\\mathbf Y-\\mathbf X \\hat \\beta)\r\n\\]\r\nwhich has expectation\r\n\\[\r\n\\mathbb E\\left[s^2\\right]=\\frac{1}{N}\\text {Tr}\\,\\mathbb E \\left[ (1-\\mathbf H) \\cdot \\left(\\mathbb E(\\mathbf Y \\vert \\mathbf X)\\mathbb E(\\mathbf Y \\vert \\mathbf X)^T + \\mathbb V(\\mathbf Y\\vert\\mathbf X)\\right) \\right] \\tag{13}\r\n\\]\r\nwhere the hat matrix \\(\\mathbf H\\) is defined as usual:\r\n\\[\r\n\\mathbf H \\equiv \\mathbf X(\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T\\tag{14}\r\n\\]\r\nIf the general linear model (4) holds, so that \\(E(\\mathbf Y \\vert \\mathbf X) = \\mathbf X \\beta\\), we have \\((1-\\mathbf H) \\mathbb E(\\mathbf Y \\vert \\mathbf X) = 0\\). If we furthermore assume homoskedasticity, we obtain:\r\n\\[\r\n\\mathbb E\\left[s^2\\right]=\\frac{N-p}{N}\\sigma^2 \\quad(\\text{Homoskedastic linear model}), \\tag{15}\r\n\\]\r\nwhere \\(p = \\text {Tr}(\\mathbf H)\\) is the number of independent covariates. On the other hand, if homoskedasticity holds, but \\(E(\\mathbf Y \\vert \\mathbf X)\\) is not linear, the left-hand side of the previous equation is an overestimate of \\(\\mathbf V \\vert X\\).\r\nMore on residuals\r\nIn order to study the properties of residuals, it is convenient to define the functional:\r\n\\[\r\n\\beta (Q) \\equiv \\arg \\min _{\\beta '}  \\mathbb E_Q[(Y - X\\beta ^\\prime)^2]=\\mathbb E_Q (X^TX)^{-1}\\mathbb E_Q(X^T Y), \\tag{16}\r\n\\]\r\nwhich depends on the arbitrary measure \\(Q\\). If \\(Q=P\\) is the original joint \\(XY\\) probability measure, we get \\(\\beta(P) \\equiv \\beta\\) as defined by Eq. (1), whereas if \\(Q = \\hat P\\) is the empirical \\(XY\\) distribution, we get \\(\\beta (\\hat P) \\equiv \\hat \\beta\\) of Eq. (2).\r\nFrom the definition it easily follows that:\r\n\\[\r\n\\mathbb E _Q [X^T (Y-X\\beta(Q))]=0. \\tag{17}\r\n\\]\r\nWith \\(Q=P\\), the previous equation yields:\r\n\\[\r\n\\mathbb E(X^T(Y-X\\beta))=0, \\tag{18}\r\n\\]\r\nwhile setting \\(Q=\\hat P\\) we obtain:\r\n\\[\r\n\\mathbf X^T(\\mathbf Y - \\mathbf X \\hat \\beta)=0. \\tag{19}\r\n\\]\r\nThese orthogonality properties, satisfied by the sample and population residuals respectively, are independent of the actual distribution of \\(X\\) and \\(Y\\), and are simple consequences of the definition of \\(\\beta\\). In particular, if \\(X\\) contains an intercept term, for example \\(X^1 = 1\\), Eqs. (18) and (19) imply that population (sample) residuals have vanishing expectation (sample mean).\r\nRelated posts\r\nConsistency and bias of OLS estimators\r\nModel misspecification and linear sandwiches\r\nLinear regression with autocorrelated noise\r\nTesting functional specification in linear regression\r\n\r\n\r\n\r\nBuja, Andreas, Richard Berk, Lawrence Brown, Edward George, Emil Pitkin, Mikhail Traskin, Linda Zhao, and Kai Zhang. 2019. “Models as Approximations i: Consequences Illustrated with Linear Regression.” https://arxiv.org/abs/1404.1578.\r\n\r\n\r\nI use the notation \\(A\\perp B\\) to indicate that the random variables \\(A\\) and \\(B\\) are statistically independent.↩︎\r\n",
    "preview": {},
    "last_modified": "2024-02-11T08:42:19+01:00",
    "input_file": {}
  }
]
