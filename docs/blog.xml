<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>Valerio Gherardi</title>
    <link>https://vgherard.github.io/</link>
    <atom:link href="https://vgherard.github.io/blog.xml" rel="self" type="application/rss+xml"/>
    <description>Valerio Gherardi
</description>
    <generator>Distill</generator>
    <lastBuildDate>Thu, 11 May 2023 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Consistency and bias of OLS estimators</title>
      <dc:creator>vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2023-05-12-consistency-and-bias-of-ols-estimators</link>
      <description>


&lt;p&gt;Given random variables &lt;span class="math inline"&gt;\(Y\colon \Omega \to
\mathbb R\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(X\colon \Omega \to
\mathbb R ^{p}\)&lt;/span&gt; defined on an event space &lt;span
class="math inline"&gt;\(\Omega\)&lt;/span&gt;, denote:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\beta = \arg \min _{\beta ^\prime } \mathbb E[(Y-X \beta^\prime )^2]=
\mathbb E(X^TX)^{-1}\mathbb E(X^TY), (\#eq:blp)
\]&lt;/span&gt; so that &lt;span class="math inline"&gt;\(X \beta\)&lt;/span&gt; is the
best linear predictor of &lt;span class="math inline"&gt;\(Y\)&lt;/span&gt; in terms
of &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; (&lt;span
class="math inline"&gt;\(X\)&lt;/span&gt; is regarded as a row vector).&lt;/p&gt;
&lt;p&gt;Let &lt;span class="math inline"&gt;\((\textbf Y, \textbf X)\)&lt;/span&gt; be
independent samples from the joint &lt;span
class="math inline"&gt;\(XY\)&lt;/span&gt; distribution, with independent
observations stacked vertically in &lt;span class="math inline"&gt;\(N \times
1\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(N \times p\)&lt;/span&gt; matrices
respectively, as customary. Then the usual Ordinary Least Squares (OLS)
estimator of &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\hat \beta = \arg \min _{\beta ^\prime}(\textbf Y - \textbf X \beta
^\prime)^2=(\textbf X^T\textbf X)^{-1} \textbf X^T \textbf Y. (\#eq:ols)
\]&lt;/span&gt; This is a consistent, but generally biased estimator of &lt;span
class="math inline"&gt;\(\beta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Comparing Eqs. @ref(eq:blp) and @ref(eq:ols), consistency follows
immediately from the law of large numbers and continuity. In order to
show that &lt;span class="math inline"&gt;\(\mathbb E (\hat \beta) \neq
\beta\)&lt;/span&gt; in general, it is sufficient to provide an example.&lt;/p&gt;
&lt;p&gt;Consider, for instance (example adapted from &lt;a
href="https://www.stat.berkeley.edu/~freedman/badols.pdf"&gt;D.A.
Freedman&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
X \sim \mathcal N (0, 1),\qquad Y=X(1+aX^2)
\]&lt;/span&gt; Recalling that &lt;span class="math inline"&gt;\(\mathbb E (X^4) =
3\)&lt;/span&gt; for the standard normal, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\beta = 1+3a,
\]&lt;/span&gt; where we have ignored a potential intercept term (which would
vanish here, since &lt;span class="math inline"&gt;\(\mathbb E (Y) =
0\)&lt;/span&gt;). To compute &lt;span class="math inline"&gt;\(\mathbb E (\hat
\beta)\)&lt;/span&gt;, we use the identity &lt;span
class="math inline"&gt;\(\frac{e^{-z}}{z} = \intop _1 ^\infty \text d t\, e
^{-zt}\)&lt;/span&gt; to rewrite this expected value as:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
\mathbb E (\hat \beta) &amp;amp; =  (2 \pi)^{-N/2}
    \intop \text d\textbf X \,e^{-\sum _j X_i ^2 /2}
                                    \dfrac{\sum _i X_i^2(1+aX_i^2)}{\sum
_i X_i^2} = \frac{N}{2}\intop_1 ^\infty \text d t\,I(t) \\
I(t)                                     &amp;amp; \equiv (2 \pi)^{-N/2}
\intop \text d\textbf X\,
                                                        e^{-t \sum _j
X_j ^2 /2}X_1^2(1+aX_1^2)
\end{split}
\]&lt;/span&gt; The inner integral can be computed easily:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
I(t) = t^{-\frac{N}{2}}(\frac{1}{t}+a\frac{3}{t^2})
\]&lt;/span&gt; and we eventually find: &lt;span class="math display"&gt;\[
\mathbb E (\hat \beta) = 1+3 a\frac{N}{N+2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The bias is thus given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\beta - \mathbb E (\hat \beta) = \frac{6a}{N+2}
\]&lt;/span&gt; This vanishes linearly, in agreement with the fact that &lt;span
class="math inline"&gt;\(\sqrt N (\hat \beta - \beta )\)&lt;/span&gt; converges
in probability to a gaussian with zero mean and finite variance (which
requires the bias to be &lt;span
class="math inline"&gt;\(o(N^{-1/2})\)&lt;/span&gt;).&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>7cfd1178ff5aa1d949ed57c9fa30e17f</distill:md5>
      <category>Statistics</category>
      <guid>https://vgherard.github.io/posts/2023-05-12-consistency-and-bias-of-ols-estimators</guid>
      <pubDate>Thu, 11 May 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Bayes, Neyman and the Magic Piggy Bank</title>
      <dc:creator>vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2023-05-01-magic-piggy-bank</link>
      <description>


&lt;h1 id="intro"&gt;Intro&lt;/h1&gt;
&lt;p&gt;Frequentist and Bayesian approaches to statistical inference are
motivated by different interpretations of the concept of probability.
These philosophical differences can, at times, shadow the comparably
important &lt;em&gt;operational&lt;/em&gt; differences between the two frameworks,
whose methods proceed, at the end of the day, from the same mathematical
theory.&lt;/p&gt;
&lt;p&gt;From the purely operational point of view, the question &lt;em&gt;“Bayesian
or Frequentist?”&lt;/em&gt; can (and should) be answered by objective
criteria, rather than subjective opinions. As one could expect, the
answer is in general neither “Frequentist” nor “Bayesian”, but rather
&lt;em&gt;“It depends”&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To illustrate this, I will discuss an hypothetical game that revolves
around reporting measurements and correctly quantifying uncertainty. As
we shall see, the winning strategies can be either Frequentist or
Bayesian in spirit, depending on a variation of the actual rules of the
game.&lt;/p&gt;
&lt;h1 id="reporting-measurements"&gt;Reporting measurements&lt;/h1&gt;
&lt;p&gt;All scientific measurements come with an associated uncertainty,
which can be expressed in the form of an interval that is supposed to
contain the object of measurement. In the Frequentist and Bayesian
frameworks, these intervals are traditionally dubbed &lt;em&gt;Confidence&lt;/em&gt;
and &lt;em&gt;Credible&lt;/em&gt; intervals, respectively. While, superficially,
these can be both characterized as &lt;em&gt;“covering the true value with
probability &lt;span class="math inline"&gt;\(p\)&lt;/span&gt;”&lt;/em&gt;, the word
&lt;em&gt;“probability”&lt;/em&gt; has quite different connotations in the two
cases, and confusing them can lead to irrational thought or, as in the
imaginary game described below, financial ruin.&lt;/p&gt;
&lt;h1 id="magic-piggy-bank"&gt;Magic Piggy Bank&lt;/h1&gt;
&lt;p&gt;There are two players, called the Bookmaker and the Gambler, that
compete against each other in a gambling game&lt;a href="#fn1"
class="footnote-ref" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. The interactions
between these two players are mediated by the &lt;em&gt;Magic Piggy Bank&lt;/em&gt;,
a magic creature that acts as a sort of referee. The Magic Piggy Bank
contains infinite biased coins, and knows the probability &lt;span
class="math inline"&gt;\(\Theta\)&lt;/span&gt; of giving “tails” for each one of
them.&lt;/p&gt;
&lt;p&gt;A single iteration of the game proceeds as follows:&lt;/p&gt;
&lt;ol style="list-style-type: decimal"&gt;
&lt;li&gt;&lt;p&gt;The Magic Piggy Bank ejects &lt;a href="#fn2" class="footnote-ref"
id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; a biased coin and gives it to the
Bookmaker.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Bookmaker can flip the coin an arbitrary number of times, to
produce an estimate of &lt;span class="math inline"&gt;\(\Theta\)&lt;/span&gt;, in
the form of an interval &lt;span class="math inline"&gt;\(I\)&lt;/span&gt;. This
must be accompanied by a &lt;em&gt;payout&lt;/em&gt;, that is a number &lt;span
class="math inline"&gt;\(p\in \left(0,1\right)\)&lt;/span&gt;, for bets on the
event &lt;span class="math inline"&gt;\(\Theta \in I\)&lt;/span&gt;. The resulting
&lt;span class="math inline"&gt;\(I\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(p\)&lt;/span&gt;, together with the original data &lt;span
class="math inline"&gt;\(X=(n_\text{tosses}, n_\text{tails})\)&lt;/span&gt; from
the Bookmaker’s experiments, are reported to the Magic Piggy
Bank.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Magic Piggy Bank communicates the payout &lt;span
class="math inline"&gt;\(p\)&lt;/span&gt; to the Gambler, and reveals &lt;em&gt;some
additional information&lt;/em&gt;. What particular information is revealed
depends on the variant of the game being played (see descriptions
below).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Based on the information received, the Gambler can choose to bet
either&lt;br /&gt;
in favor or against &lt;span class="math inline"&gt;\(\Theta \in I\)&lt;/span&gt;.
When betting in favor, the Gambler pays &lt;span
class="math inline"&gt;\(p\)&lt;/span&gt; to the Bookmaker, who returns back
&lt;span class="math inline"&gt;\(1\)&lt;/span&gt; if &lt;span
class="math inline"&gt;\(\Theta \in I\)&lt;/span&gt; obtains. When betting
against, the Bookmaker pays &lt;span class="math inline"&gt;\(p\)&lt;/span&gt; to
the Gambler, who returns back &lt;span class="math inline"&gt;\(1\)&lt;/span&gt; if
&lt;span class="math inline"&gt;\(\Theta \in I\)&lt;/span&gt; obtains.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Magic Piggy Bank reveals all data (&lt;span
class="math inline"&gt;\(X\)&lt;/span&gt;, &lt;span
class="math inline"&gt;\(I\)&lt;/span&gt;, &lt;span
class="math inline"&gt;\(\Theta\)&lt;/span&gt;) to both players and the scores
are settled.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As to the third step, we will consider three variants of the
game:&lt;/p&gt;
&lt;ol style="list-style-type: upper-alpha"&gt;
&lt;li&gt;The Magic Piggy Bank tells the Gambler the results of the
Bookmaker’s tosses &lt;span class="math inline"&gt;\(X=(n_\text{tosses},
n_\text{tails})\)&lt;/span&gt;, as well as the actual interval &lt;span
class="math inline"&gt;\(I\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The Magic Piggy Bank tells the Gambler the true value of &lt;span
class="math inline"&gt;\(\Theta\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The Gambler is given no additional information beyond the
established payout &lt;span class="math inline"&gt;\(p\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="problem"&gt;Problem&lt;/h3&gt;
&lt;p&gt;Suppose that the Bookmaker and Gambler are forced to play
indefinitely. What are the best strategies for these two players,
according to the three different variants A, B, and C described above&lt;a
href="#fn3" class="footnote-ref" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;?&lt;/p&gt;
&lt;h1 id="analysis"&gt;Analysis&lt;/h1&gt;
&lt;p&gt;One can readily verify that the Gambler’s gain (or, equivalently, the
Bookmaker’s loss) in a single iteration of the game is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
G=b\cdot (\chi_I (\Theta)-p) (\#eq:Gain)
\]&lt;/span&gt; where, &lt;span class="math inline"&gt;\(b\)&lt;/span&gt; is equal to
&lt;span class="math inline"&gt;\(\pm 1\)&lt;/span&gt; if the Gambler bets in favor
or against, respectively, and:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\chi _I (\Theta) = \begin{cases}
1 &amp;amp; \Theta \in I \\
0 &amp;amp; \Theta \notin I
\end{cases} (\#eq:CharacteristicFunction)
\]&lt;/span&gt; The expected gain is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E (G) = \intop \text{d}P(\Theta,X) \,b\cdot (\chi_I (\Theta)-p),
(\#eq:ExpectedGain)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(\text{d} P(\Theta,
X)\)&lt;/span&gt; denotes the joint probability measure of &lt;span
class="math inline"&gt;\(\Theta\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let’s now examine in detail the three different variants (A, B, C) of
the game described above.&lt;/p&gt;
&lt;h3 id="variant-a"&gt;Variant A&lt;/h3&gt;
&lt;p&gt;In the first variant of the game, the Gambler is given the same
information as the Bookmaker. In particular, the choice to bet in favor
or against, represented by the sign &lt;span
class="math inline"&gt;\(b\)&lt;/span&gt;, cannot depend on &lt;span
class="math inline"&gt;\(\Theta\)&lt;/span&gt; (which the Gambler doesn’t know),
and we can rewrite the expected gain @ref(eq:ExpectedGain) as&lt;a
href="#fn4" class="footnote-ref" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
\mathbb E (G) &amp;amp;= \intop \text{d}P(X) \,b\cdot \intop
\text{d}P(\Theta \vert X) \,(\chi_I (\Theta)-p) \\
&amp;amp; = \intop \text{d}P(X) \,b \cdot \left(\text {Pr}(\Theta \in I
\vert X)-p\right)
\end{split} (\#eq:ExpectedGainA)
\]&lt;/span&gt; where we have used the fact that, for any random variable
&lt;span class="math inline"&gt;\(Y\)&lt;/span&gt; and set &lt;span
class="math inline"&gt;\(E\)&lt;/span&gt;, the following relation holds:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E (\chi _E (Y)) = \text{Pr}(Y \in E).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now, since both &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(I\)&lt;/span&gt; are known to the Gambler, the latter is
(at least in principle) able to compute:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
b_A \equiv \text{sgn}\left(\text {Pr}(\Theta \in I \vert X)-p\right)
(\#eq:OptimalBetA)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In practice, in order to compute @ref(eq:OptimalBetA), the Gambler
would need to know the overall distribution &lt;span
class="math inline"&gt;\(\pi (\Theta)\)&lt;/span&gt; of the coins &lt;span
class="math inline"&gt;\(\Theta\)&lt;/span&gt; extracted from the Magic Piggy
Bank, but this is something that can be accurately estimated in the long
run, since the actual values of &lt;span
class="math inline"&gt;\(\Theta\)&lt;/span&gt; are revealed at the end of each
iteration &lt;a href="#fn5" class="footnote-ref"
id="fnref5"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Plugging Eq. @ref(eq:OptimalBetA) into Eq. @ref(eq:ExpectedGainA), we
find:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E (G) = \intop \text{d}P(X) \left|\text {Pr}(\Theta \in I \vert
X)-p\right|\quad\text{(Variant A)}   (\#eq:OptimalExpectedGainA).
\]&lt;/span&gt; Comparing with @ref(eq:ExpectedGainA), it is clear that
@ref(eq:OptimalExpectedGainA) is the maximum expected gain, for
&lt;em&gt;any&lt;/em&gt; choice of &lt;span class="math inline"&gt;\(b\)&lt;/span&gt;. In other
words, the choice &lt;span class="math inline"&gt;\(b_A\)&lt;/span&gt; in Eq.
@ref(eq:OptimalBetA) is an optimal one.&lt;/p&gt;
&lt;p&gt;Finally, from the Bookmaker’s point of view, Eq.
@ref(eq:OptimalExpectedGainA) represents a sure loss in the long run,
that can only be avoided by enforcing: &lt;span class="math display"&gt;\[
\text {Pr}(\Theta \in I \vert X)=p \quad \text{(Variant A)}
(\#eq:OptimalBookA)
\]&lt;/span&gt; In order to ensure this, the Bookmaker needs to know as well
the overall coins’ distribution &lt;span class="math inline"&gt;\(\pi
(\Theta)\)&lt;/span&gt;, and the same remarks made above for the Gambler apply
here.&lt;/p&gt;
&lt;p&gt;Equation @ref(eq:OptimalBookA) defines what is known as a
&lt;strong&gt;&lt;em&gt;Bayesian credible interval&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id="variant-b"&gt;Variant B&lt;/h3&gt;
&lt;p&gt;We now consider the second variant of the rules, where the Gambler is
told the true value of &lt;span class="math inline"&gt;\(\Theta\)&lt;/span&gt;, but
does not know the details of the Bookmaker’s measurement, except for the
established payout &lt;span class="math inline"&gt;\(p\)&lt;/span&gt;. Using a
reasoning similar to the previous section we rewrite:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
\mathbb E (G) &amp;amp;= \intop \text{d}P(\Theta) \,b\cdot \intop
\text{d}P(X \vert\Theta) \,(\chi_I (\Theta)-p) \\
&amp;amp; = \intop \text{d}P(\Theta) \,b \cdot \left(\text {Pr}(\Theta \in I
\vert \Theta)-p\right)
\end{split} (\#eq:ExpectedGainB)
\]&lt;/span&gt; and define&lt;a href="#fn6" class="footnote-ref"
id="fnref6"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;: &lt;span class="math display"&gt;\[
b_B \equiv \text{sgn}\left(\text {Pr}(\Theta \in I \vert
\Theta)-p\right)\quad(\text{Variant B}) (\#eq:OptimalBetB)
\]&lt;/span&gt; which is easily shown to be the optimal betting strategy for
the Gambler in the present setting. In the long run, this sign can be
accurately estimated by modeling the conditional mean of &lt;span
class="math inline"&gt;\(\chi _I (\Theta) - p\)&lt;/span&gt; (as a function of
&lt;span class="math inline"&gt;\(\Theta\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(p\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;If the Gambler bets according to @ref(eq:OptimalBetB), the Bookmaker
is forced to set payouts according to:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text {Pr}(\Theta \in I \vert \Theta)=p\quad(\text{Variant B}),
(\#eq:OptimalBookB)
\]&lt;/span&gt; in order to avoid a certain loss.&lt;/p&gt;
&lt;p&gt;Equation @ref(eq:OptimalBookB) defines what is known as a
&lt;strong&gt;&lt;em&gt;Frequentist confidence interval&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id="variant-c"&gt;Variant C&lt;/h3&gt;
&lt;p&gt;In the last case, the Gambler has no extra information beyond the
payout &lt;span class="math inline"&gt;\(p\)&lt;/span&gt;, and the expected gain
reduces to:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\mathbb E (G)=b\cdot \left(\text{Pr}(\Theta \in
I)-p\right),(\#eq:ExpectedGainC)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(\text{Pr}(\Theta \in
I)\)&lt;/span&gt; is the unconditional probability that &lt;span
class="math inline"&gt;\(I\)&lt;/span&gt; covers &lt;span
class="math inline"&gt;\(\Theta\)&lt;/span&gt;. The optimal betting choice is:
&lt;span class="math display"&gt;\[
b_C \equiv \text{sgn}\left(\text {Pr}(\Theta \in
I)-p\right)\quad(\text{Variant C}) (\#eq:OptimalBetC)
\]&lt;/span&gt; which forces the Bookmaker to set payouts according to: &lt;span
class="math display"&gt;\[
\text {Pr}(\Theta \in I)=p\quad(\text{Variant C}). (\#eq:OptimalBookC)
\]&lt;/span&gt; This is, by the way, satisfied by both the Bayesian and
Frequentist intervals, due to Eqs. @ref(eq:OptimalBookA) and
@ref(eq:OptimalBookB), respectively.&lt;/p&gt;
&lt;h3 id="summary-of-results"&gt;Summary of results&lt;/h3&gt;
&lt;p&gt;Provided access to the same data used by the Bookmaker to produce the
interval &lt;span class="math inline"&gt;\(I\)&lt;/span&gt; (Variant A), a rational
Gambler would bet in favor of &lt;span class="math inline"&gt;\(\Theta \in
I\)&lt;/span&gt; if the probability of this event &lt;em&gt;conditional to the
observed the data&lt;/em&gt; is greater than the payout &lt;span
class="math inline"&gt;\(p\)&lt;/span&gt; (Eq. @ref(eq:OptimalBetA)).&lt;/p&gt;
&lt;p&gt;On the other hand, given true value of &lt;span
class="math inline"&gt;\(\Theta\)&lt;/span&gt; (Variant B), the optimal choice
for a Gambler is to bet on &lt;span class="math inline"&gt;\(\Theta \in
I\)&lt;/span&gt; if the probability of this event &lt;em&gt;conditional to the
ground truth&lt;/em&gt; is greater than &lt;span class="math inline"&gt;\(p\)&lt;/span&gt;
(Eq. @ref(eq:OptimalBetB)).&lt;/p&gt;
&lt;p&gt;Finally, in the lack of any of this information (Variant C), the most
rational choice is simply to bet on &lt;span class="math inline"&gt;\(\Theta
\in I\)&lt;/span&gt; if this event occurs more frequently than &lt;span
class="math inline"&gt;\(p\)&lt;/span&gt; (Eq. @ref(eq:OptimalBetC)).&lt;/p&gt;
&lt;p&gt;When playing against the first two types of players, in order to
avoid a certain loss, the Bookmaker must produce Bayesian credible
intervals (Variant A) or Frequentist confidence intervals (Variant B).
In the remaining case (Variant C), the Bookmaker can either produce
Bayesian or Frequentist intervals&lt;a href="#fn7" class="footnote-ref"
id="fnref7"&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id="conclusions"&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;When I first learned about Bayesian and Frequentist inference, I
remember most discussions were focused on the philosophical differences
between these two schools of thought. There was little to no mention
about the actual mathematical properties of the constructs prescribed by
the two formalisms, which made the choice between “Bayesian” or
“Frequentist” look like a mere matter of committing to one particular
view.&lt;/p&gt;
&lt;p&gt;Technically, what I did here was to compare the frequentist
properties of credible intervals and confidence intervals. I’m sure the
literature, including the pedagogical one, is full of examples like
this, and better ones&lt;a href="#fn8" class="footnote-ref"
id="fnref8"&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;. With no pretense of originality, I believe
that including more examples of this kind in the usual presentations can
be beneficial to students and practitioners, and perhaps help them out
of the ugly black-box of orthodoxy.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes footnotes-end-of-document"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;The introduction of bets as an expedient to
operationally define subjective probabilities is historically due to the
Italian mathematician &lt;a
href="https://en.wikipedia.org/wiki/Bruno_de_Finetti"&gt;Bruno de
Finetti&lt;/a&gt;. The statistical analysis of the game proposed below can be
given a Frequentist interpretation.&lt;a href="#fnref1"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;Readers are free to imagine this process in the way they
find more convenient.&lt;a href="#fnref2"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;We assume that both players know from the outset which
variant of the game they are playing to.&lt;a href="#fnref3"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt; We denote (with some abuse of notation) by &lt;span
class="math inline"&gt;\(\text{d}P(\Theta \vert X)\)&lt;/span&gt; the conditional
probability measure of &lt;span class="math inline"&gt;\(\Theta\)&lt;/span&gt;
conditioned on &lt;span class="math inline"&gt;\(X\)&lt;/span&gt;.&lt;a href="#fnref4"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5"&gt;&lt;p&gt;In the Bayesian spirit of @ref(eq:OptimalBetA), the
Gambler could for instance estimate &lt;span
class="math inline"&gt;\(\pi(\Theta)\)&lt;/span&gt; through Bayesian updates of a
&lt;a href="https://en.wikipedia.org/wiki/Dirichlet_process"&gt;Dirichlet
prior&lt;/a&gt;.&lt;a href="#fnref5" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6"&gt;&lt;p&gt;Noteworthy, the random quantity in this equation is
&lt;span class="math inline"&gt;\(I\)&lt;/span&gt;, whereas &lt;span
class="math inline"&gt;\(\Theta\)&lt;/span&gt; is regarded as fixed. This is in
stark contrast with Eq. @ref(eq:OptimalBetA), where &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(I\)&lt;/span&gt; were fixed, and &lt;span
class="math inline"&gt;\(\Theta\)&lt;/span&gt; was random.&lt;a href="#fnref6"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn7"&gt;&lt;p&gt; There are, in fact, infinitely many more ways to
produce intervals with the unconditional coverage property Eq.
@ref(eq:OptimalBookC).&lt;a href="#fnref7"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn8"&gt;&lt;p&gt; I see that Jaynes (the father of the Maximum Entropy
foundation of Statistical Mechanics, among other things) has a full
essay paper on &lt;a
href="https://bayes.wustl.edu/etj/articles/confidence.pdf"&gt;Confidence
Intervals vs. Bayesian Intervals&lt;/a&gt;, which I haven’t read - the
abstract sounds a bit loaded to me, but it’s probably definitely worth
to read.&lt;a href="#fnref8" class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5>95a1367b9833d3b8fb3e3e183a81c6f3</distill:md5>
      <category>Statistics</category>
      <guid>https://vgherard.github.io/posts/2023-05-01-magic-piggy-bank</guid>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Correlation Without Causation</title>
      <dc:creator>vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2023-03-10-correlation-without-causation</link>
      <description>


&lt;p&gt;It is part of common knowledge that &lt;strong&gt;correlation does not
require causation&lt;/strong&gt;. Absence of causation, say between a
condition &lt;span class="math inline"&gt;\(p\)&lt;/span&gt; and an effect &lt;span
class="math inline"&gt;\(q\)&lt;/span&gt;, means that the realization of &lt;span
class="math inline"&gt;\(p\)&lt;/span&gt; has no influence on the presence of
&lt;span class="math inline"&gt;\(q\)&lt;/span&gt;. If this is the case, a
statistical correlation between &lt;span class="math inline"&gt;\(p\)&lt;/span&gt;
and &lt;span class="math inline"&gt;\(q\)&lt;/span&gt; can still be present, if the
realization of &lt;span class="math inline"&gt;\(p\)&lt;/span&gt; modifies our
&lt;em&gt;state of information&lt;/em&gt; about &lt;span
class="math inline"&gt;\(q\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As an example, let &lt;span class="math inline"&gt;\(X,Y\)&lt;/span&gt; be two
conditionally independent binary random variables, with a common
probability &lt;span class="math inline"&gt;\(\Theta\)&lt;/span&gt; of evaluating to
one. Think, for instance, of a machine that produces pairs of identical
biased coins, with a probability of tails &lt;span
class="math inline"&gt;\(\Theta\)&lt;/span&gt;. If &lt;span
class="math inline"&gt;\(\Theta\)&lt;/span&gt; is equal to a given value &lt;span
class="math inline"&gt;\(\theta\)&lt;/span&gt;, the joint probability
distribution of &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(Y\)&lt;/span&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text {Pr}(X=x,Y=y\vert \Theta = \theta) = B(x;\theta)B(y;\theta),
(\#eq:XYConditionalProb)
\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(B(z; \theta) = \theta ^z (1
- \theta) ^ {1-z}\)&lt;/span&gt;. Whether or not this provides a satisfying
probabilistic description of experiments on &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(Y\)&lt;/span&gt; depends on context.&lt;/p&gt;
&lt;p&gt;From a frequentist point of view, if &lt;span
class="math inline"&gt;\(\Theta\)&lt;/span&gt; is fixed once and for all, the
right hand side of Eq. @ref(eq:XYConditionalProb) correctly describes
the experimental outcomes of &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; and
&lt;span class="math inline"&gt;\(Y\)&lt;/span&gt; for &lt;em&gt;some&lt;/em&gt; value of &lt;span
class="math inline"&gt;\(\theta\)&lt;/span&gt;. On the other hand, if &lt;span
class="math inline"&gt;\(\Theta\)&lt;/span&gt; can change from experiment to
experiment in a random fashion, and we do not observe its values &lt;span
class="math inline"&gt;\(\theta\)&lt;/span&gt;, we clearly cannot use Eq.
@ref(eq:XYConditionalProb) as it stands, as its usage requires knowing
&lt;span class="math inline"&gt;\(\theta\)&lt;/span&gt;. Finally, from a bayesian’s
point of view, if &lt;span class="math inline"&gt;\(\Theta\)&lt;/span&gt; is fixed
but unknown, Eq. @ref(eq:XYConditionalProb) does not describe our
&lt;em&gt;state of knowledge&lt;/em&gt; about &lt;span class="math inline"&gt;\(X\)&lt;/span&gt;
and &lt;span class="math inline"&gt;\(Y\)&lt;/span&gt;, because it assumes
unavailable information (&lt;span class="math inline"&gt;\(\Theta =
\theta\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;In the last two cases, what we’re actually after is the unconditional
probability:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\text{Pr}(X=x,\,Y=y)=\intop\,\text{d}P_\Theta(\theta)
\,\text{Pr}(X=x,Y=y\vert\Theta = \theta)
(\#eq:XYUnconditionalProb)
\]&lt;/span&gt; where &lt;span
class="math inline"&gt;\(\text{d}P_\Theta(\theta)\)&lt;/span&gt; can be regarded
either as the actual probability distribution of &lt;span
class="math inline"&gt;\(\Theta\)&lt;/span&gt; (in a frequentist framework) or as
a subjective prior distribution (in a bayesian framework).&lt;/p&gt;
&lt;p&gt;Plugging Eq. @ref(eq:XYConditionalProb) into
@ref(eq:XYUnconditionalProb), we find:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\begin{split}
\text{Pr}(X=1,\,Y=1) &amp;amp; = \mathbb E(\Theta)^2+\text{Var}(\Theta)\\
\text{Pr}(X=1,\, Y=0)&amp;amp;=\mathbb E(\Theta)-\mathbb
E(\Theta)^2-\text{Var}(\Theta)\\
\text{Pr}(X=0,\, Y=1)&amp;amp;=\mathbb E(\Theta)-\mathbb
E(\Theta)^2-\text{Var}(\Theta)\\
\text{Pr}(X=0,\,Y=0) &amp;amp; = \mathbb (1-\mathbb
E(\Theta))^2+\text{Var}(\Theta) \\
\end{split}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In particular, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[
\dfrac{\text{Pr}(Y = 1 \vert\, X = 1)}{\text {Pr}(Y=1)} =
1+\frac{\text{Var}(\Theta)}{\mathbb{E}(\Theta)^2},
(\#eq:PMI)
\]&lt;/span&gt; which means that, &lt;em&gt;unconditionally&lt;/em&gt;, &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(Y\)&lt;/span&gt; are not independent, but in fact
positively correlated&lt;a href="#fn1" class="footnote-ref"
id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Observations of this kind apply, &lt;em&gt;mutatis mutandis&lt;/em&gt;, in many
practical situations. For instance if we were modeling the time series
of new visitors to a website, we could reasonably assume that the number
of yesterday’s new visitors does not influence the number of today’s
ones (if individual visitors are unlikely to interact with each other).
Yet, it would be wrong to assume, and easy to disprove, that these two
numbers are by themselves statistically independent, because yesterday’s
new visitors carry useful background information on today’s potential
new visitors.&lt;/p&gt;
&lt;p&gt;The bottom line of the post is that &lt;strong&gt;lack of causation does
not imply lack of correlation&lt;/strong&gt;, which is logically equivalent to
the original motto… but, for some strange reason, I find easier to
forget.&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="footnotes footnotes-end-of-document"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Here I’m using the word correlation in a loose sense, as
in the popular motto.&lt;a href="#fnref1"
class="footnote-back"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      <distill:md5>fe520dca72f62407e195f134051b016c</distill:md5>
      <category>Statistics</category>
      <guid>https://vgherard.github.io/posts/2023-03-10-correlation-without-causation</guid>
      <pubDate>Thu, 30 Mar 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>How to get away with selection. Part II: Mathematical Framework</title>
      <dc:creator>vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2022-11-07-posi-2</link>
      <description>Mathematicals details on Selective Inference, model misspecification and coverage guarantees.</description>
      <category>Statistics</category>
      <guid>https://vgherard.github.io/posts/2022-11-07-posi-2</guid>
      <pubDate>Fri, 25 Nov 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>How to get away with selection. Part I: Introduction</title>
      <dc:creator>vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2022-10-18-posi</link>
      <description>Introducing the problem of Selective Inference, illustrated through a simple simulation in R.</description>
      <category>R</category>
      <category>Statistics</category>
      <guid>https://vgherard.github.io/posts/2022-10-18-posi</guid>
      <pubDate>Mon, 14 Nov 2022 00:00:00 +0000</pubDate>
      <media:content url="https://vgherard.github.io/posts/2022-10-18-posi/posi_files/figure-html5/unnamed-chunk-3-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>kgrams v0.1.2 on CRAN</title>
      <dc:creator>vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2021-11-13-kgrams-v012-released</link>
      <description>kgrams: Classical k-gram Language Models in R.</description>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2021-11-13-kgrams-v012-released</guid>
      <pubDate>Sat, 13 Nov 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>R Client for R-universe APIs</title>
      <dc:creator>vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2021-07-25-r-client-for-r-universe-apis</link>
      <description>Introducing W.I.P. {runiv}, an R package to interact with R-universe 
repository APIs</description>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2021-07-25-r-client-for-r-universe-apis</guid>
      <pubDate>Sun, 25 Jul 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Automatic resumes of your R-developer portfolio from your R-Universe</title>
      <dc:creator>vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2021-07-21-automatically-resume-your-r-package-portfolio-using-the-r-universe-api</link>
      <description>Create automatic resumes of your R packages using the R-Universe API.</description>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2021-07-21-automatically-resume-your-r-package-portfolio-using-the-r-universe-api</guid>
      <pubDate>Wed, 21 Jul 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>{r2r} now on CRAN</title>
      <dc:creator>vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2021-07-06-r2r</link>
      <description>Introducing {r2r}, an R implementation of hash tables.</description>
      <category>R</category>
      <guid>https://vgherard.github.io/posts/2021-07-06-r2r</guid>
      <pubDate>Tue, 06 Jul 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Test post</title>
      <dc:creator>vgherard</dc:creator>
      <link>https://vgherard.github.io/posts/2021-07-06-test-post</link>
      <description>A short description of the post.</description>
      <category>Other</category>
      <guid>https://vgherard.github.io/posts/2021-07-06-test-post</guid>
      <pubDate>Tue, 06 Jul 2021 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
