[
  {
    "path": "posts/2023-05-14-model-misspecification-and-linear-sandwiches/",
    "title": "Model Misspecification and Linear Sandwiches",
    "description": "Being wrong in the right way. With R excerpts.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-05-14",
    "categories": [
      "Statistics",
      "Regression",
      "Linear Models",
      "Model Misspecification",
      "R"
    ],
    "contents": "\nIntroduction\nTraditional linear models, such as the output of the R function lm(), are\noften loaded with a set of strong assumptions. Take univariate regression:\n\\[\nY = q+mX+\\varepsilon. \\tag{1}\n\\]\nThis equation assumes that:\nThe conditional mean \\(\\mathbb E(Y\\vert X) = q + mX\\), a linear function\nof \\(X\\).\nThe conditional variance \\(\\mathbb {V}(Y \\vert X)=\\mathbb{V}(\\varepsilon\\vert X)\\)\nis independent of \\(X\\).\nThe conditional distribution \\(Y\\vert X\\) is gaussian.\nIn a set of measurements \\(\\left\\{\\left(X_i,Y_i\\right)\\right\\}_{i = 1,\\, \\dots, \\,N}\\), \\(Y_i\\) and the set \\(\\left\\{ X_j, Y_j\\right\\} _{j\\neq i}\\) are conditionally independent of each other, given the value of the corresponding regressor \\(X_i\\).1\nThe last assumption is satisfied in many practical situations, and we will take it here for granted2. What happens when the first\nthree assumptions are violated (that is “frequently” to “almost always”,\ndepending on context)?\nA comprehensive discussion is provided by (Buja et al. 2019). These authors show that:\nIf the conditional mean \\(\\mathbb E (Y \\vert X)\\) is not linear (“first order misspecification”), then the Ordinary Least Squares (OLS) regression\ncoefficients \\(\\hat \\beta\\) consistently estimate:\n\\[\n\\beta \\equiv \\text{arg } \\min _{\\beta^\\prime} \\mathbb E((Y-X\\beta^\\prime)^2) \\tag{2}\n\\]\nwhich can be thought as the “best linear approximation of the response”3.\nBoth non-linearity in the sense of the previous point, and \\(X\\)-dependence in\n\\(\\mathbb{V}(Y \\vert X)\\) (“second order misspecification”) affect the sampling\ndistribution of \\(\\hat \\beta\\) and, in particular, \\(\\mathbb{V}(\\hat \\beta)\\),\nwhich is the relevant quantity for inference in the large-sample limit.\nBoth problems can be efficiently addressed through the so-called “sandwich” estimators for the covariance matrix of \\(\\hat \\beta\\) (White 1980), whose consistency is robust to both type of misspecification.\nDetails can be found in the mentioned reference. The rest of the post\nillustrates with examples how to compute “sandwich” estimates in R, and why\nyou may want to do so.\nFitting misspecified linear models in R\nThe {sandwich} package\n(available on CRAN) provides estimators for the regression coefficients’\nvariance-covariance matrix \\(\\mathbb V (\\hat \\beta)\\) that are robust to first\nand second order misspecification. These can be readily used with lm objects,\nas in the example below:\n\n\nfit <- lm(mpg ~ wt, data = mtcars)\n\nstats::vcov(fit)  # standard vcov (linear model trusting estimate)\n\n            (Intercept)        wt\n(Intercept)    3.525484 -1.005693\nwt            -1.005693  0.312594\n\nsandwich::vcovHC(fit)  # sandwich vcov (model-robust estimate)\n\n            (Intercept)         wt\n(Intercept)    5.889249 -1.7418581\nwt            -1.741858  0.5448011\n\nIt is important to note that both functions stats::vcov() and\nsandwich::vcovHC() employ the same point estimates of regression coefficients\nto compute \\(\\mathbb V (\\hat \\beta)\\):\n\n\nfit\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nThe difference between these functions lies in the different assumptions they\nmake on the linear model residuals, which leads to different estimates\nfor \\(\\mathbb{V}(\\hat \\beta)\\).\nEffects of misspecification\nThis section illustrates some consequences of model misspecification through\nsimulation. The examples use:\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nFor convenience, we define some helpers to be used in the following\nexamples. The function below returns random generators for the generic additive\nerror model \\(Y = f(X) + \\varepsilon\\), where the distribution of the noise term\n\\(\\varepsilon\\) may in general depend on \\(X\\). Both \\(X\\) and \\(Y\\) are assumed here\nand below to be 1-dimensional.\n\n\nrxy_fun <- function(rx, f, reps) {\n  res <- function(n) {\n    x <- rx(n)  # X has marginal distribution 'rx'\n    y <- f(x) + reps(x)  # Y has conditional mean 'f(x)' and noise 'reps(x)'\n    return(tibble(x = x, y = y))  \n  }\n  return(structure(res, class = \"rxy\"))\n}\n\nplot.rxy <- function(x, N = 1000, seed = 840) {\n  set.seed(seed)\n  \n  ggplot(data = x(N), aes(x = x, y = y)) +\n    geom_point(alpha = 0.3) + \n    geom_smooth(method = \"lm\", se = FALSE)\n}\n\n\nThe following function simulates fitting the linear model y ~ x over multiple\ndatasets generated according to a function rxy().\n\n\nlmsim <- function(rxy, N = 100, vcov = stats::vcov, B = 1e3, seed = 840) \n{ \n  set.seed(seed)\n  \n  res <- list(coef = matrix(nrow = B, ncol = 2), vcov = vector(\"list\", B))\n  colnames(res$coef) <- c(\"(Intercept)\", \"x\")\n  class(res) <- \"lmsim\"\n                \n  for (b in 1:B) {\n    .fit <- lm(y ~ ., data = rxy(N))\n    res$coef[b, ] <- coef(.fit)  # Store intercept and slope in B x 2 matrix\n    res$vcov[[b]] <- vcov(.fit)  # Store vcov estimates in length B list. \n  }\n  \n  return(res)\n}\n\nprint.lmsim <- function(x) \n{\n  cat(\"Simulation results:\\n\\n\")\n  cat(\"* Model-trusting vcov (average of vcov estimates):\\n\")\n  print( avg_est_vcov <- Reduce(\"+\", x$vcov) / length(x$vcov) )\n  cat(\"\\n* Simulation-based vcov (vcov of coefficient estimates):\\n\")\n  print( emp_vcov <- cov(x$coef))\n  cat(\"\\n* Ratio (1st / 2nd):\\n\")\n  print( avg_est_vcov / emp_vcov )\n  return(invisible(x))\n}\n\n\nThe print method defined above shows a comparison of the covariance matrices\nobtained by:\nAveraging variance-covariance estimates from the various simulations, and\nTaking the variance-covariance matrix of regression coefficients obtained\nin the simulations.\nThe first one can be considered a “model-trusting” estimate (where the actual\n“model” is specified by the vcov argument of lmsim(), i.e. stats::vcov and\nsandwich::vcovHC for the traditional and sandwich estimates, respectively).\nThe second one is a model-free simulation-based estimate of the true\n\\(\\mathbb{V}(\\hat \\beta)\\). The comparison between the two4\nprovides a measure of the asymptotic bias of the model-trusting estimate.\nExample 1: First order misspecification\n\\[\nY = X ^ 2 + \\varepsilon,\\quad X \\sim \\text{Unif} (0,1),\\qquad \\varepsilon \\sim \\mathcal N (0,0.01)\n\\]\n\n\nrxy_01 <- rxy_fun(\n  rx = runif,\n  f = \\(x) x^2,\n  reps = \\(x) rnorm(length(x), sd = .01)\n  )\n\n\nIn this model, \\(\\mathbb E (Y \\vert X)\\) is not linear in \\(X\\)\n(first order misspecification), but the remaining assumptions of the linear\nmodel hold. This is how a typical linear fit of data generated from this model\nlooks like:\n\n\nplot(rxy_01, N = 300)\n\n\n\nHere the effect of misspecification on the variance-covariance model trusting\nestimates is to underestimate true covariance values\n(by a factor as large as 40%!):\n\n\nlmsim(rxy_01)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n              (Intercept)             x\n(Intercept)  0.0002277348 -0.0003417356\nx           -0.0003417356  0.0006833282\n\n* Simulation-based vcov (vcov of coefficient estimates):\n              (Intercept)             x\n(Intercept)  0.0003367876 -0.0005662584\nx           -0.0005662584  0.0011488351\n\n* Ratio (1st / 2nd):\n            (Intercept)         x\n(Intercept)   0.6761971 0.6034976\nx             0.6034976 0.5948009\n\nThis is fixed by the sandwich::vcovHC() estimators:\n\n\nlmsim(rxy_01, vcov = sandwich::vcovHC)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n              (Intercept)             x\n(Intercept)  0.0003475834 -0.0005732957\nx           -0.0005732957  0.0011443449\n\n* Simulation-based vcov (vcov of coefficient estimates):\n              (Intercept)             x\n(Intercept)  0.0003367876 -0.0005662584\nx           -0.0005662584  0.0011488351\n\n* Ratio (1st / 2nd):\n            (Intercept)         x\n(Intercept)    1.032055 1.0124276\nx              1.012428 0.9960916\n\nExample 2: Second order misspecification\n\\[\nY = X + \\varepsilon,\\quad X \\sim \\text{Unif} (0,1),\\qquad \\varepsilon \\sim \\mathcal N (0,X)\n\\]\n\n\nrxy_02 <- rxy_fun(\n  rx = runif,\n  f = \\(x) x,\n  reps = \\(x) rnorm(length(x), sd = x)\n  )\n\nplot(rxy_02, N = 300)\n\n\n\nThis model is first-order consistent, but second-order misspecified (variance is\nnot independent of \\(X\\)). The effects on vcov model-trusting estimates is\nmixed: some covariances are underestimated, some are overestimated.\n\n\nlmsim(rxy_02)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n            (Intercept)           x\n(Intercept)  0.01344466 -0.02014604\nx           -0.02014604  0.04008595\n\n* Simulation-based vcov (vcov of coefficient estimates):\n             (Intercept)           x\n(Intercept)  0.005456494 -0.01417346\nx           -0.014173461  0.04834196\n\n* Ratio (1st / 2nd):\n            (Intercept)         x\n(Intercept)    2.463974 1.4213920\nx              1.421392 0.8292164\n\nAgain, this large bias is corrected by the sandwich estimator:\n\n\nlmsim(rxy_02, vcov = sandwich::vcovHC)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n             (Intercept)           x\n(Intercept)  0.005637138 -0.01451506\nx           -0.014515056  0.04909868\n\n* Simulation-based vcov (vcov of coefficient estimates):\n             (Intercept)           x\n(Intercept)  0.005456494 -0.01417346\nx           -0.014173461  0.04834196\n\n* Ratio (1st / 2nd):\n            (Intercept)        x\n(Intercept)    1.033106 1.024101\nx              1.024101 1.015653\n\nExample 3: sample size effects\nThe sandwich estimators only become unbiased in the large sample\nlimit. For instance, in our previous Example 1, the sandwich covariance\nestimates require sample sizes of \\(N \\approx 50\\) or larger, in order for their\nbias to be relatively contained (\\(\\lesssim 10\\%\\)). With a small sample size:\n\n\nlmsim(rxy_01, N = 10, vcov = sandwich::vcovHC)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n             (Intercept)           x\n(Intercept)  0.008253143 -0.01356350\nx           -0.013563503  0.02691423\n\n* Simulation-based vcov (vcov of coefficient estimates):\n             (Intercept)            x\n(Intercept)  0.005084963 -0.008573385\nx           -0.008573385  0.017136158\n\n* Ratio (1st / 2nd):\n            (Intercept)        x\n(Intercept)    1.623049 1.582048\nx              1.582048 1.570611\n\nFor such small sample sizes, however, one should probably also keep into account the bias in the point estimate \\(\\hat \\beta\\) itself, so that the bias in the variance \\(\\mathbb V (\\hat \\beta)\\) becomes a kinda second-order problem.\nExample 4: variance underestimation and overestimation\nAccording to the heuristics of (Buja et al. 2019), the linear model trusting\nvariances \\(\\mathbb V (\\hat \\beta)_{ii}\\) tend to underestimate (overestimate) the\ntrue variances:\nIn the presence of non-linearity, when the strong deviations from linearity\nare far away from (close to) the center of the regressor distribution.\nIn the presence of heteroskedasticity, when the regions of high variance are\nfar away from the (close to) the center of the regressor distribution.\nWe illustrate the second case. Consider the following two models:\n\\[\nY = X + \\varepsilon,\\quad X \\sim \\text{Unif} (0,1),\\qquad \\varepsilon \\sim \\mathcal N (0,\\vert X-\\frac{1}{2}\\vert )\n\\]\n\n\nrxy_04a <- rxy_fun(\n  rx = runif,\n  f = \\(x) x,\n  reps = \\(x) rnorm(length(x), sd = abs(0.5 - x))\n  )\n\nplot(rxy_04a)\n\n\n\n\\[\nY = X + \\varepsilon,\\quad X \\sim \\text{Unif} (0,1),\\qquad \\varepsilon \\sim \\mathcal N (0,\\frac{1}{2}-\\vert X-\\frac{1}{2}\\vert )\n\\]\n\n\nrxy_04b <- rxy_fun(\n  rx = runif,\n  f = \\(x) x,\n  reps = \\(x) rnorm(length(x), sd = 0.5 - abs(0.5 - x))\n  )\n\nplot(rxy_04b)\n\n\n\nIn agreement with the heuristics, we have, for the first model:\n\n\nlmsim(rxy_04a)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n             (Intercept)            x\n(Intercept)  0.003326042 -0.004989057\nx           -0.004989057  0.009977552\n\n* Simulation-based vcov (vcov of coefficient estimates):\n             (Intercept)            x\n(Intercept)  0.005390525 -0.009154439\nx           -0.009154439  0.018296535\n\n* Ratio (1st / 2nd):\n            (Intercept)         x\n(Intercept)   0.6170162 0.5449878\nx             0.5449878 0.5453247\n\nand, for the second model:\n\n\nlmsim(rxy_04b)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n             (Intercept)            x\n(Intercept)  0.003420946 -0.005150512\nx           -0.005150512  0.010300847\n\n* Simulation-based vcov (vcov of coefficient estimates):\n             (Intercept)            x\n(Intercept)  0.001590907 -0.001503471\nx           -0.001503471  0.003131620\n\n* Ratio (1st / 2nd):\n            (Intercept)        x\n(Intercept)    2.150312 3.425748\nx              3.425748 3.289303\n\nIt is interesting to notice that, far away from the large-sample limit, the\nsandwich estimates also have a bias (as discussed in the previous example),\nbut the bias leads to an overestimate of \\(\\mathbb V (\\hat \\beta)\\)\nin both cases5:\n\n\nlmsim(rxy_04a, N = 10, vcov = sandwich::vcovHC)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n            (Intercept)          x\n(Intercept)  0.07714254 -0.1302820\nx           -0.13028198  0.2595908\n\n* Simulation-based vcov (vcov of coefficient estimates):\n            (Intercept)          x\n(Intercept)  0.05560994 -0.0957307\nx           -0.09573070  0.1947398\n\n* Ratio (1st / 2nd):\n            (Intercept)        x\n(Intercept)    1.387208 1.360922\nx              1.360922 1.333013\n\nlmsim(rxy_04b, N = 10, vcov = sandwich::vcovHC)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n            (Intercept)           x\n(Intercept)  0.05301354 -0.07223407\nx           -0.07223407  0.13959714\n\n* Simulation-based vcov (vcov of coefficient estimates):\n            (Intercept)           x\n(Intercept)  0.02725563 -0.03408101\nx           -0.03408101  0.06735272\n\n* Ratio (1st / 2nd):\n            (Intercept)        x\n(Intercept)    1.945049 2.119481\nx              2.119481 2.072628\n\nConclusions\nSandwich estimators provide valid inference for parameter covariances and\nstandard errors in misspecified linear regression settings.\nThese model-robust tools are available in R through\n{sandwich} (which also provides\nmethods for more general glm objects).\nFor fairly large datasets, this model-robust approach can be coupled with data\nsplitting, leading to a modeling procedure which I’m finding to be quite solid\nand versatile in practice:\nPerform data exploration and model selection on a separate portion of data.\nThis is to avoid biasing inferential results with random selective procedures.\nOnce a reasonable model is found, fit the model on the remaining data,\nadopting robust covariance estimates for model parameters.\nThis works very well with independent data for which a (generalized) linear\nmodel can provide a useful parametric description. Generalizations may be\ndiscussed in a separate post.\n\n\n\nBuja, Andreas, Lawrence Brown, Richard Berk, Edward George, Emil Pitkin, Mikhail Traskin, Kai Zhang, and Linda Zhao. 2019. “Models as Approximations i.” Statistical Science 34 (4): 523–44.\n\n\nWhite, Halbert. 1980. “A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.” Econometrica: Journal of the Econometric Society, 817–38.\n\n\n\nThis is already somewhat implicit in the representation (1), that\nmodels \\(Y\\) and \\(X\\) as single random variables. The reason for stating this condition in an apparently convoluted way, rather than a simpler “data points \\((X_i,Y_i)\\) are independent draws from the same joint distribution”, is that this formulation includes cases where the \\(X_i\\)’s are not independent, cf. the following note.↩︎\nThere are of course important exceptions, like time series or spatial data. Noteworthy, our formulation of strict linear model assumptions can also cover some cases of temporal or spatial dependence in the regressors \\(X_i\\), provided that such dependence is not reflected on \\(Y_i \\vert X_i\\).↩︎\nAccording to an \\(L_2\\) loss criterion.↩︎\nI use an element-wise ratio,\nin order to avoid confusion from the different scales involved in the various\nentries of \\(\\mathbb V (\\hat \\beta)\\).↩︎\n\nI don’t know whether this result (that sandwich estimates are, at worst,\noverestimates) is a general one.↩︎\n",
    "preview": "posts/2023-05-14-model-misspecification-and-linear-sandwiches/misspecification-and-linear-sandwiches_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2023-05-22T01:26:47+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-05-12-consistency-and-bias-of-ols-estimators/",
    "title": "Consistency and bias of OLS estimators",
    "description": "OLS estimators are consistent but generally biased - here's an example.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-05-12",
    "categories": [
      "Statistics",
      "Regression",
      "Linear Models",
      "Model Misspecification"
    ],
    "contents": "\nGiven random variables \\(Y\\colon \\Omega \\to \\mathbb R\\) and\n\\(X\\colon \\Omega \\to \\mathbb R ^{p}\\) defined on an event space \\(\\Omega\\), denote:\n\\[\n\\beta = \\arg \\min _{\\beta ^\\prime } \\mathbb E[(Y-X \\beta^\\prime )^2]= \\mathbb E(X^TX)^{-1}\\mathbb E(X^TY), \\tag{1}\n\\]\nso that \\(X \\beta\\) is the best linear predictor of \\(Y\\) in terms of \\(X\\) (\\(X\\) is\nregarded as a row vector).\nLet \\((\\textbf Y, \\textbf X)\\) be independent samples from the joint \\(XY\\)\ndistribution, with independent observations stacked vertically in \\(N \\times 1\\)\nand \\(N \\times p\\) matrices respectively, as customary. Then the usual Ordinary\nLeast Squares (OLS) estimator of \\(\\beta\\) is given by:\n\\[\n\\hat \\beta = \\arg \\min _{\\beta ^\\prime}(\\textbf Y - \\textbf X \\beta ^\\prime)^2=(\\textbf X^T\\textbf X)^{-1} \\textbf X^T \\textbf Y. \\tag{2}\n\\]\nThis is a consistent, but generally biased estimator of \\(\\beta\\).\nComparing Eqs. (1) and (2), consistency follows immediately\nfrom the law of large numbers and continuity. In order to show that\n\\(\\mathbb E (\\hat \\beta) \\neq \\beta\\) in general, it is sufficient to provide an\nexample.\nConsider, for instance (example adapted from D.A. Freedman):\n\\[\nX \\sim \\mathcal N (0, 1),\\qquad Y=X(1+aX^2)\n\\]\nRecalling that \\(\\mathbb E (X^4) = 3\\) for the standard normal, we have:\n\\[\n\\beta = 1+3a,\n\\]\nwhere we have ignored a potential intercept term (which would vanish here, since\n\\(\\mathbb E (Y) = 0\\)). To compute \\(\\mathbb E (\\hat \\beta)\\), we use the identity\n\\(\\frac{e^{-z}}{z} = \\intop _1 ^\\infty \\text d t\\, e ^{-zt}\\) to rewrite this\nexpected value as:\n\\[\n\\begin{split}\n\\mathbb E (\\hat \\beta) & =  (2 \\pi)^{-N/2}\n    \\intop \\text d\\textbf X \\,e^{-\\sum _j X_i ^2 /2}\n                                    \\dfrac{\\sum _i X_i^2(1+aX_i^2)}{\\sum _i X_i^2} = \\frac{N}{2}\\intop_1 ^\\infty \\text d t\\,I(t) \\\\\nI(t)                                     & \\equiv (2 \\pi)^{-N/2} \\intop \\text d\\textbf X\\,\n                                                        e^{-t \\sum _j X_j ^2 /2}X_1^2(1+aX_1^2)\n\\end{split}\n\\]\nThe inner integral can be computed easily:\n\\[\nI(t) = t^{-\\frac{N}{2}}(\\frac{1}{t}+a\\frac{3}{t^2})\n\\]\nand we eventually find:\n\\[\n\\mathbb E (\\hat \\beta) = 1+3 a\\frac{N}{N+2}\n\\]\nThe bias is thus given by:\n\\[\n\\beta - \\mathbb E (\\hat \\beta) = \\frac{6a}{N+2}\n\\]\nThis vanishes linearly, in agreement with the fact that\n\\(\\sqrt N (\\hat \\beta - \\beta )\\) converges in probability to a gaussian with\nzero mean and finite variance (which requires the bias to be \\(o(N^{-1/2})\\)).\n\n\n\n",
    "preview": {},
    "last_modified": "2023-05-22T01:26:47+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-05-01-magic-piggy-bank/",
    "title": "Bayes, Neyman and the Magic Piggy Bank",
    "description": "Compares frequentist properties of credible intervals and confidence \nintervals in a gambling game involving a magic piggy bank.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-05-01",
    "categories": [
      "Statistics",
      "Confidence Intervals",
      "Frequentist Methods",
      "Bayesian Methods"
    ],
    "contents": "\nIntro\nFrequentist and Bayesian approaches to statistical inference are motivated by\ndifferent interpretations of the concept of probability.\nThese philosophical differences can, at times, shadow the comparably important\noperational differences between the two frameworks, whose methods proceed, at\nthe end of the day, from the same mathematical theory.\nFrom the purely operational point of view, the\nquestion “Bayesian or Frequentist?” can (and should) be answered by\nobjective criteria, rather than subjective opinions. As one could\nexpect, the answer is in general neither “Frequentist” nor “Bayesian”,\nbut rather “It depends”.\nTo illustrate this, I will discuss an hypothetical game that revolves around\nreporting measurements and correctly quantifying uncertainty. As we shall see,\nthe winning strategies can be either Frequentist or Bayesian in spirit,\ndepending on a variation of the actual rules of the game.\nReporting measurements\nAll scientific measurements come with an associated uncertainty, which can be\nexpressed in the form of an interval that is supposed to contain the object\nof measurement. In the Frequentist and Bayesian\nframeworks, these intervals are traditionally dubbed Confidence and Credible\nintervals, respectively. While, superficially, these can be both characterized\nas “covering the true value with probability \\(p\\)”, the word “probability”\nhas quite different connotations in the two cases, and confusing them can lead\nto irrational thought or, as in the imaginary game described below, financial\nruin.\nMagic Piggy Bank\nThere are two players, called the Bookmaker and the Gambler, that compete\nagainst each other in a gambling game1. The interactions between these two players are mediated by\nthe Magic Piggy Bank, a magic creature that acts as a sort of referee.\nThe Magic Piggy Bank contains infinite biased coins, and knows the probability\n\\(\\Theta\\) of giving “tails” for each one of them.\nA single iteration of the game proceeds as follows:\nThe Magic Piggy Bank ejects 2 a biased coin and gives it to the Bookmaker.\nThe Bookmaker can flip the coin an arbitrary number of times, to produce an\nestimate of \\(\\Theta\\), in the form of an interval \\(I\\). This must be accompanied\nby a payout, that is a number \\(p\\in \\left(0,1\\right)\\), for bets on the event\n\\(\\Theta \\in I\\). The resulting \\(I\\) and\n\\(p\\), together with the original data \\(X=(n_\\text{tosses}, n_\\text{tails})\\) from\nthe Bookmaker’s experiments, are reported to the Magic Piggy Bank.\nThe Magic Piggy Bank communicates the payout \\(p\\) to the Gambler, and reveals\nsome additional information. What particular information is revealed depends\non the variant of the game being played (see descriptions below).\nBased on the information received, the Gambler can choose to bet either\nin favor or against \\(\\Theta \\in I\\). When betting in favor, the Gambler pays \\(p\\)\nto the Bookmaker, who returns back \\(1\\) if \\(\\Theta \\in I\\) obtains. When betting\nagainst, the Bookmaker pays \\(p\\) to the Gambler, who returns back \\(1\\) if\n\\(\\Theta \\in I\\) obtains.\nThe Magic Piggy Bank reveals all data (\\(X\\), \\(I\\), \\(\\Theta\\)) to both players\nand the scores are settled.\nAs to the third step, we will consider three variants of the game:\nThe Magic Piggy Bank tells the Gambler the results of the Bookmaker’s tosses\n\\(X=(n_\\text{tosses}, n_\\text{tails})\\), as well as the actual interval \\(I\\).\nThe Magic Piggy Bank tells the Gambler the true value of \\(\\Theta\\).\nThe Gambler is given no additional information beyond the established payout\n\\(p\\).\nProblem\nSuppose that the Bookmaker and Gambler are forced to play indefinitely.\nWhat are the best strategies for these two players, according to\nthe three different variants A, B, and C described above3?\nAnalysis\nOne can readily verify that the Gambler’s gain (or, equivalently, the\nBookmaker’s loss) in a single iteration of the game is given by:\n\\[\nG=b\\cdot (\\chi_I (\\Theta)-p) \\tag{1}\n\\]\nwhere, \\(b\\) is equal to \\(\\pm 1\\) if the Gambler bets in favor or against,\nrespectively, and:\n\\[\n\\chi _I (\\Theta) = \\begin{cases}\n1 & \\Theta \\in I \\\\\n0 & \\Theta \\notin I\n\\end{cases} \\tag{2}\n\\]\nThe expected gain is given by:\n\\[\n\\mathbb E (G) = \\intop \\text{d}P(\\Theta,X) \\,b\\cdot (\\chi_I (\\Theta)-p), \\tag{3}\n\\]\nwhere \\(\\text{d} P(\\Theta, X)\\) denotes the joint probability measure of \\(\\Theta\\)\nand \\(X\\).\nLet’s now examine in detail the three different variants (A, B, C) of the game\ndescribed above.\nVariant A\nIn the first variant of the game, the Gambler is given the same information as the\nBookmaker. In particular, the choice to bet in favor or against, represented by\nthe sign \\(b\\), cannot depend on \\(\\Theta\\) (which the Gambler doesn’t know), and we\ncan rewrite the expected gain (3) as4:\n\\[\n\\begin{split}\n\\mathbb E (G) &= \\intop \\text{d}P(X) \\,b\\cdot \\intop \\text{d}P(\\Theta \\vert X) \\,(\\chi_I (\\Theta)-p) \\\\\n& = \\intop \\text{d}P(X) \\,b \\cdot \\left(\\text {Pr}(\\Theta \\in I \\vert X)-p\\right)\n\\end{split} \\tag{4}\n\\]\nwhere we have used the fact that, for any random variable \\(Y\\) and set \\(E\\), the\nfollowing relation holds:\n\\[\n\\mathbb E (\\chi _E (Y)) = \\text{Pr}(Y \\in E).\n\\]\nNow, since both \\(X\\) and \\(I\\) are known to the Gambler, the latter is (at least in\nprinciple) able to compute:\n\\[\nb_A \\equiv \\text{sgn}\\left(\\text {Pr}(\\Theta \\in I \\vert X)-p\\right) \\tag{5}\n\\]\nIn practice, in order to compute (5), the Gambler would need\nto know the overall distribution \\(\\pi (\\Theta)\\) of the coins \\(\\Theta\\) extracted\nfrom the Magic Piggy Bank, but this is something that can be accurately\nestimated in the long run, since the actual values of \\(\\Theta\\) are revealed\nat the end of each iteration 5.\nPlugging Eq. (5) into Eq. (4), we find:\n\\[\n\\mathbb E (G) = \\intop \\text{d}P(X) \\left|\\text {Pr}(\\Theta \\in I \\vert X)-p\\right|\\quad\\text{(Variant A)}   \\tag{6}.\n\\]\nComparing with (4), it is clear that\n(6) is the maximum expected gain, for any choice\nof \\(b\\). In other words, the choice \\(b_A\\) in Eq. (5) is an\noptimal one.\nFinally, from the Bookmaker’s point of view,\nEq. (6) represents a sure loss in the long run,\nthat can only be avoided by enforcing:\n\\[\n\\text {Pr}(\\Theta \\in I \\vert X)=p \\quad \\text{(Variant A)} \\tag{7}\n\\]\nIn order to ensure this, the Bookmaker needs to know as well the overall\ncoins’ distribution \\(\\pi (\\Theta)\\), and the same remarks made above for the\nGambler apply here.\nEquation (7) defines what is known as a\nBayesian credible interval.\nVariant B\nWe now consider the second variant of the rules, where the Gambler is told the\ntrue value of \\(\\Theta\\), but does not know the details of the Bookmaker’s\nmeasurement, except for the established payout \\(p\\). Using a reasoning similar\nto the previous section we rewrite:\n\\[\n\\begin{split}\n\\mathbb E (G) &= \\intop \\text{d}P(\\Theta) \\,b\\cdot \\intop \\text{d}P(X \\vert\\Theta) \\,(\\chi_I (\\Theta)-p) \\\\\n& = \\intop \\text{d}P(\\Theta) \\,b \\cdot \\left(\\text {Pr}(\\Theta \\in I \\vert \\Theta)-p\\right)\n\\end{split} \\tag{8}\n\\]\nand define6:\n\\[\nb_B \\equiv \\text{sgn}\\left(\\text {Pr}(\\Theta \\in I \\vert \\Theta)-p\\right)\\quad(\\text{Variant B}) \\tag{9}\n\\]\nwhich is easily shown to be the optimal betting strategy for the Gambler in the\npresent setting. In the long run, this sign can be accurately estimated by\nmodeling the conditional mean of \\(\\chi _I (\\Theta) - p\\) (as a function of\n\\(\\Theta\\) and \\(p\\)).\nIf the Gambler bets according to (9), the Bookmaker is forced\nto set payouts according to:\n\\[\n\\text {Pr}(\\Theta \\in I \\vert \\Theta)=p\\quad(\\text{Variant B}), \\tag{10}\n\\]\nin order to avoid a certain loss.\nEquation (10) defines what is known as a\nFrequentist confidence interval.\nVariant C\nIn the last case, the Gambler has no extra information beyond the payout \\(p\\),\nand the expected gain reduces to:\n\\[\n\\mathbb E (G)=b\\cdot \\left(\\text{Pr}(\\Theta \\in I)-p\\right),\\tag{11}\n\\]\nwhere \\(\\text{Pr}(\\Theta \\in I)\\) is the unconditional probability that \\(I\\) covers\n\\(\\Theta\\). The optimal betting choice is:\n\\[\nb_C \\equiv \\text{sgn}\\left(\\text {Pr}(\\Theta \\in I)-p\\right)\\quad(\\text{Variant C}) \\tag{12}\n\\]\nwhich forces the Bookmaker to set payouts according to:\n\\[\n\\text {Pr}(\\Theta \\in I)=p\\quad(\\text{Variant C}). \\tag{13}\n\\]\nThis is, by the way, satisfied by both the Bayesian and Frequentist intervals,\ndue to Eqs. (7) and (10), respectively.\nSummary of results\nProvided access to the same data used by the Bookmaker to\nproduce the interval \\(I\\) (Variant A), a rational Gambler would bet in favor of\n\\(\\Theta \\in I\\) if the probability of this event\nconditional to the observed the data is greater than the payout \\(p\\)\n(Eq. (5)).\nOn the other hand, given true value of \\(\\Theta\\) (Variant B), the optimal choice\nfor a Gambler is to bet on \\(\\Theta \\in I\\) if the probability of this event\nconditional to the ground truth is greater than \\(p\\)\n(Eq. (9)).\nFinally, in the lack of any of this information (Variant C), the most rational\nchoice is simply to bet on \\(\\Theta \\in I\\) if this event occurs more frequently\nthan \\(p\\) (Eq. (12)).\nWhen playing against the first two types of players, in order to avoid a certain\nloss, the Bookmaker must produce Bayesian credible intervals (Variant A) or\nFrequentist confidence intervals (Variant B). In the remaining case (Variant C),\nthe Bookmaker can either produce Bayesian or Frequentist intervals7.\nConclusions\nWhen I first learned about Bayesian and Frequentist inference, I remember most\ndiscussions were focused on the philosophical differences between these two\nschools of thought. There was little to no mention about the actual mathematical\nproperties of the constructs prescribed by the two formalisms, which made the\nchoice between “Bayesian” or “Frequentist” look like a mere matter of\ncommitting to one particular view.\nTechnically, what I did here was to compare the frequentist properties of\ncredible intervals and confidence intervals. I’m sure the literature,\nincluding the pedagogical one, is full of examples like this, and better ones8. With no pretense of originality, I believe that including more examples\nof this kind in the usual presentations can be beneficial to students and\npractitioners, and perhaps help them out of the ugly black-box of orthodoxy.\n\nThe introduction of bets as an expedient\nto operationally define subjective probabilities is historically due to the\nItalian mathematician\nBruno de Finetti.\nThe statistical analysis of the game proposed below can be given a Frequentist\ninterpretation.↩︎\nReaders are free to imagine this process in the way they find more convenient.↩︎\nWe assume that both\nplayers know from the outset which variant of the game they are playing to.↩︎\n\nWe denote (with some abuse of notation) by\n\\(\\text{d}P(\\Theta \\vert X)\\) the conditional probability measure of \\(\\Theta\\)\nconditioned on \\(X\\).↩︎\nIn the Bayesian spirit of (5),\nthe Gambler could for instance estimate \\(\\pi(\\Theta)\\) through Bayesian updates\nof a Dirichlet prior.↩︎\nNoteworthy, the random quantity in this equation is \\(I\\),\nwhereas \\(\\Theta\\) is regarded as fixed. This is in stark contrast with\nEq. (5), where \\(X\\) and \\(I\\) were fixed, and \\(\\Theta\\) was\nrandom.↩︎\n\nThere are, in fact, infinitely many more ways to produce intervals with the\nunconditional coverage property Eq. (13).↩︎\n\nI see that Jaynes (the father of the Maximum Entropy foundation of\nStatistical Mechanics, among other things) has a full essay paper on\nConfidence Intervals vs. Bayesian Intervals, which I haven’t read -\nthe abstract sounds a bit loaded to me, but it’s probably definitely worth to\nread.↩︎\n",
    "preview": {},
    "last_modified": "2023-05-22T01:26:47+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-10-correlation-without-causation/",
    "title": "Correlation Without Causation",
    "description": "*Cum hoc ergo propter hoc*",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-03-30",
    "categories": [
      "Statistics"
    ],
    "contents": "\nIt is part of common knowledge that correlation does not require causation.\nAbsence of causation, say between a condition \\(p\\) and an effect \\(q\\), means that the realization of \\(p\\) has no influence on the presence of \\(q\\). If this is the case,\na statistical correlation between \\(p\\) and \\(q\\) can still be present, if the realization of \\(p\\) modifies our state of information about \\(q\\).\nAs an example, let \\(X,Y\\) be two conditionally independent binary random variables, with a common probability \\(\\Theta\\) of evaluating to one. Think, for instance,\nof a machine that produces pairs of identical biased coins, with a probability of tails \\(\\Theta\\).\nIf \\(\\Theta\\) is equal to a given value \\(\\theta\\), the joint probability distribution of \\(X\\) and \\(Y\\) is:\n\\[\n\\text {Pr}(X=x,Y=y\\vert \\Theta = \\theta) = B(x;\\theta)B(y;\\theta), \\tag{1}\n\\]\nwhere \\(B(z; \\theta) = \\theta ^z (1 - \\theta) ^ {1-z}\\).\nWhether or not this provides a satisfying probabilistic description of experiments on \\(X\\) and \\(Y\\) depends on context.\nFrom a frequentist point of view, if \\(\\Theta\\) is fixed once and for all, the right hand side of Eq. (1) correctly describes the experimental outcomes of \\(X\\) and \\(Y\\) for some value of \\(\\theta\\). On the other hand, if \\(\\Theta\\) can change from experiment to experiment in a random fashion, and we do not observe its values \\(\\theta\\), we clearly cannot use Eq. (1) as it stands, as its usage requires knowing \\(\\theta\\).\nFinally, from a bayesian’s point of view, if \\(\\Theta\\) is fixed but unknown, Eq. (1) does not describe our state of knowledge about \\(X\\) and \\(Y\\), because it assumes unavailable information (\\(\\Theta = \\theta\\)).\nIn the last two cases, what we’re actually after is the unconditional probability:\n\\[\n\\text{Pr}(X=x,\\,Y=y)=\\intop\\,\\text{d}P_\\Theta(\\theta) \\,\\text{Pr}(X=x,Y=y\\vert\\Theta = \\theta)\n\\tag{2}\n\\]\nwhere \\(\\text{d}P_\\Theta(\\theta)\\) can be regarded either as the actual probability distribution of \\(\\Theta\\) (in a frequentist framework) or as a subjective prior distribution (in a bayesian framework).\nPlugging Eq. (1) into (2), we find:\n\\[\n\\begin{split}\n\\text{Pr}(X=1,\\,Y=1) & = \\mathbb E(\\Theta)^2+\\text{Var}(\\Theta)\\\\\n\\text{Pr}(X=1,\\, Y=0)&=\\mathbb E(\\Theta)-\\mathbb E(\\Theta)^2-\\text{Var}(\\Theta)\\\\\n\\text{Pr}(X=0,\\, Y=1)&=\\mathbb E(\\Theta)-\\mathbb E(\\Theta)^2-\\text{Var}(\\Theta)\\\\\n\\text{Pr}(X=0,\\,Y=0) & = \\mathbb (1-\\mathbb E(\\Theta))^2+\\text{Var}(\\Theta) \\\\\n\\end{split}\n\\]\nIn particular, we have:\n\\[\n\\dfrac{\\text{Pr}(Y = 1 \\vert\\, X = 1)}{\\text {Pr}(Y=1)} = 1+\\frac{\\text{Var}(\\Theta)}{\\mathbb{E}(\\Theta)^2},\n\\tag{3}\n\\]\nwhich means that, unconditionally, \\(X\\) and \\(Y\\) are not independent, but in fact positively correlated1.\nObservations of this kind apply, mutatis mutandis, in many practical situations. For instance if we were modeling the time series of new visitors to a website, we could reasonably assume that the number of yesterday’s new visitors does not influence the number of today’s ones (if individual visitors are unlikely to interact with each other). Yet, it would be wrong to assume, and easy to disprove, that these two numbers are by themselves statistically independent, because yesterday’s new visitors carry useful background information on today’s potential new visitors.\nThe bottom line of the post is that lack of causation does not imply lack of correlation, which is logically equivalent to the original motto… but, for some strange reason, I find easier to forget.\n\nHere I’m using the word correlation in a loose sense, as in the popular motto.↩︎\n",
    "preview": {},
    "last_modified": "2023-05-22T01:26:47+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-07-posi-2/",
    "title": "How to get away with selection. Part II: Mathematical Framework",
    "description": "Mathematicals details on Selective Inference, model misspecification and coverage guarantees.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2022-11-25",
    "categories": [
      "Statistics",
      "Selective Inference",
      "Model Misspecification"
    ],
    "contents": "\nIntroduction\nIn a previous post I introduced the problem of\nSelective Inference and illustrated, in a simplified setting, how selection\ngenerally affects the coverage of confidence intervals - when they are both\nselected and constructed using the same data. While the example was\n(hopefully) helpful to build some intuition, in order to discuss\n“How to get away with selection” in a comprehensive manner we need to make a\nfew clarifications. In particular, we need to answer the following questions:\nWhat is the target of our Selective Inference?\nWhat statistical properties would we like our inferences to have?\nSearching through the literature, I realized there exist a bunch of variations\non these two themes, which give rise to different mathematical formalisms.\nSpecifying these points is mandatory for any further discussion, so my main goal\nhere is to present these different points of view and explain some of their\npros and cons.\nMathematical Framework\nRegression and parameter estimation\nIn order to avoid getting carried away with too much abstraction, I will focus\non a specific type of problem, that is parameter estimation in regression. As\nfar as I can tell, this represents no serious loss in generality, and most of\nthe notions I’m going to outline would carry over to more general problems in a\nstraightforward manner.\nBroadly speaking, the goal of regression is to understand the dependence of a\nset of random variables \\(Y\\) from another set of random variables \\(X\\). More precisely,\nwe’re interested in the conditional probability distribution of \\(Y\\),\nconditioned on the observation of \\(X\\), which can always be represented as:\n\\[\nY = f(X)+\\varepsilon,\\qquad \\mathbb E(\\varepsilon|X)\\equiv 0.\n\\tag{1}\n\\]\nwhere \\(f(X) = \\mathbb E(Y|X)\\) is the conditional mean of \\(Y|X\\), and \\(\\varepsilon\\)\nis a random variable with vanishing conditional mean, sometimes called the “error term”.\nParameter estimation means that we have (somehow) chosen functional forms for\nthe conditional mean and for the probability distribution of the error term,\nand we want to provide estimates for the parameters defining these two functions.\nEnter selection\nNow, in many applications we actually don’t have much insight about the correct\nfunctional form \\(f\\), nor of the distribution of the error term \\(\\varepsilon\\).\nGiven a dataset of experimental observations of \\(Y\\) and \\(X\\), we are thus faced\nwith two tasks:\nSelection. Choose an adequate model \\(\\hat M = (\\hat f,\\,\\hat \\varepsilon)\\)\nfor the true \\(f\\) and \\(\\varepsilon\\), usually from a (more or less) pre-specified\nfamily of initial guesses \\(\\mathcal M =\\{(f_i,\\varepsilon_i)\\}_i\\), using a\n(more or less) pre-specified criterion.\nPost-Selection Inference. Perform inference with the chosen model. In the\nstudy case we’re considering, this amounts to provide confidence intervals for\nmodel parameters.\nIt is, of course, the need to use the same data for both tasks which gives rise\nto complications.\nInferential target\nWe now come to the first question raised in the Introduction, regarding the\nnature of the inferential target. And now more concretely:\nwhat are the true values of the parameters we’re trying to estimate?\nOne can appreciate that the answer necessarily depends on how we consider the\nfinal output of the modeling procedure:\n(Model Trusting) As the true data generating process, or\n(Assumption Lean) As an approximation of the (partially or totally\nunknown) data generating process, chosen in a data-driven fashion within\na family of initial guesses \\(\\mathcal M\\).\nAccording to the first interpretation, there’s no room for ambiguity: the\ntargets of our estimates should clearly be the true parameter values,\nwhose definition does not depend on any modeling choice. The second\ninterpretation, on the other hand, leaves a certain amount of\nfreedom in this respect. Here, I will follow the point of view advocated by\n(Berk et al. 2013), according to which the target parameters\nare those providing the best approximation1 to the true data generating\nprocess, according to the functional form chosen in the selection stage.\nI believe both positions have their merits and flaws, and which one is more\nappropriate largely depends on context. In a reductionist field like\nHigh Energy Physics, whose eventual goal is to explain the fundamental laws of\nNature, the Model Trusting point of view is usually taken,\nand with good reason. When studying more emergent phenomena, on the other hand,\nthe quest for fundamental laws is often meaningless (or at best wishful\nthinking), and the Assumption Lean standpoint looks more reasonable. In any\ncase, here the differences are not merely philosophical ones, as the two\ninterpretations give rise to different mathematical formalisms.\nIn the following posts, I will be mostly focusing on the Assumption Lean\npoint of view. In my opinion, this has two big advantages2:\nConceptual: Inferences have a well-defined meaning even when the model is\nmisspecified3 - which, apart from quite particular cases (see above),\naccounts for the great majority of cases encountered by data analysts in\nthe practice.\nMathematical: It allows to reduce the problem of selective inference to\nthat of simultaneous inference (more on this below).\nFor the latter type of problems, the theory of\nmultiple testing\nreadily provides at least conservative bounds.\nNotions of coverage\nIn addition to the conceptual distinction about the interpretation of the\nselected model, there is also a technical distinction regarding the type\nof coverage guarantees that selective confidence intervals should be endowed\nwith (this is the concrete version of the second question posed in the\nIntroduction).\nHere are some of the notions of coverage I’ve come across:\nMarginal coverage over the selected parameters. We bound at level \\(\\alpha\\)\nthe probability that our procedure constructs any non-covering confidence\ninterval for model parameters \\(\\beta_i\\). Denote by \\(\\widehat M\\) the selected model\nand, with abuse of notation, the corresponding set of selected parameters.\nIf \\(\\widehat{\\text{CI}}_i\\) are the confidence intervals for parameters \\(\\beta _i\\), we\nrequire:\n\\[\n\\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\in \\widehat M) \\geq 1-\\alpha\n\\tag{2}\n\\]\nConditional coverage over the selected parameters. We bound at level\n\\(\\alpha\\) the conditional\nprobability of constructing a non-covering confidence interval, conditioned on\nthe outcome of selection \\(\\widehat M\\). If \\(m\\) is the selected model, we require:\n\\[\n\\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\in m|\\,\\widehat M=m) \\geq 1-\\alpha\n\\tag{3}\n\\]\nFalse Coverage Rate. We bound at level \\(q\\)4 the expected fraction of\nnon-covering confidence intervals out of all intervals constructed:\n\\[\n\\mathbb E \\left( \\dfrac{|i \\in \\widehat M \\colon \\ \\beta_i \\in \\widehat{\\text{CI}}_i|}{|\\widehat M|} \\right)\n\\geq1-q\n\\tag{4}\n\\]\nwhere \\(|S|\\) denotes the cardinality of a set \\(S\\).\nNotice that the random variables in the previous equations are \\(\\widehat M\\) and\n\\(\\widehat{\\text{CI}}_i\\) (denoted by a hat), whereas the true coefficients \\(\\beta_i\\)\nand the selected set \\(m\\) in the case of conditional coverage\n(Eq. (3)) are fixed quantities.\nVariations of these measures focusing on single coefficients are also possible.\nIn practice, in the Assumption Lean framework I just introduced,\nall these coverage measures would not be computed\nunder the selected model’s probability distribution, but rather under a\npre-fixed, more general model for the true probability distribution of \\(Y\\)\nconditional on \\(X\\). We may, for instance, assume that the true error term\n\\(\\varepsilon\\) in Eq. (1) is gaussian with constant\n(\\(X\\)-independent) variance, without making any further assumption on \\(f(X)\\).\nWith enough data, we may even be able to bypass any assumption at all, and\ncompute all relevant quantiles using a bootstrap (Kuchibhotla et al. 2020).\nIn the Model Trusting framework, on the other hand, the conditional coverage\nmeasure would be computed under the selected model… and I’m honestly not\nsure whether it’s possible to make sense of the other two measures in this\nframework.\nSelective vs. Simultaneous Inference\nThe connection between selective and simultaneous inference can now be\nunderstood, through the notion of marginal coverage. In fact, suppose that we\nwere able to provide simultaneous coverage for all parameters\n(selected or not):\n\\[\n\\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i) \\geq 1-\\alpha\n\\tag{5}\n\\]\nThen, it’s easy to see that the same confidence interval would also provide\nmarginal coverage over the selected parameters. In order to see that, simply\nobserve that the simultaneous coverage event can be decomposed as:\n\\[\n(\\beta _i \\in \\widehat {\\text{CI}}_i\\,\\,\\forall i) = (\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\in \\widehat M) \\cap (\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\notin \\widehat M)\n\\]\nwhich implies that:\n\\[\n\\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\in \\widehat M) \\geq \\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i) \\geq 1-\\alpha,\n\\tag{6}\n\\]\nthat is simultaneous coverage implies marginal coverage over the\nselected parameters. In fact, with a few more set-theory manipulations,\none can arrive to a powerful Lemma (see Kuchibhotla et al. 2020 for details):\ncontrolling the marginal coverage (2) at level \\(\\alpha\\)\nfor any model selection procedure5 is equivalent to controlling\nsimultaneous coverage for all possible model selections.\nThis provides us a first, very simple recipe for selective inference, which can\nbe applied whenever one is able to construct confidence intervals for parameters\nin the absence of selection: use any procedure (e.g. \nBonferroni corrections)\nwhich controls simultaneous coverage for all parameters we may select a priori.\nConclusions\nThis was a long and somewhat abstract post, so perhaps the best way to conclude\nis with some bottom lines:\nWhen performing model-based inference, nothing forces us to make working\nhypotheses about the correctness of the model we arrive at. Not making such\nassumptions corresponds to what I called an Assumption Lean framework.\nIn an Assumption Lean framework, the inferential targets are, in general,\nthe best approximations to the truth allowed by the selected model.\nThere exist many type of coverage guarantees for selective confidence\nintervals.\nBounding the probability of any false coverage statement\n(“marginal coverage over the selected parameters”) allows to turn a problem of\nselective inference into one of simultaneous inference.\nIn particular, it is worth to mention that the last observation lead us to a\nsimple recipe for constructing (somewhat conservative, but valid) selective\nconfidence intervals with marginal coverage. In the posts which follow,\nI will discuss some more advanced methods which produce confidence intervals\nsatisfying the requirements discussed here.\n\n\n\nBenjamini, Yoav, and Daniel Yekutieli. 2005. “False Discovery Rate–Adjusted Multiple Confidence Intervals for Selected Parameters.” Journal of the American Statistical Association 100 (469): 71–81.\n\n\nBerk, Richard, Lawrence Brown, Andreas Buja, Kai Zhang, and Linda Zhao. 2013. “Valid Post-Selection Inference.” The Annals of Statistics, 802–37.\n\n\nKuchibhotla, Arun K, Lawrence D Brown, Andreas Buja, Junhui Cai, Edward I George, and Linda H Zhao. 2020. “Valid Post-Selection Inference in Model-Free Linear Regression.” The Annals of Statistics 48 (5): 2953–81.\n\n\nWhere what’s to be considered best is defined in terms of some\nreasonable metric. For instance, for the conditional mean \\(f(X)\\) of a continuous\nresponse \\(Y\\), a convenient target \\(f^*(X)\\) within a prescribed family of\nfunctions \\(\\mathcal F\\) can be defined by\n\\(f^* =\\arg\\min _{\\phi \\in \\mathcal F} \\mathbb E (\\vert f(X) - \\phi (X)\\vert^2)\\).↩︎\nThere’s also a third advantage, which is that I find much harder to think\nabout selective inference from the Model Trusting point of view, hence to write\nblog posts about it - but that’s likely a limitation of my imagination, rather\nthan of the point of view itself.↩︎\nA cool word for “wrong”.↩︎\nWhy \\(q\\) and not \\(\\alpha\\)? Ask (Benjamini and Yekutieli 2005).↩︎\nIt is assumed that the selection is performed from a from a fixed family\nof models \\(\\mathcal M\\).↩︎\n",
    "preview": {},
    "last_modified": "2023-05-22T01:26:47+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-18-posi/",
    "title": "How to get away with selection. Part I: Introduction",
    "description": "Introducing the problem of Selective Inference, illustrated through a simple simulation in R.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2022-11-14",
    "categories": [
      "Statistics",
      "Selective Inference",
      "R"
    ],
    "contents": "\nPrologue\nA few months back, for undocumented circumstances, my browser’s search\nhistory was full of terms like “parameter estimation with variable selection”,\nor “confidence intervals after cross-validation”, or again\n“linear model uncertainties after staring into the abyss”, …\nSparing you my rock bottom, I eventually stumbled upon the right keywords, and\nstarted digging into the mathematical aspects of Selective Inference, or\nPost-Model Selection Inference. Now, while my hands\nare still full of dirt, I’ve decided it’s the right moment to write some\nnotes about what I’ve learned - whose main recipient is the future me,\nwhich will otherwise inevitably forget what the present me thinks he\nknows. If you’re not the future me:\nWelcome 👋\nIf you have detected some imprecision, or have suggestions for this or the\nnext posts, you are more than welcome to create an issue on the source\nrepository of this blog.\nIntroduction\nBroadly speaking, the problem of Selective Inference is that of\nperforming valid statistical inferences when the actual questions of the data analysis are not fixed in advance, but rather selected through data examination. In model-based inference, this lack of\npre-determination usually stems from the (often unavoidable) practice of\nusing the same data to choose an adequate model for the data generating\nprocess and to perform inference. The intrinsic\nrandomness of the selection process has important consequences on the\nprobability of making different guesses about the selected questions,\nwhich, if not properly taken into account, can completely invalidate the\nanalysis results.\nIf this sounds unfamiliar, think about machine-learning: when training a\npredictive model on a given dataset, you would usually consider the\nerror on the same dataset as a poor (optimistic) estimate of the true\nmodel’s error rate, because the model was tuned to perform well on that\ndata in the first place. There we go, Selective Inference! A selection\nfrom an extended family of models1 is performed through data examination,\nand this event introduces a bias in the error estimate of the final\nmodel from training data.\nThe example from machine-learning also suggests a very simple-minded and\nrelatively a-theoretical approach to Selective Inference: data-splitting2. According to this method, we would use only part of the available data to select the questions to be answered by the analysis, while the remaining part would\nbe reserved to perform the actual inference. For this program to\nsucceed, there are however two important requirements: first, we must have\nenough data to ensure decent statistics for both the selection and inference\ntasks; and second, we must be able to split data in two independent\n(or close to independent) sets. This can suppose problems with, e.g.,\ntime-series data. If, on the other hand, these requirements cannot be met, we\nhave to resort to more sophisticated methods.\nAt this point, I would like to stress that the conceptual problems\nI’ve just pointed out will probably look obvious to any reader with a\ndecent intuition for probability3. What is less obvious, but in fact\na fairly active research field in statistics,\nis how to perform valid selective inferences when the “easy” solution of\ndata-splitting I mentioned above is not available. This is where theory\nre-enters the game, and what I’m going to ramble about in this and the next\nposts.\nIllustrations of Selective Inference\nEnough for the speech, let us see how selection can affect (and invalidate)\nclassical inference with a simple-minded simulation.\nSetting\nTo illustrate why naive classical inference can fail in the presence of\nselection, we consider a very simple regression\nproblem involving a single regressor \\(X\\) and a response \\(Y\\), where all the assumptions of the classical linear model hold. In fact, we will assume the true data generating process to be:\n\\[\nY = mX + \\varepsilon, \\qquad \\varepsilon \\sim \\mathcal N (0, \\sigma),\n\\tag{1}\n\\]\nwhere \\(\\varepsilon \\sim \\mathcal N (0, \\sigma)\\) means “\\(\\varepsilon\\) follows a gaussian distribution with mean \\(0\\) and standard deviation \\(\\sigma\\)”.\nA selective modeling procedure\nNow, suppose we are given a dataset of \\(N\\) independent observations\n\\((y_i, x_i)_{i = 1, \\,2,\\, \\dots,\\,N}\\), and we would like to study the\ndependence of \\(Y\\) from \\(X\\). Of course we don’t know the true law, Eq.\n(1), but by a stroke of luck (or by a Taylor expansion\nargument) we make the correct initial guess that such dependence is\nlinear in \\(X\\). We are, however, unsure whether it would be appropriate\nto also include an intercept term in the fit. We thus establish the\nfollowing selective modeling procedure:\nFit a linear model with intercept term,\n\\(Y = mX + q + \\varepsilon\\).\nStop if the intercept estimate is significantly different from zero (say, at the level of 1-\\(\\sigma\\), \\(p\\text{-value}<32\\%\\)). Otherwise:\nFit a model with no intercept, \\(Y = mX + \\varepsilon\\).\nFinally, we use the last fitted model to construct a “naive 95%”\nconfidence interval \\((\\hat m_-, \\hat m_+)\\) for the slope \\(m\\).\nThis is defined by:\n\\[\n\\hat m_\\pm = \\hat m\\pm t_{0.975, \\,N-d} \\cdot \\hat \\sigma _\\hat m\\qquad (95\\%\\,\\text {C.L.}).\n\\tag{2}\n\\]\nHere \\(t_{0.975,\\, N-d}\\) is the 97.5%-quantile of the \\(t\\)-student\ndistribution with \\(N-d\\) degrees of freedom, and \\(d\\) is the number of\nestimated parameters, (\\(2\\) or \\(1\\), according to where we stopped in the\nmodeling procedure). \\(\\hat m\\) and \\(\\hat \\sigma _{\\hat m}\\) are the\nOrdinary Least Squares (OLS) estimates of the slope and its standard\ndeviation, respectively. These are the classical confidence intervals\nreported by the lm() function in R.\nAt a first glance, this procedure might look reasonable. After all, both\nintervals we may end up constructing do have a genuine 95% coverage probability,\nwhen constructed unconditionally… and by selecting the “best” model we’re\nsupposedly choosing the “best” confidence interval. In spite of this qualitative\nargument, we inquire:\n… does it work?\nNow, the question is: how often do the naive CIs (2)\ncover the true parameter \\(m\\) of Eq. (1)? The answer\nbetter be “at least 95% of the times” for our confidence claim in Eq.\n(2) to be valid!\nWe can check the actual coverage of (2) through a simulation.\nHere I’ll assume \\(m = \\sigma = 1\\), and that the\ndataset consists of \\(N=10\\) independent observations of \\(Y\\) at fixed points\n\\(X = (1, \\,2, \\,\\dots ,\\, 10)\\).\n\n\nm <- sigma <- 1  # True parameters\nx <- 1:10  # x covariate, assumed fixed\n\n\nThe following function generates observations of \\(Y\\) according to the distribution (1):\n\n\ngenerate_y <- function(x, m, sigma) {\n  eps <- rnorm(length(x), mean = 0, sd = sigma)\n  return(m * x + eps)\n  }\n\n\nFor example:\n\n\nset.seed(840)\nplot(x, generate_y(x, m, sigma), xlab = \"X\", ylab = \"Y\")\n\n\n\nBelow we generate \\(B=10^4\\) such \\((X,Y)\\) datasets, for each of which we fit a linear model according to the procedure specified above, and check how many\ntimes the true slope \\(m = 1\\) falls in the confidence interval defined by Eq. (2).\n\n\n# Simulation parameters\nB <- 1e4  # Number of replications\n\n# Preallocate logical vectors to be assigned for each replica - for efficiency. \nq_dropped <- logical(B)  # Was the intercept term 'q' dropped? \nm_covered <- logical(B)  # Was the true parameter 'm' covered?\n\n# Set seed for reproducibility\nset.seed(841)\n\n# Logging\ntime_start <- Sys.time()  \n\n# Start the simulation\nfor (b in 1:B) {\n  y <- generate_y(x, m, sigma)\n  \n  # Fit full model (including intercept 'q')\n  fit <- lm(y ~ x + 1)  \n  q_pval <- summary(fit)$coefficients[1, 4]\n  \n  # Is 'q' term \"significant\"? If not, drop 'q' and fit a simpler model\n  if (q_pval > 0.32)  { \n    q_dropped[[b]] <- TRUE\n    fit <- lm(y ~ x - 1) \n  } else {\n    q_dropped[[b]] <- FALSE\n  }\n  \n  # Construct CI for 'm',  using the selected model's fit\n  m_ci <- confint(fit, 'x', level = 0.95)\n  m_covered[[b]] <- m_ci[[1]] < m && m < m_ci[[2]]\n}\n\ntime_end <- Sys.time()\ncat(\"Done :) Took \", as.numeric(time_end - time_start), \" seconds.\")\n\nDone :) Took  12.12071  seconds.\n\nThe variable m_covered[[b]] is TRUE if the slope \\(m\\) fell in the\nnaive CI \\((m_-, m_+)\\) defined by Eq. (2) in the\nb-th replica of the simulation. Hence, the actual coverage fraction of\nthe CI is given by:\n\n\nmean(m_covered)  # Actual coverage of naive \"95%\" CIs.\n\n[1] 0.9172\n\n92%! If this difference from the nominal 95% coverage guarantee does not\nstrike you as enormous, think about it in these terms: the naive CIs\n(2) fail to cover the true parameter about 8% of the\ntimes; This is a relative +60% of failures with respect to an honest 95%\nCI.\nWhat’s going on\nWe can understand a bit better what’s happening here by decomposing the\ncoverage probability as follows:\n\\[\n\\text {Pr}(m \\in \\text{CI})  = \\text {Pr}(m \\in \\text{CI}_{q \\text{ dropped}}\\,\\vert\\,q \\text{ dropped})\\cdot \\text {Pr}(q \\text{ dropped}) +\\\\ +\\text {Pr}(m \\in \\text{CI}_{q  \\text{ kept}}\\,\\vert\\,q \\text{ kept})\\cdot \\text {Pr}(q \\text{ kept})\n\\tag{3}\n\\]\nThe right hand side of this equation shows how our selective modeling\nprocedure alters the probability \\(\\text{Pr}(m\\in \\text{CI})\\). There are\ntwo contributing factors here: the probability of dropping the intercept\nterm, and the covering probabilities of the CIs constructed in the two\ncases (\\(\\text{CI}_{q \\text{ dropped}}\\) and\n\\(\\text{CI}_{q \\text{ kept}}\\)). We can estimate all these\nprobabilities as:\n\n\nmean(q_dropped)  # Pr(q dropped)\n\n[1] 0.6782\n\nmean(m_covered[q_dropped])  # Pr(m covered | q dropped)\n\n[1] 0.9510469\n\nmean(m_covered[!q_dropped])  # Pr(m covered | q kept)\n\n[1] 0.845867\n\nThe first result directly follows from our procedure, which uses a\nhypothesis test with significance \\(\\alpha = 32\\%\\) to test the (true)\nnull hypothesis \\(q = 0\\). It is a bit harder but in fact possible to\nprove that4\n\\(\\text {Pr}(m \\in \\text{CI}_{q \\text{ dropped}}\\,\\vert\\,q \\text{ dropped}) = 95\\%\\),\nas the second estimate would seem to suggest. The third result is\nfinally what invalidates the naive coverage guarantee in Eq.\n(2).\nConcluding Remarks\nTo summarize:\nWe started with two linear models for \\(Y\\) vs. \\(X\\), which were in fact both well-specified (that is, correct).\nWe stipulated to choose one of the two models by testing the null hypothesis \\(q = 0\\).\nAfter selection, we constructed \\(95\\%\\) confidence intervals for the slope\n\\(\\hat m\\) using the selected model, as if this had been fixed in advance.\nA simulation shows that such intervals have a true coverage probability of\n\\(\\approx 92\\%\\).\nThe mathematical explanation of the last result is provided by Eq. (3), while the (hopefully) plain English one in the introductory part of this post. I will conclude with a few parenthetical remarks.\nFirst, the selective procedure proposed here would likely hardly be applied in practice in such a simple situation5. However, one could easily think of a more complex scenario with multiple covariates, where eliminating redundant ones could turn out to be beneficial for interpretation (if not compulsory, if the number of covariates exceeds the sample size).\nSecond, in order to avoid cluttering the discussion with too much\ntechnicalities, I have deliberately chosen a quite special point in true-model space (\\(q = 0\\)). This implies that both fits with and without intercept estimate the same slope \\(m\\); this is a peculiar property of \\(q = 0\\), which would not be true in the general case \\(q \\in \\mathbb R\\). In general, we would have to carefully define the inferential targets for the \\(q=0\\) and \\(q \\in \\mathbb R\\) cases, in a differential manner.\nConclusion\nThat was all for today. In the next post, I will discuss some mathematical details\nregarding the formulation of the Selective Inference problem in model-building.\nFor those surviving down to the bottom of the funnel, my future plan is to\nreview some (valid) selective inference methods I found interesting, including:\nBenjamini-Yekutieli control of False Coverage Rate (Benjamini and Yekutieli 2005),\nPOSI bounds for marginal coverage (Berk et al. 2013),\nData Fission, an elegant generalization of good old data splitting (Leiner et al. 2021).\n…whatever cool stuff I may discover in the meantime.\nCiao!\n\n\n\nBenjamini, Yoav, and Daniel Yekutieli. 2005. “False Discovery Rate–Adjusted Multiple Confidence Intervals for Selected Parameters.” Journal of the American Statistical Association 100 (469): 71–81.\n\n\nBerk, Richard, Lawrence Brown, Andreas Buja, Kai Zhang, and Linda Zhao. 2013. “Valid Post-Selection Inference.” The Annals of Statistics, 802–37.\n\n\nIsidori, Gino, Davide Lancierini, Patrick Owen, and Nicola Serra. 2021. “On the Significance of New Physics in b→ Sℓ+ ℓ- Decays.” Physics Letters B 822: 136644.\n\n\nLeiner, James, Boyan Duan, Larry Wasserman, and Aaditya Ramdas. 2021. “Data Fission: Splitting a Single Data Point.” arXiv. https://doi.org/10.48550/ARXIV.2112.11079.\n\n\nShalizi, C. R. 2020. “Post-Model-Selection Inference.” 2020. http://bactra.org/notebooks/post-model-selection-inference.html.\n\n\nVrbik, Jan. 2020. “Regression Analysis (Lecture Notes).” 2020. http://spartan.ac.brocku.ca/~jvrbik/MATH3P82/notes.pdf.\n\n\nHere, in the “extended family of models”, I’m also implicitly\naccounting for the multiplicity introduced by continuous model\nparameters and training parameters (also known as hyper-parameters).↩︎\nThe preferential method according to (Shalizi 2020), from which\nI borrowed the “a-theoretical” description, and which I recommend as a starting point for literature review.↩︎\nThis is not to say that correctly accounting for Selective Inference is\nthe default in scientific practice. A relevant example from the field I come\nfrom (Particle Physics), is documented in this stimulating reference:\n(Isidori et al. 2021).↩︎\nI’m always amazed by the great deal of theory one can learn by\nrunning a dumb simulation, and trying to explain a posteriori what\nseems to be a too perfect result. Technically, this follows from the fact that the slope estimate\n\\(\\hat m\\) and residual sum of squares \\(\\text{RSS}\\) of the reduced\nmodel, and the \\(F\\)-statistic used to test \\(q = 0\\), are all\nindependent random variables under the same null hypothesis, here\ntrue by construction. All these facts are in turn consequences of\ngeneral theorems from linear model theory, see for example\n(Vrbik 2020, chap. 4)… and, to be sure, it took me more than a single\nnight without sleep to figure all this out.↩︎\nAnd I’m actually not sure that, after properly taking into account Selective Inference, it would lead to a substantial gain in estimation accuracy, compared to simply fitting the possibly redundant model with intercept.↩︎\n",
    "preview": "posts/2022-10-18-posi/posi_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2023-05-22T01:26:47+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-11-13-kgrams-v012-released/",
    "title": "kgrams v0.1.2 on CRAN",
    "description": "kgrams: Classical k-gram Language Models in R.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2021-11-13",
    "categories": [
      "Natural Language Processing",
      "R"
    ],
    "contents": "\nSummary\nVersion v0.1.2 of my R package kgrams was just accepted by CRAN. This package provides tools for training and evaluating k-gram language models in R, supporting several probability smoothing techniques, perplexity computations, random text generation and more.\nShort demo\n\n\nlibrary(kgrams)\n# Get k-gram frequency counts from Shakespeare's \"Much Ado About Nothing\"\nfreqs <- kgram_freqs(kgrams::much_ado, N = 4)\n\n# Build modified Kneser-Ney 4-gram model, with discount parameters D1, D2, D3.\nmkn <- language_model(freqs, smoother = \"mkn\", D1 = 0.25, D2 = 0.5, D3 = 0.75)\n\n# Sample sentences from the language model at different temperatures\nset.seed(840)\nsample_sentences(model = mkn, n = 3, max_length = 10, t = 1)\n\n[1] \"i have studied eight or nine truly by your office [...] (truncated output)\"\n[2] \"ere you go : <EOS>\"                                                        \n[3] \"don pedro welcome signior : <EOS>\"                                         \n\nsample_sentences(model = mkn, n = 3, max_length = 10, t = 0.1)\n\n[1] \"i will not be sworn but love may transform me [...] (truncated output)\" \n[2] \"i will not fail . <EOS>\"                                                \n[3] \"i will go to benedick and counsel him to fight [...] (truncated output)\"\n\nsample_sentences(model = mkn, n = 3, max_length = 10, t = 10)\n\n[1] \"july cham's incite start ancientry effect torture tore pains endings [...] (truncated output)\"   \n[2] \"lastly gallants happiness publish margaret what by spots commodity wake [...] (truncated output)\"\n[3] \"born all's 'fool' nest praise hurt messina build afar dancing [...] (truncated output)\"          \n\nNEWS\nOverall Software Improvements\nThe package’s test suite has been greatly extended.\nImproved error/warning conditions for wrong arguments.\nRe-enabled compiler diagnostics as per CRAN policy (#19)\nAPI Changes\nverbose arguments now default to FALSE.\nprobability(), perplexity() and sample_sentences() are restricted to\naccept only language_model class objects as their model argument.\nNew features\nas_dictionary(NULL) now returns an empty dictionary.\nBug Fixes\nFixed bug causing .preprocess and .tknz_sent arguments to be ignored in process_sentences().\nFixed previously wrong defaults for max_lines and batch_size arguments in kgram_freqs.connection().\nAdded print method for class dictionary.\nFixed bug causing invalid results in dictionary() with batch processing and\nnon-trivial size constraints on vocabulary size.\nOther\nMaintainer’s email updated\n\n\n\n",
    "preview": {},
    "last_modified": "2023-05-22T01:26:47+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-25-r-client-for-r-universe-apis/",
    "title": "R Client for R-universe APIs",
    "description": "Introducing W.I.P. {runiv}, an R package to interact with R-universe \nrepository APIs",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2021-07-25",
    "categories": [
      "R"
    ],
    "contents": "\nIntroduction\nFollowing my previous post on how to use your R-universe API to automatically generate a list of the packages on your R-universe, I started working on a simple R client to interact with such APIs.\nFor those who missed it, R-universe is a new project from rOpenSci that allows you to mantain a personal CRAN-like repository, which automatically syncs with the GitHub repositories hosting your projects.\nAmong other features, each repository has associated a RESTful API with which users can interact for managing and retrieving informations about packages in the repo. Quoting R-universe:\n\nThe package server provides REST APIs for managing package submissions and querying information about individual packages as well as on the repository level. These data can be accessed programmatically or displayed in a front-end dashboard.\n\n{runiv}\nSince this has already proved to be useful to me (and could hopefully be so also to others), I started playing around to implement an R client for R-universe APIs. The package is called runiv and the code is here. Up to now, only a small subset of the full API features are available. You can peek at the development version from GitHub, using:\nremotes::install_github(\"vgherard/runiv\")\nFor instance, the procedure for obtaining your packages DESCRIPTION outlined in my previous post is performed by:\n\n\ndf <- runiv::runiv_descriptions(\"vgherard\") # 'vgherard' is my R-universe name.\n\n\n\ndf is a dataframe containing all the entries of the DESCRIPTION files of my packages:\n\n\ndf[, c(\"Package\", \"Title\")]\n\n\n   Package                                             Title\n1      r2r                    R-Object to R-Object Hash Maps\n2   kgrams                  Classical k-gram Language Models\n3 scribblr                          A Notepad Inside RStudio\n4  gsample   Efficient Weighted Sampling Without Replacement\n5      sbo Text Prediction via Stupid Back-Off N-Gram Models\n6     fcci              Feldman-Cousins Confidence Intervals\n\ndf[1, \"Description\"] |> strtrim(60) |> paste(\"[...]\")\n\n\n[1] \"Implementation of hash tables (hash sets and hash maps) in R [...]\"\n\nConclusion\nI hope you find this useful. I have very little experience with web API R packages (this was another personal reason to tackle this), so that if you have any suggestion, or maybe want to collaborate on runiv, you are welcome to reach out to me through GitHub.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-05-22T01:26:46+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-21-automatically-resume-your-r-package-portfolio-using-the-r-universe-api/",
    "title": "Automatic resumes of your R-developer portfolio from your R-Universe",
    "description": "Create automatic resumes of your R packages using the R-Universe API.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2021-07-21",
    "categories": [
      "R"
    ],
    "contents": "\nHi R-bloggers 👋\nStarting from today, all posts from this blog in the R category will also appear on R-bloggers. I would like to thank Tal for aggregating my blog, and say “hi!” to all R-bloggers readers. I’m a particle physicist with a passion for R, Statistics and Machine Learning. If you want to find out something more about me, you can take a look at my website, and links therein.\nIntroduction\nR-universe is a cool initiative from rOpenSci, which allows you to create your own CRAN-like repository. The latter is synced with the GitHub repositories (main or specific branches, or releases) associated to your R packages, so that using an R-universe is a very effortless way to organize and share your personal package ecosystem.\nIf you want to setup your own R-universe, follow the instructions in this blog post. In this post, I assume that you have created your own R-universe, and show you how to retrieve metadata on your packages using the R-universe API.\nRetrieving packages descriptions from your R-universe API\nOnce you will have it set up, your R-universe will be available at the URL your-user-name.r-universe.dev. For instance, mine is vgherard.r-universe.dev. From your R-universe home page, you can access the documentation of the API. We will use the command:\nGET /stats/descriptions\n    NDJSON stream with data from package DESCRIPTION files.\nThe JSON stream can be read with jsonlite, as follows:\n\n\ncon <- url(\"https://vgherard.r-universe.dev/stats/descriptions\")\npkgs <- jsonlite::stream_in(con)\n\n\n\n Found 6 records...\n Imported 6 records. Simplifying...\n\nThe result is a dataframe with alll the entries of your packages’ DESCRIPTION file, e.g.:\n\n\npkgs[, c(\"Package\", \"Title\", \"Version\")]\n\n\n   Package                                             Title\n1      r2r                    R-Object to R-Object Hash Maps\n2   kgrams                  Classical k-gram Language Models\n3 scribblr                          A Notepad Inside RStudio\n4  gsample   Efficient Weighted Sampling Without Replacement\n5      sbo Text Prediction via Stupid Back-Off N-Gram Models\n6     fcci              Feldman-Cousins Confidence Intervals\n     Version\n1 0.1.1.9000\n2      0.1.0\n3 0.2.0.9000\n4      0.1.0\n5      0.5.0\n6      1.0.0\n\nI use this query on my personal website to automatically generate a resume of the packages available on my R-universe (this is combined with a GitHub Action scheduled workflow which periodically updates the Code section of my website). More precisely, I define an R string txt containing the Markdown code for my resume, and I inline it in R Markdown using the synthax `r `. This is the code I use on my website:\n\n\ntxt <- \"\"\nfor (i in seq_len(nrow(pkgs))) {\n  txt <- paste0(\n    txt, \n    \"### [`\", pkgs[i, \"Package\"], \"`](\", pkgs[i, \"RemoteUrl\"], \")\", \"\\n\",\n    \"[![CRAN status](https://www.r-pkg.org/badges/version/\", pkgs[i,\"Package\"],\n    \")](https://CRAN.R-project.org/package=\",pkgs[i, \"Package\"], \")\",\n    \"\\n\\n\",\n    \"*\", pkgs[i, \"Title\"], \".* \", pkgs[i, \"Description\"],\n    \"\\n\\n\"\n    )\n}\n\n\n\nand this is the output:\nr2r\n\nR-Object to R-Object Hash Maps. Implementation of hash tables (hash sets and hash maps) in R, featuring arbitrary R objects as keys, arbitrary hash and key-comparison functions, and customizable behaviour upon queries of missing keys.\nkgrams\n\nClassical k-gram Language Models. Tools for training and evaluating k-gram language models in R, supporting several probability smoothing techniques, perplexity computations, random text generation and more.\nscribblr\n\nA Notepad Inside RStudio. A project aware notepad inside RStudio, for taking quick project-related notes without distractions. RStudio addin.\ngsample\n\nEfficient Weighted Sampling Without Replacement. Sample without replacement using the Gumbel-Max trick (c.f. ).\nsbo\n\nText Prediction via Stupid Back-Off N-Gram Models. Utilities for training and evaluating text predictors based on Stupid Back-Off N-gram models (Brants et al., 2007, https://www.aclweb.org/anthology/D07-1090/).\nfcci\n\nFeldman-Cousins Confidence Intervals. Provides support for building Feldman-Cousins confidence intervals [G. J. Feldman and R. D. Cousins (1998) doi:10.1103/PhysRevD.57.3873].\n\n\n\n",
    "preview": {},
    "last_modified": "2023-05-22T01:26:46+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-06-r2r/",
    "title": "{r2r} now on CRAN",
    "description": "Introducing {r2r}, an R implementation of hash tables.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2021-07-06",
    "categories": [
      "Data Structures",
      "R"
    ],
    "contents": "\nIntroduction\nMy package {r2r} (v0.1.1) has been accepted by CRAN, and is now available for download from the public repository.\nr2r\n\n\n\n\n\nr2r provides a flexible implementation of hash tables in R, allowing for:\narbitrary R objects as keys and values,\narbitrary key comparison and hash functions,\ncustomizable behaviour (throw or return a default value) on missing key exceptions.\nInstallation\nYou can install the released version of r2r from CRAN with:\ninstall.packages(\"r2r\")\nand the development version from my R-universe repository, with:\ninstall.packages(\"r2r\", repos = \"https://vgherard.r-universe.dev\")\nUsage\n\n\nlibrary(r2r)\nm <- hashmap()\n\n# Insert and query a single key-value pair\nm[[ \"user\" ]] <- \"vgherard\"\nm[[ \"user\" ]]\n\n[1] \"vgherard\"\n\n# Insert and query multiple key-value pairs\nm[ c(1, 2, 3) ] <- c(\"one\", \"two\", \"three\")\nm[ c(1, 3) ]\n\n[[1]]\n[1] \"one\"\n\n[[2]]\n[1] \"three\"\n\n# Keys and values can be arbitrary R objects\nm[[ lm(mpg ~ wt, mtcars) ]] <- c(TRUE, FALSE, TRUE)\nm[[ lm(mpg ~ wt, mtcars) ]]\n\n[1]  TRUE FALSE  TRUE\n\nGetting help\nFor further details, including an introductory vignette illustrating the features of r2r hash maps, you can consult the r2r website. If you encounter a bug, want to suggest a feature or need further help, you can open a GitHub issue.\nComparison with hash\nCRAN package {hash} also offers an implementation of hash tables based on R environments. The two tables below offer a comparison between {r2r} and {hash} (for more details, see the benchmarks Vignette)\n\nTable 1: Features supported by {r2r} and {hash}.\nFeature\nr2r\nhash\nBasic data structure\nR environment\nR environment\nArbitrary type keys\nX\n\nArbitrary type values\nX\nX\nArbitrary hash function\nX\n\nArbitrary key comparison function\nX\n\nThrow or return default on missing keys\nX\n\nHash table inversion\n\nX\n\n\nTable 2: Performances of {r2r} and {hash} for basic hash table operations.\nTask\nComparison\nKey insertion\n{r2r} ~ {hash}\nKey query\n{r2r} < {hash}\nKey deletion\n{r2r} << {hash}\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-05-22T01:26:46+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-06-test-post/",
    "title": "Test post",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "vgherard",
        "url": {}
      }
    ],
    "date": "2021-07-06",
    "categories": [
      "Other"
    ],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2023-05-22T01:26:46+00:00",
    "input_file": {}
  }
]
