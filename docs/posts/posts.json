[
  {
    "path": "posts/2022-11-07-posi-2/",
    "title": "How to get away with selection. Part II: Mathematical Framework",
    "description": "Mathematicals details on Selective Inference, model misspecification and coverage guarantees.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2022-11-25",
    "categories": [
      "Statistics"
    ],
    "contents": "\nIntroduction\nIn a previous post I introduced the problem of\nSelective Inference and illustrated, in a simplified setting, how selection\ngenerally affects the coverage of confidence intervals - when they are both\nselected and constructed using the same data. While the example was\n(hopefully) helpful to build some intuition, in order to discuss\n“How to get away with selection” in a comprehensive manner we need to make a\nfew clarifications. In particular, we need to answer the following questions:\nWhat is the target of our Selective Inference?\nWhat statistical properties would we like our inferences to have?\nSearching through the literature, I realized there exist a bunch of variations\non these two themes, which give rise to different mathematical formalisms.\nSpecifying these points is mandatory for any further discussion, so my main goal\nhere is to present these different points of view and explain some of their\npros and cons.\nMathematical Framework\nRegression and parameter estimation\nIn order to avoid getting carried away with too much abstraction, I will focus\non a specific type of problem, that is parameter estimation in regression. As\nfar as I can tell, this represents no serious loss in generality, and most of\nthe notions I’m going to outline would carry over to more general problems in a\nstraightforward manner.\nBroadly speaking, the goal of regression is to understand the dependence of a\nset of random variables \\(Y\\) from another set of random variables \\(X\\). More precisely,\nwe’re interested in the conditional probability distribution of \\(Y\\),\nconditioned on the observation of \\(X\\), which can always be represented as:\n\\[\nY = f(X)+\\varepsilon,\\qquad \\mathbb E(\\varepsilon|X)\\equiv 0.\n\\tag{1}\n\\]\nwhere \\(f(X) = \\mathbb E(Y|X)\\) is the conditional mean of \\(Y|X\\), and \\(\\varepsilon\\)\nis a random variable with vanishing conditional mean, sometimes called the “error term”.\nParameter estimation means that we have (somehow) chosen functional forms for\nthe conditional mean and for the probability distribution of the error term,\nand we want to provide estimates for the parameters defining these two functions.\nEnter selection\nNow, in many applications we actually don’t have much insight about the correct\nfunctional form \\(f\\), nor of the distribution of the error term \\(\\varepsilon\\).\nGiven a dataset of experimental observations of \\(Y\\) and \\(X\\), we are thus faced\nwith two tasks:\nSelection. Choose an adequate model \\(\\hat M = (\\hat f,\\,\\hat \\varepsilon)\\)\nfor the true \\(f\\) and \\(\\varepsilon\\), usually from a (more or less) pre-specified\nfamily of initial guesses \\(\\mathcal M =\\{(f_i,\\varepsilon_i)\\}_i\\), using a\n(more or less) pre-specified criterion.\nPost-Selection Inference. Perform inference with the chosen model. In the\nstudy case we’re considering, this amounts to provide confidence intervals for\nmodel parameters.\nIt is, of course, the need to use the same data for both tasks which gives rise\nto complications.\nInferential target\nWe now come to the first question raised in the Introduction, regarding the\nnature of the inferential target. And now more concretely:\nwhat are the true values of the parameters we’re trying to estimate?\nOne can appreciate that the answer necessarily depends on how we consider the\nfinal output of the modeling procedure:\n(Model Trusting) As the true data generating process, or\n(Assumption Lean) As an approximation of the (partially or totally\nunknown) data generating process, chosen in a data-driven fashion within\na family of initial guesses \\(\\mathcal M\\).\nAccording to the first interpretation, there’s no room for ambiguity: the\ntargets of our estimates should clearly be the true parameter values,\nwhose definition does not depend on any modeling choice. The second\ninterpretation, on the other hand, leaves a certain amount of\nfreedom in this respect. Here, I will follow the point of view advocated by\n(Berk et al. 2013), according to which the target parameters\nare those providing the best approximation1 to the true data generating\nprocess, according to the functional form chosen in the selection stage.\nI believe both positions have their merits and flaws, and which one is more\nappropriate largely depends on context. In a reductionist field like\nHigh Energy Physics, whose eventual goal is to explain the fundamental laws of\nNature, the Model Trusting point of view is usually taken,\nand with good reason. When studying more emergent phenomena, on the other hand,\nthe quest for fundamental laws is often meaningless (or at best wishful\nthinking), and the Assumption Lean standpoint looks more reasonable. In any\ncase, here the differences are not merely philosophical ones, as the two\ninterpretations give rise to different mathematical formalisms.\nIn the following posts, I will be mostly focusing on the Assumption Lean\npoint of view. In my opinion, this has two big advantages2:\nConceptual: Inferences have a well-defined meaning even when the model is\nmisspecified3 - which, apart from quite particular cases (see above),\naccounts for the great majority of cases encountered by data analysts in\nthe practice.\nMathematical: It allows to reduce the problem of selective inference to\nthat of simultaneous inference (more on this below).\nFor the latter type of problems, the theory of\nmultiple testing\nreadily provides at least conservative bounds.\nNotions of coverage\nIn addition to the conceptual distinction about the interpretation of the\nselected model, there is also a technical distinction regarding the type\nof coverage guarantees that selective confidence intervals should be endowed\nwith (this is the concrete version of the second question posed in the\nIntroduction).\nHere are some of the notions of coverage I’ve come across:\nMarginal coverage over the selected parameters. We bound at level \\(\\alpha\\)\nthe probability that our procedure constructs any non-covering confidence\ninterval for model parameters \\(\\beta_i\\). Denote by \\(\\widehat M\\) the selected model\nand, with abuse of notation, the corresponding set of selected parameters.\nIf \\(\\widehat{\\text{CI}}_i\\) are the confidence intervals for parameters \\(\\beta _i\\), we\nrequire:\n\\[\n\\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\in \\widehat M) \\geq 1-\\alpha\n\\tag{2}\n\\]\nConditional coverage over the selected parameters. We bound at level\n\\(\\alpha\\) the conditional\nprobability of constructing a non-covering confidence interval, conditioned on\nthe outcome of selection \\(\\widehat M\\). If \\(m\\) is the selected model, we require:\n\\[\n\\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\in m|\\,\\widehat M=m) \\geq 1-\\alpha\n\\tag{3}\n\\]\nFalse Coverage Rate. We bound at level \\(q\\)4 the expected fraction of\nnon-covering confidence intervals out of all intervals constructed:\n\\[\n\\mathbb E \\left( \\dfrac{|i \\in \\widehat M \\colon \\ \\beta_i \\in \\widehat{\\text{CI}}_i|}{|\\widehat M|} \\right)\n\\geq1-q\n\\tag{4}\n\\]\nwhere \\(|S|\\) denotes the cardinality of a set \\(S\\).\nNotice that the random variables in the previous equations are \\(\\widehat M\\) and\n\\(\\widehat{\\text{CI}}_i\\) (denoted by a hat), whereas the true coefficients \\(\\beta_i\\)\nand the selected set \\(m\\) in the case of conditional coverage\n(Eq. (3)) are fixed quantities.\nVariations of these measures focusing on single coefficients are also possible.\nIn practice, in the Assumption Lean framework I just introduced,\nall these coverage measures would not be computed\nunder the selected model’s probability distribution, but rather under a\npre-fixed, more general model for the true probability distribution of \\(Y\\)\nconditional on \\(X\\). We may, for instance, assume that the true error term\n\\(\\varepsilon\\) in Eq. (1) is gaussian with constant\n(\\(X\\)-independent) variance, without making any further assumption on \\(f(X)\\).\nWith enough data, we may even be able to bypass any assumption at all, and\ncompute all relevant quantiles using a bootstrap (Kuchibhotla et al. 2020).\nIn the Model Trusting framework, on the other hand, the conditional coverage\nmeasure would be computed under the selected model… and I’m honestly not\nsure whether it’s possible to make sense of the other two measures in this\nframework.\nSelective vs. Simultaneous Inference\nThe connection between selective and simultaneous inference can now be\nunderstood, through the notion of marginal coverage. In fact, suppose that we\nwere able to provide simultaneous coverage for all parameters\n(selected or not):\n\\[\n\\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i) \\geq 1-\\alpha\n\\tag{5}\n\\]\nThen, it’s easy to see that the same confidence interval would also provide\nmarginal coverage over the selected parameters. In order to see that, simply\nobserve that the simultaneous coverage event can be decomposed as:\n\\[\n(\\beta _i \\in \\widehat {\\text{CI}}_i\\,\\,\\forall i) = (\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\in \\widehat M) \\cap (\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\notin \\widehat M)\n\\]\nwhich implies that:\n\\[\n\\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\in \\widehat M) \\geq \\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i) \\geq 1-\\alpha,\n\\tag{6}\n\\]\nthat is simultaneous coverage implies marginal coverage over the\nselected parameters. In fact, with a few more set-theory manipulations,\none can arrive to a powerful Lemma (see Kuchibhotla et al. 2020 for details):\ncontrolling the marginal coverage (2) at level \\(\\alpha\\)\nfor any model selection procedure5 is equivalent to controlling\nsimultaneous coverage for all possible model selections.\nThis provides us a first, very simple recipe for selective inference, which can\nbe applied whenever one is able to construct confidence intervals for parameters\nin the absence of selection: use any procedure (e.g. \nBonferroni corrections)\nwhich controls simultaneous coverage for all parameters we may select a priori.\nConclusions\nThis was a long and somewhat abstract post, so perhaps the best way to conclude\nis with some bottom lines:\nWhen performing model-based inference, nothing forces us to make working\nhypotheses about the correctness of the model we arrive at. Not making such\nassumptions corresponds to what I called an Assumption Lean framework.\nIn an Assumption Lean framework, the inferential targets are, in general,\nthe best approximations to the truth allowed by the selected model.\nThere exist many type of coverage guarantees for selective confidence\nintervals.\nBounding the probability of any false coverage statement\n(“marginal coverage over the selected parameters”) allows to turn a problem of\nselective inference into one of simultaneous inference.\nIn particular, it is worth to mention that the last observation lead us to a\nsimple recipe for constructing (somewhat conservative, but valid) selective\nconfidence intervals with marginal coverage. In the posts which follow,\nI will discuss some more advanced methods which produce confidence intervals\nsatisfying the requirements discussed here.\n\n\n\nBenjamini, Yoav, and Daniel Yekutieli. 2005. “False Discovery Rate–Adjusted Multiple Confidence Intervals for Selected Parameters.” Journal of the American Statistical Association 100 (469): 71–81.\n\n\nBerk, Richard, Lawrence Brown, Andreas Buja, Kai Zhang, and Linda Zhao. 2013. “Valid Post-Selection Inference.” The Annals of Statistics, 802–37.\n\n\nKuchibhotla, Arun K, Lawrence D Brown, Andreas Buja, Junhui Cai, Edward I George, and Linda H Zhao. 2020. “Valid Post-Selection Inference in Model-Free Linear Regression.” The Annals of Statistics 48 (5): 2953–81.\n\n\nWhere what’s to be considered best is defined in terms of some\nreasonable metric. For instance, for the conditional mean \\(f(X)\\) of a continuous\nresponse \\(Y\\), a convenient target \\(f^*(X)\\) within a prescribed family of\nfunctions \\(\\mathcal F\\) can be defined by\n\\(f^* =\\arg\\min _{\\phi \\in \\mathcal F} \\mathbb E (\\vert f(X) - \\phi (X)\\vert^2)\\).↩︎\nThere’s also a third advantage, which is that I find much harder to think\nabout selective inference from the Model Trusting point of view, hence to write\nblog posts about it - but that’s likely a limitation of my imagination, rather\nthan of the point of view itself.↩︎\nA cool word for “wrong”.↩︎\nWhy \\(q\\) and not \\(\\alpha\\)? Ask (Benjamini and Yekutieli 2005).↩︎\nIt is assumed that the selection is performed from a from a fixed family\nof models \\(\\mathcal M\\).↩︎\n",
    "preview": {},
    "last_modified": "2022-11-25T17:51:24+01:00",
    "input_file": "posi-2.knit.md"
  },
  {
    "path": "posts/2022-10-18-posi/",
    "title": "How to get away with selection. Part I: Introduction",
    "description": "Introducing the problem of Selective Inference, illustrated through a simple simulation in R.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2022-11-14",
    "categories": [
      "R",
      "Statistics"
    ],
    "contents": "\nPrologue\nA few months back, for undocumented circumstances, my browser’s search\nhistory was full of terms like “parameter estimation with variable selection”,\nor “confidence intervals after cross-validation”, or again\n“linear model uncertainties after staring into the abyss”, …\nSparing you my rock bottom, I eventually stumbled upon the right keywords, and\nstarted digging into the mathematical aspects of Selective Inference, or\nPost-Model Selection Inference. Now, while my hands\nare still full of dirt, I’ve decided it’s the right moment to write some\nnotes about what I’ve learned - whose main recipient is the future me,\nwhich will otherwise inevitably forget what the present me thinks he\nknows. If you’re not the future me:\nWelcome 👋\nIf you have detected some imprecision, or have suggestions for this or the\nnext posts, you are more than welcome to create an issue on the source\nrepository of this blog.\nIntroduction\nBroadly speaking, the problem of Selective Inference is that of\nperforming valid statistical inferences when the actual questions of the data analysis are not fixed in advance, but rather selected through data examination. In model-based inference, this lack of\npre-determination usually stems from the (often unavoidable) practice of\nusing the same data to choose an adequate model for the data generating\nprocess and to perform inference. The intrinsic\nrandomness of the selection process has important consequences on the\nprobability of making different guesses about the selected questions,\nwhich, if not properly taken into account, can completely invalidate the\nanalysis results.\nIf this sounds unfamiliar, think about machine-learning: when training a\npredictive model on a given dataset, you would usually consider the\nerror on the same dataset as a poor (optimistic) estimate of the true\nmodel’s error rate, because the model was tuned to perform well on that\ndata in the first place. There we go, Selective Inference! A selection\nfrom an extended family of models1 is performed through data examination,\nand this event introduces a bias in the error estimate of the final\nmodel from training data.\nThe example from machine-learning also suggests a very simple-minded and\nrelatively a-theoretical approach to Selective Inference: data-splitting2. According to this method, we would use only part of the available data to select the questions to be answered by the analysis, while the remaining part would\nbe reserved to perform the actual inference. For this program to\nsucceed, there are however two important requirements: first, we must have\nenough data to ensure decent statistics for both the selection and inference\ntasks; and second, we must be able to split data in two independent\n(or close to independent) sets. This can suppose problems with, e.g.,\ntime-series data. If, on the other hand, these requirements cannot be met, we\nhave to resort to more sophisticated methods.\nAt this point, I would like to stress that the conceptual problems\nI’ve just pointed out will probably look obvious to any reader with a\ndecent intuition for probability3. What is less obvious, but in fact\na fairly active research field in statistics,\nis how to perform valid selective inferences when the “easy” solution of\ndata-splitting I mentioned above is not available. This is where theory\nre-enters the game, and what I’m going to ramble about in this and the next\nposts.\nIllustrations of Selective Inference\nEnough for the speech, let us see how selection can affect (and invalidate)\nclassical inference with a simple-minded simulation.\nSetting\nTo illustrate why naive classical inference can fail in the presence of\nselection, we consider a very simple regression\nproblem involving a single regressor \\(X\\) and a response \\(Y\\), where all the assumptions of the classical linear model hold. In fact, we will assume the true data generating process to be:\n\\[\nY = mX + \\varepsilon, \\qquad \\varepsilon \\sim \\mathcal N (0, \\sigma),\n\\tag{1}\n\\]\nwhere \\(\\varepsilon \\sim \\mathcal N (0, \\sigma)\\) means “\\(\\varepsilon\\) follows a gaussian distribution with mean \\(0\\) and standard deviation \\(\\sigma\\)”.\nA selective modeling procedure\nNow, suppose we are given a dataset of \\(N\\) independent observations\n\\((y_i, x_i)_{i = 1, \\,2,\\, \\dots,\\,N}\\), and we would like to study the\ndependence of \\(Y\\) from \\(X\\). Of course we don’t know the true law, Eq.\n(1), but by a stroke of luck (or by a Taylor expansion\nargument) we make the correct initial guess that such dependence is\nlinear in \\(X\\). We are, however, unsure whether it would be appropriate\nto also include an intercept term in the fit. We thus establish the\nfollowing selective modeling procedure:\nFit a linear model with intercept term,\n\\(Y = mX + q + \\varepsilon\\).\nStop if the intercept estimate is significantly different from zero (say, at the level of 1-\\(\\sigma\\), \\(p\\text{-value}<32\\%\\)). Otherwise:\nFit a model with no intercept, \\(Y = mX + \\varepsilon\\).\nFinally, we use the last fitted model to construct a “naive 95%”\nconfidence interval \\((\\hat m_-, \\hat m_+)\\) for the slope \\(m\\).\nThis is defined by:\n\\[\n\\hat m_\\pm = \\hat m\\pm t_{0.975, \\,N-d} \\cdot \\hat \\sigma _\\hat m\\qquad (95\\%\\,\\text {C.L.}).\n\\tag{2}\n\\]\nHere \\(t_{0.975,\\, N-d}\\) is the 97.5%-quantile of the \\(t\\)-student\ndistribution with \\(N-d\\) degrees of freedom, and \\(d\\) is the number of\nestimated parameters, (\\(2\\) or \\(1\\), according to where we stopped in the\nmodeling procedure). \\(\\hat m\\) and \\(\\hat \\sigma _{\\hat m}\\) are the\nOrdinary Least Squares (OLS) estimates of the slope and its standard\ndeviation, respectively. These are the classical confidence intervals\nreported by the lm() function in R.\nAt a first glance, this procedure might look reasonable. After all, both\nintervals we may end up constructing do have a genuine 95% coverage probability,\nwhen constructed unconditionally… and by selecting the “best” model we’re\nsupposedly choosing the “best” confidence interval. In spite of this qualitative\nargument, we inquire:\n… does it work?\nNow, the question is: how often do the naive CIs (2)\ncover the true parameter \\(m\\) of Eq. (1)? The answer\nbetter be “at least 95% of the times” for our confidence claim in Eq.\n(2) to be valid!\nWe can check the actual coverage of (2) through a simulation.\nHere I’ll assume \\(m = \\sigma = 1\\), and that the\ndataset consists of \\(N=10\\) independent observations of \\(Y\\) at fixed points\n\\(X = (1, \\,2, \\,\\dots ,\\, 10)\\).\n\n\nm <- sigma <- 1  # True parameters\nx <- 1:10  # x covariate, assumed fixed\n\n\nThe following function generates observations of \\(Y\\) according to the distribution (1):\n\n\ngenerate_y <- function(x, m, sigma) {\n  eps <- rnorm(length(x), mean = 0, sd = sigma)\n  return(m * x + eps)\n  }\n\n\nFor example:\n\n\nset.seed(840)\nplot(x, generate_y(x, m, sigma), xlab = \"X\", ylab = \"Y\")\n\n\n\nBelow we generate \\(B=10^4\\) such \\((X,Y)\\) datasets, for each of which we fit a linear model according to the procedure specified above, and check how many\ntimes the true slope \\(m = 1\\) falls in the confidence interval defined by Eq. (2).\n\n\n# Simulation parameters\nB <- 1e4  # Number of replications\n\n# Preallocate logical vectors to be assigned for each replica - for efficiency. \nq_dropped <- logical(B)  # Was the intercept term 'q' dropped? \nm_covered <- logical(B)  # Was the true parameter 'm' covered?\n\n# Set seed for reproducibility\nset.seed(841)\n\n# Logging\ntime_start <- Sys.time()  \n\n# Start the simulation\nfor (b in 1:B) {\n  y <- generate_y(x, m, sigma)\n  \n  # Fit full model (including intercept 'q')\n  fit <- lm(y ~ x + 1)  \n  q_pval <- summary(fit)$coefficients[1, 4]\n  \n  # Is 'q' term \"significant\"? If not, drop 'q' and fit a simpler model\n  if (q_pval > 0.32)  { \n    q_dropped[[b]] <- TRUE\n    fit <- lm(y ~ x - 1) \n  } else {\n    q_dropped[[b]] <- FALSE\n  }\n  \n  # Construct CI for 'm',  using the selected model's fit\n  m_ci <- confint(fit, 'x', level = 0.95)\n  m_covered[[b]] <- m_ci[[1]] < m && m < m_ci[[2]]\n}\n\ntime_end <- Sys.time()\ncat(\"Done :) Took \", as.numeric(time_end - time_start), \" seconds.\")\n\nDone :) Took  12.12071  seconds.\n\nThe variable m_covered[[b]] is TRUE if the slope \\(m\\) fell in the\nnaive CI \\((m_-, m_+)\\) defined by Eq. (2) in the\nb-th replica of the simulation. Hence, the actual coverage fraction of\nthe CI is given by:\n\n\nmean(m_covered)  # Actual coverage of naive \"95%\" CIs.\n\n[1] 0.9172\n\n92%! If this difference from the nominal 95% coverage guarantee does not\nstrike you as enormous, think about it in these terms: the naive CIs\n(2) fail to cover the true parameter about 8% of the\ntimes; This is a relative +60% of failures with respect to an honest 95%\nCI.\nWhat’s going on\nWe can understand a bit better what’s happening here by decomposing the\ncoverage probability as follows:\n\\[\n\\text {Pr}(m \\in \\text{CI})  = \\text {Pr}(m \\in \\text{CI}_{q \\text{ dropped}}\\,\\vert\\,q \\text{ dropped})\\cdot \\text {Pr}(q \\text{ dropped}) +\\\\ +\\text {Pr}(m \\in \\text{CI}_{q  \\text{ kept}}\\,\\vert\\,q \\text{ kept})\\cdot \\text {Pr}(q \\text{ kept})\n\\tag{3}\n\\]\nThe right hand side of this equation shows how our selective modeling\nprocedure alters the probability \\(\\text{Pr}(m\\in \\text{CI})\\). There are\ntwo contributing factors here: the probability of dropping the intercept\nterm, and the covering probabilities of the CIs constructed in the two\ncases (\\(\\text{CI}_{q \\text{ dropped}}\\) and\n\\(\\text{CI}_{q \\text{ kept}}\\)). We can estimate all these\nprobabilities as:\n\n\nmean(q_dropped)  # Pr(q dropped)\n\n[1] 0.6782\n\nmean(m_covered[q_dropped])  # Pr(m covered | q dropped)\n\n[1] 0.9510469\n\nmean(m_covered[!q_dropped])  # Pr(m covered | q kept)\n\n[1] 0.845867\n\nThe first result directly follows from our procedure, which uses a\nhypothesis test with significance \\(\\alpha = 32\\%\\) to test the (true)\nnull hypothesis \\(q = 0\\). It is a bit harder but in fact possible to\nprove that4\n\\(\\text {Pr}(m \\in \\text{CI}_{q \\text{ dropped}}\\,\\vert\\,q \\text{ dropped}) = 95\\%\\),\nas the second estimate would seem to suggest. The third result is\nfinally what invalidates the naive coverage guarantee in Eq.\n(2).\nConcluding Remarks\nTo summarize:\nWe started with two linear models for \\(Y\\) vs. \\(X\\), which were in fact both well-specified (that is, correct).\nWe stipulated to choose one of the two models by testing the null hypothesis \\(q = 0\\).\nAfter selection, we constructed \\(95\\%\\) confidence intervals for the slope\n\\(\\hat m\\) using the selected model, as if this had been fixed in advance.\nA simulation shows that such intervals have a true coverage probability of\n\\(\\approx 92\\%\\).\nThe mathematical explanation of the last result is provided by Eq. (3), while the (hopefully) plain English one in the introductory part of this post. I will conclude with a few parenthetical remarks.\nFirst, the selective procedure proposed here would likely hardly be applied in practice in such a simple situation5. However, one could easily think of a more complex scenario with multiple covariates, where eliminating redundant ones could turn out to be beneficial for interpretation (if not compulsory, if the number of covariates exceeds the sample size).\nSecond, in order to avoid cluttering the discussion with too much\ntechnicalities, I have deliberately chosen a quite special point in true-model space (\\(q = 0\\)). This implies that both fits with and without intercept estimate the same slope \\(m\\); this is a peculiar property of \\(q = 0\\), which would not be true in the general case \\(q \\in \\mathbb R\\). In general, we would have to carefully define the inferential targets for the \\(q=0\\) and \\(q \\in \\mathbb R\\) cases, in a differential manner.\nConclusion\nThat was all for today. In the next post, I will discuss some mathematical details\nregarding the formulation of the Selective Inference problem in model-building.\nFor those surviving down to the bottom of the funnel, my future plan is to\nreview some (valid) selective inference methods I found interesting, including:\nBenjamini-Yekutieli control of False Coverage Rate (Benjamini and Yekutieli 2005),\nPOSI bounds for marginal coverage (Berk et al. 2013),\nData Fission, an elegant generalization of good old data splitting (Leiner et al. 2021).\n…whatever cool stuff I may discover in the meantime.\nCiao!\n\n\n\nBenjamini, Yoav, and Daniel Yekutieli. 2005. “False Discovery Rate–Adjusted Multiple Confidence Intervals for Selected Parameters.” Journal of the American Statistical Association 100 (469): 71–81.\n\n\nBerk, Richard, Lawrence Brown, Andreas Buja, Kai Zhang, and Linda Zhao. 2013. “Valid Post-Selection Inference.” The Annals of Statistics, 802–37.\n\n\nIsidori, Gino, Davide Lancierini, Patrick Owen, and Nicola Serra. 2021. “On the Significance of New Physics in b→ Sℓ+ ℓ- Decays.” Physics Letters B 822: 136644.\n\n\nLeiner, James, Boyan Duan, Larry Wasserman, and Aaditya Ramdas. 2021. “Data Fission: Splitting a Single Data Point.” arXiv. https://doi.org/10.48550/ARXIV.2112.11079.\n\n\nShalizi, C. R. 2020. “Post-Model-Selection Inference.” 2020. http://bactra.org/notebooks/post-model-selection-inference.html.\n\n\nVrbik, Jan. 2020. “Regression Analysis (Lecture Notes).” 2020. http://spartan.ac.brocku.ca/~jvrbik/MATH3P82/notes.pdf.\n\n\nHere, in the “extended family of models”, I’m also implicitly\naccounting for the multiplicity introduced by continuous model\nparameters and training parameters (also known as hyper-parameters).↩︎\nThe preferential method according to (Shalizi 2020), from which\nI borrowed the “a-theoretical” description, and which I recommend as a starting point for literature review.↩︎\nThis is not to say that correctly accounting for Selective Inference is\nthe default in scientific practice. A relevant example from the field I come\nfrom (Particle Physics), is documented in this stimulating reference:\n(Isidori et al. 2021).↩︎\nI’m always amazed by the great deal of theory one can learn by\nrunning a dumb simulation, and trying to explain a posteriori what\nseems to be a too perfect result. Technically, this follows from the fact that the slope estimate\n\\(\\hat m\\) and residual sum of squares \\(\\text{RSS}\\) of the reduced\nmodel, and the \\(F\\)-statistic used to test \\(q = 0\\), are all\nindependent random variables under the same null hypothesis, here\ntrue by construction. All these facts are in turn consequences of\ngeneral theorems from linear model theory, see for example\n(Vrbik 2020, chap. 4)… and, to be sure, it took me more than a single\nnight without sleep to figure all this out.↩︎\nAnd I’m actually not sure that, after properly taking into account Selective Inference, it would lead to a substantial gain in estimation accuracy, compared to simply fitting the possibly redundant model with intercept.↩︎\n",
    "preview": "posts/2022-10-18-posi/posi_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-11-14T19:30:52+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-11-13-kgrams-v012-released/",
    "title": "kgrams v0.1.2 on CRAN",
    "description": "kgrams: Classical k-gram Language Models in R.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2021-11-13",
    "categories": [
      "R"
    ],
    "contents": "\nSummary\nVersion v0.1.2 of my R package kgrams was just accepted by CRAN. This package provides tools for training and evaluating k-gram language models in R, supporting several probability smoothing techniques, perplexity computations, random text generation and more.\nShort demo\n\n\nlibrary(kgrams)\n# Get k-gram frequency counts from Shakespeare's \"Much Ado About Nothing\"\nfreqs <- kgram_freqs(kgrams::much_ado, N = 4)\n\n# Build modified Kneser-Ney 4-gram model, with discount parameters D1, D2, D3.\nmkn <- language_model(freqs, smoother = \"mkn\", D1 = 0.25, D2 = 0.5, D3 = 0.75)\n\n# Sample sentences from the language model at different temperatures\nset.seed(840)\nsample_sentences(model = mkn, n = 3, max_length = 10, t = 1)\n\n\n[1] \"i have studied eight or nine truly by your office [...] (truncated output)\"\n[2] \"ere you go : <EOS>\"                                                        \n[3] \"don pedro welcome signior : <EOS>\"                                         \n\nsample_sentences(model = mkn, n = 3, max_length = 10, t = 0.1)\n\n\n[1] \"i will not be sworn but love may transform me [...] (truncated output)\" \n[2] \"i will not fail . <EOS>\"                                                \n[3] \"i will go to benedick and counsel him to fight [...] (truncated output)\"\n\nsample_sentences(model = mkn, n = 3, max_length = 10, t = 10)\n\n\n[1] \"july cham's incite start ancientry effect torture tore pains endings [...] (truncated output)\"   \n[2] \"lastly gallants happiness publish margaret what by spots commodity wake [...] (truncated output)\"\n[3] \"born all's 'fool' nest praise hurt messina build afar dancing [...] (truncated output)\"          \n\nNEWS\nOverall Software Improvements\nThe package’s test suite has been greatly extended.\nImproved error/warning conditions for wrong arguments.\nRe-enabled compiler diagnostics as per CRAN policy (#19)\nAPI Changes\nverbose arguments now default to FALSE.\nprobability(), perplexity() and sample_sentences() are restricted to accept only language_model class objects as their model argument.\nNew features\nas_dictionary(NULL) now returns an empty dictionary.\nBug Fixes\nFixed bug causing .preprocess and .tknz_sent arguments to be ignored in process_sentences().\nFixed previously wrong defaults for max_lines and batch_size arguments in kgram_freqs.connection().\nAdded print method for class dictionary.\nFixed bug causing invalid results in dictionary() with batch processing and non-trivial size constraints on vocabulary size.\nOther\nMaintainer’s email updated\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-17T08:21:30+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-25-r-client-for-r-universe-apis/",
    "title": "R Client for R-universe APIs",
    "description": "Introducing W.I.P. {runiv}, an R package to interact with R-universe \nrepository APIs",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2021-07-25",
    "categories": [
      "R"
    ],
    "contents": "\nIntroduction\nFollowing my previous post on how to use your R-universe API to automatically generate a list of the packages on your R-universe, I started working on a simple R client to interact with such APIs.\nFor those who missed it, R-universe is a new project from rOpenSci that allows you to mantain a personal CRAN-like repository, which automatically syncs with the GitHub repositories hosting your projects.\nAmong other features, each repository has associated a RESTful API with which users can interact for managing and retrieving informations about packages in the repo. Quoting R-universe:\n\nThe package server provides REST APIs for managing package submissions and querying information about individual packages as well as on the repository level. These data can be accessed programmatically or displayed in a front-end dashboard.\n\n{runiv}\nSince this has already proved to be useful to me (and could hopefully be so also to others), I started playing around to implement an R client for R-universe APIs. The package is called runiv and the code is here. Up to now, only a small subset of the full API features are available. You can peek at the development version from GitHub, using:\nremotes::install_github(\"vgherard/runiv\")\nFor instance, the procedure for obtaining your packages DESCRIPTION outlined in my previous post is performed by:\n\n\ndf <- runiv::runiv_descriptions(\"vgherard\") # 'vgherard' is my R-universe name.\n\n\n\ndf is a dataframe containing all the entries of the DESCRIPTION files of my packages:\n\n\ndf[, c(\"Package\", \"Title\")]\n\n\n   Package                                             Title\n1      r2r                    R-Object to R-Object Hash Maps\n2   kgrams                  Classical k-gram Language Models\n3 scribblr                          A Notepad Inside RStudio\n4  gsample   Efficient Weighted Sampling Without Replacement\n5      sbo Text Prediction via Stupid Back-Off N-Gram Models\n6     fcci              Feldman-Cousins Confidence Intervals\n\ndf[1, \"Description\"] |> strtrim(60) |> paste(\"[...]\")\n\n\n[1] \"Implementation of hash tables (hash sets and hash maps) in R [...]\"\n\nConclusion\nI hope you find this useful. I have very little experience with web API R packages (this was another personal reason to tackle this), so that if you have any suggestion, or maybe want to collaborate on runiv, you are welcome to reach out to me through GitHub.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-13T16:53:50+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-21-automatically-resume-your-r-package-portfolio-using-the-r-universe-api/",
    "title": "Automatic resumes of your R-developer portfolio from your R-Universe",
    "description": "Create automatic resumes of your R packages using the R-Universe API.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2021-07-21",
    "categories": [
      "R"
    ],
    "contents": "\nHi R-bloggers 👋\nStarting from today, all posts from this blog in the R category will also appear on R-bloggers. I would like to thank Tal for aggregating my blog, and say “hi!” to all R-bloggers readers. I’m a particle physicist with a passion for R, Statistics and Machine Learning. If you want to find out something more about me, you can take a look at my website, and links therein.\nIntroduction\nR-universe is a cool initiative from rOpenSci, which allows you to create your own CRAN-like repository. The latter is synced with the GitHub repositories (main or specific branches, or releases) associated to your R packages, so that using an R-universe is a very effortless way to organize and share your personal package ecosystem.\nIf you want to setup your own R-universe, follow the instructions in this blog post. In this post, I assume that you have created your own R-universe, and show you how to retrieve metadata on your packages using the R-universe API.\nRetrieving packages descriptions from your R-universe API\nOnce you will have it set up, your R-universe will be available at the URL your-user-name.r-universe.dev. For instance, mine is vgherard.r-universe.dev. From your R-universe home page, you can access the documentation of the API. We will use the command:\nGET /stats/descriptions\n    NDJSON stream with data from package DESCRIPTION files.\nThe JSON stream can be read with jsonlite, as follows:\n\n\ncon <- url(\"https://vgherard.r-universe.dev/stats/descriptions\")\npkgs <- jsonlite::stream_in(con)\n\n\n\n Found 6 records...\n Imported 6 records. Simplifying...\n\nThe result is a dataframe with alll the entries of your packages’ DESCRIPTION file, e.g.:\n\n\npkgs[, c(\"Package\", \"Title\", \"Version\")]\n\n\n   Package                                             Title\n1      r2r                    R-Object to R-Object Hash Maps\n2   kgrams                  Classical k-gram Language Models\n3 scribblr                          A Notepad Inside RStudio\n4  gsample   Efficient Weighted Sampling Without Replacement\n5      sbo Text Prediction via Stupid Back-Off N-Gram Models\n6     fcci              Feldman-Cousins Confidence Intervals\n     Version\n1 0.1.1.9000\n2      0.1.0\n3 0.2.0.9000\n4      0.1.0\n5      0.5.0\n6      1.0.0\n\nI use this query on my personal website to automatically generate a resume of the packages available on my R-universe (this is combined with a GitHub Action scheduled workflow which periodically updates the Code section of my website). More precisely, I define an R string txt containing the Markdown code for my resume, and I inline it in R Markdown using the synthax `r `. This is the code I use on my website:\n\n\ntxt <- \"\"\nfor (i in seq_len(nrow(pkgs))) {\n  txt <- paste0(\n    txt, \n    \"### [`\", pkgs[i, \"Package\"], \"`](\", pkgs[i, \"RemoteUrl\"], \")\", \"\\n\",\n    \"[![CRAN status](https://www.r-pkg.org/badges/version/\", pkgs[i,\"Package\"],\n    \")](https://CRAN.R-project.org/package=\",pkgs[i, \"Package\"], \")\",\n    \"\\n\\n\",\n    \"*\", pkgs[i, \"Title\"], \".* \", pkgs[i, \"Description\"],\n    \"\\n\\n\"\n    )\n}\n\n\n\nand this is the output:\nr2r\n\nR-Object to R-Object Hash Maps. Implementation of hash tables (hash sets and hash maps) in R, featuring arbitrary R objects as keys, arbitrary hash and key-comparison functions, and customizable behaviour upon queries of missing keys.\nkgrams\n\nClassical k-gram Language Models. Tools for training and evaluating k-gram language models in R, supporting several probability smoothing techniques, perplexity computations, random text generation and more.\nscribblr\n\nA Notepad Inside RStudio. A project aware notepad inside RStudio, for taking quick project-related notes without distractions. RStudio addin.\ngsample\n\nEfficient Weighted Sampling Without Replacement. Sample without replacement using the Gumbel-Max trick (c.f. ).\nsbo\n\nText Prediction via Stupid Back-Off N-Gram Models. Utilities for training and evaluating text predictors based on Stupid Back-Off N-gram models (Brants et al., 2007, https://www.aclweb.org/anthology/D07-1090/).\nfcci\n\nFeldman-Cousins Confidence Intervals. Provides support for building Feldman-Cousins confidence intervals [G. J. Feldman and R. D. Cousins (1998) doi:10.1103/PhysRevD.57.3873].\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-13T16:53:50+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-06-r2r/",
    "title": "{r2r} now on CRAN",
    "description": "Introducing {r2r}, an R implementation of hash tables.",
    "author": [
      {
        "name": "vgherard",
        "url": {}
      }
    ],
    "date": "2021-07-06",
    "categories": [
      "R"
    ],
    "contents": "\nIntroduction\nMy package {r2r} (v0.1.1) has been accepted by CRAN, and is now available for download from the public repository.\nr2r\n\n   \nr2r provides a flexible implementation of hash tables in R, allowing for:\narbitrary R objects as keys and values,\narbitrary key comparison and hash functions,\ncustomizable behaviour (throw or return a default value) on missing key exceptions.\nInstallation\nYou can install the released version of r2r from CRAN with:\ninstall.packages(\"r2r\")\nand the development version from my R-universe repository, with:\ninstall.packages(\"r2r\", repos = \"https://vgherard.r-universe.dev\")\nUsage\n\n\nlibrary(r2r)\nm <- hashmap()\n\n# Insert and query a single key-value pair\nm[[ \"user\" ]] <- \"vgherard\"\nm[[ \"user\" ]]\n\n\n[1] \"vgherard\"\n\n# Insert and query multiple key-value pairs\nm[ c(1, 2, 3) ] <- c(\"one\", \"two\", \"three\")\nm[ c(1, 3) ]\n\n\n[[1]]\n[1] \"one\"\n\n[[2]]\n[1] \"three\"\n\n# Keys and values can be arbitrary R objects\nm[[ lm(mpg ~ wt, mtcars) ]] <- c(TRUE, FALSE, TRUE)\nm[[ lm(mpg ~ wt, mtcars) ]]\n\n\n[1]  TRUE FALSE  TRUE\n\nGetting help\nFor further details, including an introductory vignette illustrating the features of r2r hash maps, you can consult the r2r website. If you encounter a bug, want to suggest a feature or need further help, you can open a GitHub issue.\nComparison with hash\nCRAN package {hash} also offers an implementation of hash tables based on R environments. The two tables below offer a comparison between {r2r} and {hash} (for more details, see the benchmarks Vignette)\n\nTable 1: Features supported by {r2r} and {hash}.\nFeature\nr2r\nhash\nBasic data structure\nR environment\nR environment\nArbitrary type keys\nX\n\nArbitrary type values\nX\nX\nArbitrary hash function\nX\n\nArbitrary key comparison function\nX\n\nThrow or return default on missing keys\nX\n\nHash table inversion\n\nX\n\n\nTable 2: Performances of {r2r} and {hash} for basic hash table operations.\nTask\nComparison\nKey insertion\n{r2r} ~ {hash}\nKey query\n{r2r} < {hash}\nKey deletion\n{r2r} << {hash}\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-13T16:53:50+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-06-test-post/",
    "title": "Test post",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "vgherard",
        "url": {}
      }
    ],
    "date": "2021-07-06",
    "categories": [
      "Other"
    ],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-13T16:53:50+01:00",
    "input_file": {}
  }
]
