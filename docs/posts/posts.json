[
  {
    "path": "posts/2023-05-12-bias-in-ols-estimator/",
    "title": "Consistency and bias in OLS estimator",
    "description": "OLS estimators are consistent but generally biased - here's an example.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-05-12",
    "categories": [
      "Statistics"
    ],
    "contents": "\nGiven random variables \\(Y\\colon \\Omega \\to \\mathbb R\\) and\n\\(X\\colon \\Omega \\to \\mathbb R ^{p}\\) defined on an event space \\(\\Omega\\), denote:\n\\[\n\\beta = \\arg \\min _{\\beta ^\\prime } \\mathbb E[(Y-X \\beta^\\prime )^2]= \\mathbb E(X^TX)^{-1}\\mathbb E(X^TY), \\tag{1}\n\\]\nso that \\(X \\beta\\) is the best linear predictor of \\(Y\\) in terms of \\(X\\) (\\(X\\) is\nregarded as a row vector).\nLet \\((\\textbf Y, \\textbf X)\\) be independent samples from the joint \\(XY\\)\ndistribution, with independent observations stacked vertically in \\(N \\times 1\\)\nand \\(N \\times p\\) matrices respectively, as customary. Then the usual Ordinary\nLeast Squares (OLS) estimator of \\(\\beta\\) is given by:\n\\[\n\\hat \\beta = \\arg \\min _{\\beta ^\\prime}(\\textbf Y - \\textbf X \\beta ^\\prime)^2=(\\textbf X^T\\textbf X)^{-1} \\textbf X^T \\textbf Y. \\tag{2}\n\\]\nThis is a consistent, but generally biased estimator of \\(\\beta\\).\nComparing Eqs. (1) and (2), consistency follows immediately\nfrom the law of large numbers and continuity. In order to show that\n\\(\\mathbb E (\\hat \\beta) \\neq \\beta\\) in general, it is sufficient to provide an\nexample.\nConsider, for instance (example adapted from D.A. Freedman):\n\\[\nX \\sim \\mathcal N (0, 1),\\qquad Y=X(1+aX^2)\n\\]\nRecalling that \\(\\mathbb E (X^4) = 3\\) for the standard normal, we have:\n\\[\n\\beta = 1+3a,\n\\]\nwhere we have ignored a potential intercept term (which would vanish here, since\n\\(\\mathbb E (Y) = 0\\)). To compute \\(\\mathbb E (\\hat \\beta)\\), we use the identity\n\\(\\frac{e^{-z}}{z} = \\intop _1 ^\\infty \\text d t\\, e ^{-zt}\\) to rewrite this\nexpected value as:\n\\[\n\\begin{split}\n\\mathbb E (\\hat \\beta) & =  (2 \\pi)^{-N/2}\n    \\intop \\text d\\textbf X \\,e^{-\\sum _j X_i ^2 /2}\n                                    \\dfrac{\\sum _i X_i^2(1+aX_i^2)}{\\sum _i X_i^2} = \\frac{N}{2}\\intop_1 ^\\infty \\text d t\\,I(t) \\\\\nI(t)                                     & \\equiv (2 \\pi)^{-N/2} \\intop \\text d\\textbf X\\,\n                                                        e^{-t \\sum _j X_j ^2 /2}X_1^2(1+aX_1^2)\n\\end{split}\n\\]\nThe inner integral can be computed easily:\n\\[\nI(t) = t^{-\\frac{N}{2}}(\\frac{1}{t}+a\\frac{3}{t^2})\n\\]\nand we eventually find:\n\\[\n\\mathbb E (\\hat \\beta) = 1+3 a\\frac{N}{N+2}\n\\]\nThe bias is thus given by:\n\\[\n\\beta - \\mathbb E (\\hat \\beta) = \\frac{6a}{N+2}\n\\]\nThis vanishes linearly, in agreement with the fact that\n\\(\\sqrt N (\\hat \\beta - \\beta )\\) converges in probability to a gaussian with\nzero mean and finite variance (which requires the bias to be \\(o(N^{-1/2})\\)).\n\n\n\n",
    "preview": {},
    "last_modified": "2023-05-12T22:42:29+02:00",
    "input_file": "bias-in-ols-estimator.knit.md"
  },
  {
    "path": "posts/2023-05-01-magic-piggy-bank/",
    "title": "Bayes, Neyman and the Magic Piggy Bank",
    "description": "Compares frequentist properties of credible intervals and confidence \nintervals in a gambling game involving a magic piggy bank.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-05-01",
    "categories": [
      "Statistics"
    ],
    "contents": "\nIntro\nFrequentist and Bayesian approaches to statistical inference are motivated by\ndifferent interpretations of the concept of probability.\nThese philosophical differences can, at times, shadow the comparably important\noperational differences between the two frameworks, whose methods proceed, at\nthe end of the day, from the same mathematical theory.\nFrom the purely operational point of view, the\nquestion ‚ÄúBayesian or Frequentist?‚Äù can (and should) be answered by\nobjective criteria, rather than subjective opinions. As one could\nexpect, the answer is in general neither ‚ÄúFrequentist‚Äù nor ‚ÄúBayesian‚Äù,\nbut rather ‚ÄúIt depends‚Äù.\nTo illustrate this, I will discuss an hypothetical game that revolves around\nreporting measurements and correctly quantifying uncertainty. As we shall see,\nthe winning strategies can be either Frequentist or Bayesian in spirit,\ndepending on a variation of the actual rules of the game.\nReporting measurements\nAll scientific measurements come with an associated uncertainty, which can be\nexpressed in the form of an interval that is supposed to contain the object\nof measurement. In the Frequentist and Bayesian\nframeworks, these intervals are traditionally dubbed Confidence and Credible\nintervals, respectively. While, superficially, these can be both characterized\nas ‚Äúcovering the true value with probability \\(p\\)‚Äù, the word ‚Äúprobability‚Äù\nhas quite different connotations in the two cases, and confusing them can lead\nto irrational thought or, as in the imaginary game described below, financial\nruin.\nMagic Piggy Bank\nThere are two players, called the Bookmaker and the Gambler, that compete\nagainst each other in a gambling game1. The interactions between these two players are mediated by\nthe Magic Piggy Bank, a magic creature that acts as a sort of referee.\nThe Magic Piggy Bank contains infinite biased coins, and knows the probability\n\\(\\Theta\\) of giving ‚Äútails‚Äù for each one of them.\nA single iteration of the game proceeds as follows:\nThe Magic Piggy Bank ejects 2 a biased coin and gives it to the Bookmaker.\nThe Bookmaker can flip the coin an arbitrary number of times, to produce an\nestimate of \\(\\Theta\\), in the form of an interval \\(I\\). This must be accompanied\nby a payout, that is a number \\(p\\in \\left(0,1\\right)\\), for bets on the event\n\\(\\Theta \\in I\\). The resulting \\(I\\) and\n\\(p\\), together with the original data \\(X=(n_\\text{tosses}, n_\\text{tails})\\) from\nthe Bookmaker‚Äôs experiments, are reported to the Magic Piggy Bank.\nThe Magic Piggy Bank communicates the payout \\(p\\) to the Gambler, and reveals\nsome additional information. What particular information is revealed depends\non the variant of the game being played (see descriptions below).\nBased on the information received, the Gambler can choose to bet either\nin favor or against \\(\\Theta \\in I\\). When betting in favor, the Gambler pays \\(p\\)\nto the Bookmaker, who returns back \\(1\\) if \\(\\Theta \\in I\\) obtains. When betting\nagainst, the Bookmaker pays \\(p\\) to the Gambler, who returns back \\(1\\) if\n\\(\\Theta \\in I\\) obtains.\nThe Magic Piggy Bank reveals all data (\\(X\\), \\(I\\), \\(\\Theta\\)) to both players\nand the scores are settled.\nAs to the third step, we will consider three variants of the game:\nThe Magic Piggy Bank tells the Gambler the results of the Bookmaker‚Äôs tosses\n\\(X=(n_\\text{tosses}, n_\\text{tails})\\), as well as the actual interval \\(I\\).\nThe Magic Piggy Bank tells the Gambler the true value of \\(\\Theta\\).\nThe Gambler is given no additional information beyond the established payout\n\\(p\\).\nProblem\nSuppose that the Bookmaker and Gambler are forced to play indefinitely.\nWhat are the best strategies for these two players, according to\nthe three different variants A, B, and C described above3?\nAnalysis\nOne can readily verify that the Gambler‚Äôs gain (or, equivalently, the\nBookmaker‚Äôs loss) in a single iteration of the game is given by:\n\\[\nG=b\\cdot (\\chi_I (\\Theta)-p) \\tag{1}\n\\]\nwhere, \\(b\\) is equal to \\(\\pm 1\\) if the Gambler bets in favor or against,\nrespectively, and:\n\\[\n\\chi _I (\\Theta) = \\begin{cases}\n1 & \\Theta \\in I \\\\\n0 & \\Theta \\notin I\n\\end{cases} \\tag{2}\n\\]\nThe expected gain is given by:\n\\[\n\\mathbb E (G) = \\intop \\text{d}P(\\Theta,X) \\,b\\cdot (\\chi_I (\\Theta)-p), \\tag{3}\n\\]\nwhere \\(\\text{d} P(\\Theta, X)\\) denotes the joint probability measure of \\(\\Theta\\)\nand \\(X\\).\nLet‚Äôs now examine in detail the three different variants (A, B, C) of the game\ndescribed above.\nVariant A\nIn the first variant of the game, the Gambler is given the same information as the\nBookmaker. In particular, the choice to bet in favor or against, represented by\nthe sign \\(b\\), cannot depend on \\(\\Theta\\) (which the Gambler doesn‚Äôt know), and we\ncan rewrite the expected gain (3) as4:\n\\[\n\\begin{split}\n\\mathbb E (G) &= \\intop \\text{d}P(X) \\,b\\cdot \\intop \\text{d}P(\\Theta \\vert X) \\,(\\chi_I (\\Theta)-p) \\\\\n& = \\intop \\text{d}P(X) \\,b \\cdot \\left(\\text {Pr}(\\Theta \\in I \\vert X)-p\\right)\n\\end{split} \\tag{4}\n\\]\nwhere we have used the fact that, for any random variable \\(Y\\) and set \\(E\\), the\nfollowing relation holds:\n\\[\n\\mathbb E (\\chi _E (Y)) = \\text{Pr}(Y \\in E).\n\\]\nNow, since both \\(X\\) and \\(I\\) are known to the Gambler, the latter is (at least in\nprinciple) able to compute:\n\\[\nb_A \\equiv \\text{sgn}\\left(\\text {Pr}(\\Theta \\in I \\vert X)-p\\right) \\tag{5}\n\\]\nIn practice, in order to compute (5), the Gambler would need\nto know the overall distribution \\(\\pi (\\Theta)\\) of the coins \\(\\Theta\\) extracted\nfrom the Magic Piggy Bank, but this is something that can be accurately\nestimated in the long run, since the actual values of \\(\\Theta\\) are revealed\nat the end of each iteration 5.\nPlugging Eq. (5) into Eq. (4), we find:\n\\[\n\\mathbb E (G) = \\intop \\text{d}P(X) \\left|\\text {Pr}(\\Theta \\in I \\vert X)-p\\right|\\quad\\text{(Variant A)}   \\tag{6}.\n\\]\nComparing with (4), it is clear that\n(6) is the maximum expected gain, for any choice\nof \\(b\\). In other words, the choice \\(b_A\\) in Eq. (5) is an\noptimal one.\nFinally, from the Bookmaker‚Äôs point of view,\nEq. (6) represents a sure loss in the long run,\nthat can only be avoided by enforcing:\n\\[\n\\text {Pr}(\\Theta \\in I \\vert X)=p \\quad \\text{(Variant A)} \\tag{7}\n\\]\nIn order to ensure this, the Bookmaker needs to know as well the overall\ncoins‚Äô distribution \\(\\pi (\\Theta)\\), and the same remarks made above for the\nGambler apply here.\nEquation (7) defines what is known as a\nBayesian credible interval.\nVariant B\nWe now consider the second variant of the rules, where the Gambler is told the\ntrue value of \\(\\Theta\\), but does not know the details of the Bookmaker‚Äôs\nmeasurement, except for the established payout \\(p\\). Using a reasoning similar\nto the previous section we rewrite:\n\\[\n\\begin{split}\n\\mathbb E (G) &= \\intop \\text{d}P(\\Theta) \\,b\\cdot \\intop \\text{d}P(X \\vert\\Theta) \\,(\\chi_I (\\Theta)-p) \\\\\n& = \\intop \\text{d}P(\\Theta) \\,b \\cdot \\left(\\text {Pr}(\\Theta \\in I \\vert \\Theta)-p\\right)\n\\end{split} \\tag{8}\n\\]\nand define6:\n\\[\nb_B \\equiv \\text{sgn}\\left(\\text {Pr}(\\Theta \\in I \\vert \\Theta)-p\\right)\\quad(\\text{Variant B}) \\tag{9}\n\\]\nwhich is easily shown to be the optimal betting strategy for the Gambler in the\npresent setting. In the long run, this sign can be accurately estimated by\nmodeling the conditional mean of \\(\\chi _I (\\Theta) - p\\) (as a function of\n\\(\\Theta\\) and \\(p\\)).\nIf the Gambler bets according to (9), the Bookmaker is forced\nto set payouts according to:\n\\[\n\\text {Pr}(\\Theta \\in I \\vert \\Theta)=p\\quad(\\text{Variant B}), \\tag{10}\n\\]\nin order to avoid a certain loss.\nEquation (10) defines what is known as a\nFrequentist confidence interval.\nVariant C\nIn the last case, the Gambler has no extra information beyond the payout \\(p\\),\nand the expected gain reduces to:\n\\[\n\\mathbb E (G)=b\\cdot \\left(\\text{Pr}(\\Theta \\in I)-p\\right),\\tag{11}\n\\]\nwhere \\(\\text{Pr}(\\Theta \\in I)\\) is the unconditional probability that \\(I\\) covers\n\\(\\Theta\\). The optimal betting choice is:\n\\[\nb_C \\equiv \\text{sgn}\\left(\\text {Pr}(\\Theta \\in I)-p\\right)\\quad(\\text{Variant C}) \\tag{12}\n\\]\nwhich forces the Bookmaker to set payouts according to:\n\\[\n\\text {Pr}(\\Theta \\in I)=p\\quad(\\text{Variant C}). \\tag{13}\n\\]\nThis is, by the way, satisfied by both the Bayesian and Frequentist intervals,\ndue to Eqs. (7) and (10), respectively.\nSummary of results\nProvided access to the same data used by the Bookmaker to\nproduce the interval \\(I\\) (Variant A), a rational Gambler would bet in favor of\n\\(\\Theta \\in I\\) if the probability of this event\nconditional to the observed the data is greater than the payout \\(p\\)\n(Eq. (5)).\nOn the other hand, given true value of \\(\\Theta\\) (Variant B), the optimal choice\nfor a Gambler is to bet on \\(\\Theta \\in I\\) if the probability of this event\nconditional to the ground truth is greater than \\(p\\)\n(Eq. (9)).\nFinally, in the lack of any of this information (Variant C), the most rational\nchoice is simply to bet on \\(\\Theta \\in I\\) if this event occurs more frequently\nthan \\(p\\) (Eq. (12)).\nWhen playing against the first two types of players, in order to avoid a certain\nloss, the Bookmaker must produce Bayesian credible intervals (Variant A) or\nFrequentist confidence intervals (Variant B). In the remaining case (Variant C),\nthe Bookmaker can either produce Bayesian or Frequentist intervals7.\nConclusions\nWhen I first learned about Bayesian and Frequentist inference, I remember most\ndiscussions were focused on the philosophical differences between these two\nschools of thought. There was little to no mention about the actual mathematical\nproperties of the constructs prescribed by the two formalisms, which made the\nchoice between ‚ÄúBayesian‚Äù or ‚ÄúFrequentist‚Äù look like a mere matter of\ncommitting to one particular view.\nTechnically, what I did here was to compare the frequentist properties of\ncredible intervals and confidence intervals. I‚Äôm sure the literature,\nincluding the pedagogical one, is full of examples like this, and better ones8. With no pretense of originality, I believe that including more examples\nof this kind in the usual presentations can be beneficial to students and\npractitioners, and perhaps help them out of the ugly black-box of orthodoxy.\n\nThe introduction of bets as an expedient\nto operationally define subjective probabilities is historically due to the\nItalian mathematician\nBruno de Finetti.\nThe statistical analysis of the game proposed below can be given a Frequentist\ninterpretation.‚Ü©Ô∏é\nReaders are free to imagine this process in the way they find more convenient.‚Ü©Ô∏é\nWe assume that both\nplayers know from the outset which variant of the game they are playing to.‚Ü©Ô∏é\n\nWe denote (with some abuse of notation) by\n\\(\\text{d}P(\\Theta \\vert X)\\) the conditional probability measure of \\(\\Theta\\)\nconditioned on \\(X\\).‚Ü©Ô∏é\nIn the Bayesian spirit of (5),\nthe Gambler could for instance estimate \\(\\pi(\\Theta)\\) through Bayesian updates\nof a Dirichlet prior.‚Ü©Ô∏é\nNoteworthy, the random quantity in this equation is \\(I\\),\nwhereas \\(\\Theta\\) is regarded as fixed. This is in stark contrast with\nEq. (5), where \\(X\\) and \\(I\\) were fixed, and \\(\\Theta\\) was\nrandom.‚Ü©Ô∏é\n\nThere are, in fact, infinitely many more ways to produce intervals with the\nunconditional coverage property Eq. (13).‚Ü©Ô∏é\n\nI see that Jaynes (the father of the Maximum Entropy foundation of\nStatistical Mechanics, among other things) has a full essay paper on\nConfidence Intervals vs.¬†Bayesian Intervals, which I haven‚Äôt read -\nthe abstract sounds a bit loaded to me, but it‚Äôs probably definitely worth to\nread.‚Ü©Ô∏é\n",
    "preview": {},
    "last_modified": "2023-05-01T20:40:36+02:00",
    "input_file": "magic-piggy-bank.knit.md"
  },
  {
    "path": "posts/2023-03-10-correlation-without-causation/",
    "title": "Correlation Without Causation",
    "description": "*Cum hoc ergo propter hoc*",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-03-30",
    "categories": [
      "Statistics"
    ],
    "contents": "\nIt is part of common knowledge that correlation does not require causation.\nAbsence of causation, say between a condition \\(p\\) and an effect \\(q\\), means that the realization of \\(p\\) has no influence on the presence of \\(q\\). If this is the case,\na statistical correlation between \\(p\\) and \\(q\\) can still be present, if the realization of \\(p\\) modifies our state of information about \\(q\\).\nAs an example, let \\(X,Y\\) be two conditionally independent binary random variables, with a common probability \\(\\Theta\\) of evaluating to one. Think, for instance,\nof a machine that produces pairs of identical biased coins, with a probability of tails \\(\\Theta\\).\nIf \\(\\Theta\\) is equal to a given value \\(\\theta\\), the joint probability distribution of \\(X\\) and \\(Y\\) is:\n\\[\n\\text {Pr}(X=x,Y=y\\vert \\Theta = \\theta) = B(x;\\theta)B(y;\\theta), \\tag{1}\n\\]\nwhere \\(B(z; \\theta) = \\theta ^z (1 - \\theta) ^ {1-z}\\).\nWhether or not this provides a satisfying probabilistic description of experiments on \\(X\\) and \\(Y\\) depends on context.\nFrom a frequentist point of view, if \\(\\Theta\\) is fixed once and for all, the right hand side of Eq. (1) correctly describes the experimental outcomes of \\(X\\) and \\(Y\\) for some value of \\(\\theta\\). On the other hand, if \\(\\Theta\\) can change from experiment to experiment in a random fashion, and we do not observe its values \\(\\theta\\), we clearly cannot use Eq. (1) as it stands, as its usage requires knowing \\(\\theta\\).\nFinally, from a bayesian‚Äôs point of view, if \\(\\Theta\\) is fixed but unknown, Eq. (1) does not describe our state of knowledge about \\(X\\) and \\(Y\\), because it assumes unavailable information (\\(\\Theta = \\theta\\)).\nIn the last two cases, what we‚Äôre actually after is the unconditional probability:\n\\[\n\\text{Pr}(X=x,\\,Y=y)=\\intop\\,\\text{d}P_\\Theta(\\theta) \\,\\text{Pr}(X=x,Y=y\\vert\\Theta = \\theta)\n\\tag{2}\n\\]\nwhere \\(\\text{d}P_\\Theta(\\theta)\\) can be regarded either as the actual probability distribution of \\(\\Theta\\) (in a frequentist framework) or as a subjective prior distribution (in a bayesian framework).\nPlugging Eq. (1) into (2), we find:\n\\[\n\\begin{split}\n\\text{Pr}(X=1,\\,Y=1) & = \\mathbb E(\\Theta)^2+\\text{Var}(\\Theta)\\\\\n\\text{Pr}(X=1,\\, Y=0)&=\\mathbb E(\\Theta)-\\mathbb E(\\Theta)^2-\\text{Var}(\\Theta)\\\\\n\\text{Pr}(X=0,\\, Y=1)&=\\mathbb E(\\Theta)-\\mathbb E(\\Theta)^2-\\text{Var}(\\Theta)\\\\\n\\text{Pr}(X=0,\\,Y=0) & = \\mathbb (1-\\mathbb E(\\Theta))^2+\\text{Var}(\\Theta) \\\\\n\\end{split}\n\\]\nIn particular, we have:\n\\[\n\\dfrac{\\text{Pr}(Y = 1 \\vert\\, X = 1)}{\\text {Pr}(Y=1)} = 1+\\frac{\\text{Var}(\\Theta)}{\\mathbb{E}(\\Theta)^2},\n\\tag{3}\n\\]\nwhich means that, unconditionally, \\(X\\) and \\(Y\\) are not independent, but in fact positively correlated1.\nObservations of this kind apply, mutatis mutandis, in many practical situations. For instance if we were modeling the time series of new visitors to a website, we could reasonably assume that the number of yesterday‚Äôs new visitors does not influence the number of today‚Äôs ones (if individual visitors are unlikely to interact with each other). Yet, it would be wrong to assume, and easy to disprove, that these two numbers are by themselves statistically independent, because yesterday‚Äôs new visitors carry useful background information on today‚Äôs potential new visitors.\nThe bottom line of the post is that lack of causation does not imply lack of correlation, which is logically equivalent to the original motto‚Ä¶ but, for some strange reason, I find easier to forget.\n\nHere I‚Äôm using the word correlation in a loose sense, as in the popular motto.‚Ü©Ô∏é\n",
    "preview": {},
    "last_modified": "2023-05-01T20:34:31+02:00",
    "input_file": "correlation-without-causation.knit.md"
  },
  {
    "path": "posts/2022-11-07-posi-2/",
    "title": "How to get away with selection. Part II: Mathematical Framework",
    "description": "Mathematicals details on Selective Inference, model misspecification and coverage guarantees.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2022-11-25",
    "categories": [
      "Statistics"
    ],
    "contents": "\nIntroduction\nIn a previous post I introduced the problem of\nSelective Inference and illustrated, in a simplified setting, how selection\ngenerally affects the coverage of confidence intervals - when they are both\nselected and constructed using the same data. While the example was\n(hopefully) helpful to build some intuition, in order to discuss\n‚ÄúHow to get away with selection‚Äù in a comprehensive manner we need to make a\nfew clarifications. In particular, we need to answer the following questions:\nWhat is the target of our Selective Inference?\nWhat statistical properties would we like our inferences to have?\nSearching through the literature, I realized there exist a bunch of variations\non these two themes, which give rise to different mathematical formalisms.\nSpecifying these points is mandatory for any further discussion, so my main goal\nhere is to present these different points of view and explain some of their\npros and cons.\nMathematical Framework\nRegression and parameter estimation\nIn order to avoid getting carried away with too much abstraction, I will focus\non a specific type of problem, that is parameter estimation in regression. As\nfar as I can tell, this represents no serious loss in generality, and most of\nthe notions I‚Äôm going to outline would carry over to more general problems in a\nstraightforward manner.\nBroadly speaking, the goal of regression is to understand the dependence of a\nset of random variables \\(Y\\) from another set of random variables \\(X\\). More precisely,\nwe‚Äôre interested in the conditional probability distribution of \\(Y\\),\nconditioned on the observation of \\(X\\), which can always be represented as:\n\\[\nY = f(X)+\\varepsilon,\\qquad \\mathbb E(\\varepsilon|X)\\equiv 0.\n\\tag{1}\n\\]\nwhere \\(f(X) = \\mathbb E(Y|X)\\) is the conditional mean of \\(Y|X\\), and \\(\\varepsilon\\)\nis a random variable with vanishing conditional mean, sometimes called the ‚Äúerror term‚Äù.\nParameter estimation means that we have (somehow) chosen functional forms for\nthe conditional mean and for the probability distribution of the error term,\nand we want to provide estimates for the parameters defining these two functions.\nEnter selection\nNow, in many applications we actually don‚Äôt have much insight about the correct\nfunctional form \\(f\\), nor of the distribution of the error term \\(\\varepsilon\\).\nGiven a dataset of experimental observations of \\(Y\\) and \\(X\\), we are thus faced\nwith two tasks:\nSelection. Choose an adequate model \\(\\hat M = (\\hat f,\\,\\hat \\varepsilon)\\)\nfor the true \\(f\\) and \\(\\varepsilon\\), usually from a (more or less) pre-specified\nfamily of initial guesses \\(\\mathcal M =\\{(f_i,\\varepsilon_i)\\}_i\\), using a\n(more or less) pre-specified criterion.\nPost-Selection Inference. Perform inference with the chosen model. In the\nstudy case we‚Äôre considering, this amounts to provide confidence intervals for\nmodel parameters.\nIt is, of course, the need to use the same data for both tasks which gives rise\nto complications.\nInferential target\nWe now come to the first question raised in the Introduction, regarding the\nnature of the inferential target. And now more concretely:\nwhat are the true values of the parameters we‚Äôre trying to estimate?\nOne can appreciate that the answer necessarily depends on how we consider the\nfinal output of the modeling procedure:\n(Model Trusting) As the true data generating process, or\n(Assumption Lean) As an approximation of the (partially or totally\nunknown) data generating process, chosen in a data-driven fashion within\na family of initial guesses \\(\\mathcal M\\).\nAccording to the first interpretation, there‚Äôs no room for ambiguity: the\ntargets of our estimates should clearly be the true parameter values,\nwhose definition does not depend on any modeling choice. The second\ninterpretation, on the other hand, leaves a certain amount of\nfreedom in this respect. Here, I will follow the point of view advocated by\n(Berk et al. 2013), according to which the target parameters\nare those providing the best approximation1 to the true data generating\nprocess, according to the functional form chosen in the selection stage.\nI believe both positions have their merits and flaws, and which one is more\nappropriate largely depends on context. In a reductionist field like\nHigh Energy Physics, whose eventual goal is to explain the fundamental laws of\nNature, the Model Trusting point of view is usually taken,\nand with good reason. When studying more emergent phenomena, on the other hand,\nthe quest for fundamental laws is often meaningless (or at best wishful\nthinking), and the Assumption Lean standpoint looks more reasonable. In any\ncase, here the differences are not merely philosophical ones, as the two\ninterpretations give rise to different mathematical formalisms.\nIn the following posts, I will be mostly focusing on the Assumption Lean\npoint of view. In my opinion, this has two big advantages2:\nConceptual: Inferences have a well-defined meaning even when the model is\nmisspecified3 - which, apart from quite particular cases (see above),\naccounts for the great majority of cases encountered by data analysts in\nthe practice.\nMathematical: It allows to reduce the problem of selective inference to\nthat of simultaneous inference (more on this below).\nFor the latter type of problems, the theory of\nmultiple testing\nreadily provides at least conservative bounds.\nNotions of coverage\nIn addition to the conceptual distinction about the interpretation of the\nselected model, there is also a technical distinction regarding the type\nof coverage guarantees that selective confidence intervals should be endowed\nwith (this is the concrete version of the second question posed in the\nIntroduction).\nHere are some of the notions of coverage I‚Äôve come across:\nMarginal coverage over the selected parameters. We bound at level \\(\\alpha\\)\nthe probability that our procedure constructs any non-covering confidence\ninterval for model parameters \\(\\beta_i\\). Denote by \\(\\widehat M\\) the selected model\nand, with abuse of notation, the corresponding set of selected parameters.\nIf \\(\\widehat{\\text{CI}}_i\\) are the confidence intervals for parameters \\(\\beta _i\\), we\nrequire:\n\\[\n\\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\in \\widehat M) \\geq 1-\\alpha\n\\tag{2}\n\\]\nConditional coverage over the selected parameters. We bound at level\n\\(\\alpha\\) the conditional\nprobability of constructing a non-covering confidence interval, conditioned on\nthe outcome of selection \\(\\widehat M\\). If \\(m\\) is the selected model, we require:\n\\[\n\\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\in m|\\,\\widehat M=m) \\geq 1-\\alpha\n\\tag{3}\n\\]\nFalse Coverage Rate. We bound at level \\(q\\)4 the expected fraction of\nnon-covering confidence intervals out of all intervals constructed:\n\\[\n\\mathbb E \\left( \\dfrac{|i \\in \\widehat M \\colon \\ \\beta_i \\in \\widehat{\\text{CI}}_i|}{|\\widehat M|} \\right)\n\\geq1-q\n\\tag{4}\n\\]\nwhere \\(|S|\\) denotes the cardinality of a set \\(S\\).\nNotice that the random variables in the previous equations are \\(\\widehat M\\) and\n\\(\\widehat{\\text{CI}}_i\\) (denoted by a hat), whereas the true coefficients \\(\\beta_i\\)\nand the selected set \\(m\\) in the case of conditional coverage\n(Eq. (3)) are fixed quantities.\nVariations of these measures focusing on single coefficients are also possible.\nIn practice, in the Assumption Lean framework I just introduced,\nall these coverage measures would not be computed\nunder the selected model‚Äôs probability distribution, but rather under a\npre-fixed, more general model for the true probability distribution of \\(Y\\)\nconditional on \\(X\\). We may, for instance, assume that the true error term\n\\(\\varepsilon\\) in Eq. (1) is gaussian with constant\n(\\(X\\)-independent) variance, without making any further assumption on \\(f(X)\\).\nWith enough data, we may even be able to bypass any assumption at all, and\ncompute all relevant quantiles using a bootstrap (Kuchibhotla et al. 2020).\nIn the Model Trusting framework, on the other hand, the conditional coverage\nmeasure would be computed under the selected model‚Ä¶ and I‚Äôm honestly not\nsure whether it‚Äôs possible to make sense of the other two measures in this\nframework.\nSelective vs.¬†Simultaneous Inference\nThe connection between selective and simultaneous inference can now be\nunderstood, through the notion of marginal coverage. In fact, suppose that we\nwere able to provide simultaneous coverage for all parameters\n(selected or not):\n\\[\n\\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i) \\geq 1-\\alpha\n\\tag{5}\n\\]\nThen, it‚Äôs easy to see that the same confidence interval would also provide\nmarginal coverage over the selected parameters. In order to see that, simply\nobserve that the simultaneous coverage event can be decomposed as:\n\\[\n(\\beta _i \\in \\widehat {\\text{CI}}_i\\,\\,\\forall i) = (\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\in \\widehat M) \\cap (\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\notin \\widehat M)\n\\]\nwhich implies that:\n\\[\n\\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\in \\widehat M) \\geq \\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i) \\geq 1-\\alpha,\n\\tag{6}\n\\]\nthat is simultaneous coverage implies marginal coverage over the\nselected parameters. In fact, with a few more set-theory manipulations,\none can arrive to a powerful Lemma (see Kuchibhotla et al. 2020 for details):\ncontrolling the marginal coverage (2) at level \\(\\alpha\\)\nfor any model selection procedure5 is equivalent to controlling\nsimultaneous coverage for all possible model selections.\nThis provides us a first, very simple recipe for selective inference, which can\nbe applied whenever one is able to construct confidence intervals for parameters\nin the absence of selection: use any procedure (e.g.¬†\nBonferroni corrections)\nwhich controls simultaneous coverage for all parameters we may select a priori.\nConclusions\nThis was a long and somewhat abstract post, so perhaps the best way to conclude\nis with some bottom lines:\nWhen performing model-based inference, nothing forces us to make working\nhypotheses about the correctness of the model we arrive at. Not making such\nassumptions corresponds to what I called an Assumption Lean framework.\nIn an Assumption Lean framework, the inferential targets are, in general,\nthe best approximations to the truth allowed by the selected model.\nThere exist many type of coverage guarantees for selective confidence\nintervals.\nBounding the probability of any false coverage statement\n(‚Äúmarginal coverage over the selected parameters‚Äù) allows to turn a problem of\nselective inference into one of simultaneous inference.\nIn particular, it is worth to mention that the last observation lead us to a\nsimple recipe for constructing (somewhat conservative, but valid) selective\nconfidence intervals with marginal coverage. In the posts which follow,\nI will discuss some more advanced methods which produce confidence intervals\nsatisfying the requirements discussed here.\n\n\n\nBenjamini, Yoav, and Daniel Yekutieli. 2005. ‚ÄúFalse Discovery Rate‚ÄìAdjusted Multiple Confidence Intervals for Selected Parameters.‚Äù Journal of the American Statistical Association 100 (469): 71‚Äì81.\n\n\nBerk, Richard, Lawrence Brown, Andreas Buja, Kai Zhang, and Linda Zhao. 2013. ‚ÄúValid Post-Selection Inference.‚Äù The Annals of Statistics, 802‚Äì37.\n\n\nKuchibhotla, Arun K, Lawrence D Brown, Andreas Buja, Junhui Cai, Edward I George, and Linda H Zhao. 2020. ‚ÄúValid Post-Selection Inference in Model-Free Linear Regression.‚Äù The Annals of Statistics 48 (5): 2953‚Äì81.\n\n\nWhere what‚Äôs to be considered best is defined in terms of some\nreasonable metric. For instance, for the conditional mean \\(f(X)\\) of a continuous\nresponse \\(Y\\), a convenient target \\(f^*(X)\\) within a prescribed family of\nfunctions \\(\\mathcal F\\) can be defined by\n\\(f^* =\\arg\\min _{\\phi \\in \\mathcal F} \\mathbb E (\\vert f(X) - \\phi (X)\\vert^2)\\).‚Ü©Ô∏é\nThere‚Äôs also a third advantage, which is that I find much harder to think\nabout selective inference from the Model Trusting point of view, hence to write\nblog posts about it - but that‚Äôs likely a limitation of my imagination, rather\nthan of the point of view itself.‚Ü©Ô∏é\nA cool word for ‚Äúwrong‚Äù.‚Ü©Ô∏é\nWhy \\(q\\) and not \\(\\alpha\\)? Ask (Benjamini and Yekutieli 2005).‚Ü©Ô∏é\nIt is assumed that the selection is performed from a from a fixed family\nof models \\(\\mathcal M\\).‚Ü©Ô∏é\n",
    "preview": {},
    "last_modified": "2022-11-25T17:51:25+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-18-posi/",
    "title": "How to get away with selection. Part I: Introduction",
    "description": "Introducing the problem of Selective Inference, illustrated through a simple simulation in R.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2022-11-14",
    "categories": [
      "R",
      "Statistics"
    ],
    "contents": "\nPrologue\nA few months back, for undocumented circumstances, my browser‚Äôs search\nhistory was full of terms like ‚Äúparameter estimation with variable selection‚Äù,\nor ‚Äúconfidence intervals after cross-validation‚Äù, or again\n‚Äúlinear model uncertainties after staring into the abyss‚Äù, ‚Ä¶\nSparing you my rock bottom, I eventually stumbled upon the right keywords, and\nstarted digging into the mathematical aspects of Selective Inference, or\nPost-Model Selection Inference. Now, while my hands\nare still full of dirt, I‚Äôve decided it‚Äôs the right moment to write some\nnotes about what I‚Äôve learned - whose main recipient is the future me,\nwhich will otherwise inevitably forget what the present me thinks he\nknows. If you‚Äôre not the future me:\nWelcome üëã\nIf you have detected some imprecision, or have suggestions for this or the\nnext posts, you are more than welcome to create an issue on the source\nrepository of this blog.\nIntroduction\nBroadly speaking, the problem of Selective Inference is that of\nperforming valid statistical inferences when the actual questions of the data analysis are not fixed in advance, but rather selected through data examination. In model-based inference, this lack of\npre-determination usually stems from the (often unavoidable) practice of\nusing the same data to choose an adequate model for the data generating\nprocess and to perform inference. The intrinsic\nrandomness of the selection process has important consequences on the\nprobability of making different guesses about the selected questions,\nwhich, if not properly taken into account, can completely invalidate the\nanalysis results.\nIf this sounds unfamiliar, think about machine-learning: when training a\npredictive model on a given dataset, you would usually consider the\nerror on the same dataset as a poor (optimistic) estimate of the true\nmodel‚Äôs error rate, because the model was tuned to perform well on that\ndata in the first place. There we go, Selective Inference! A selection\nfrom an extended family of models1 is performed through data examination,\nand this event introduces a bias in the error estimate of the final\nmodel from training data.\nThe example from machine-learning also suggests a very simple-minded and\nrelatively a-theoretical approach to Selective Inference: data-splitting2. According to this method, we would use only part of the available data to select the questions to be answered by the analysis, while the remaining part would\nbe reserved to perform the actual inference. For this program to\nsucceed, there are however two important requirements: first, we must have\nenough data to ensure decent statistics for both the selection and inference\ntasks; and second, we must be able to split data in two independent\n(or close to independent) sets. This can suppose problems with, e.g.,\ntime-series data. If, on the other hand, these requirements cannot be met, we\nhave to resort to more sophisticated methods.\nAt this point, I would like to stress that the conceptual problems\nI‚Äôve just pointed out will probably look obvious to any reader with a\ndecent intuition for probability3. What is less obvious, but in fact\na fairly active research field in statistics,\nis how to perform valid selective inferences when the ‚Äúeasy‚Äù solution of\ndata-splitting I mentioned above is not available. This is where theory\nre-enters the game, and what I‚Äôm going to ramble about in this and the next\nposts.\nIllustrations of Selective Inference\nEnough for the speech, let us see how selection can affect (and invalidate)\nclassical inference with a simple-minded simulation.\nSetting\nTo illustrate why naive classical inference can fail in the presence of\nselection, we consider a very simple regression\nproblem involving a single regressor \\(X\\) and a response \\(Y\\), where all the assumptions of the classical linear model hold. In fact, we will assume the true data generating process to be:\n\\[\nY = mX + \\varepsilon, \\qquad \\varepsilon \\sim \\mathcal N (0, \\sigma),\n\\tag{1}\n\\]\nwhere \\(\\varepsilon \\sim \\mathcal N (0, \\sigma)\\) means ‚Äú\\(\\varepsilon\\) follows a gaussian distribution with mean \\(0\\) and standard deviation \\(\\sigma\\)‚Äù.\nA selective modeling procedure\nNow, suppose we are given a dataset of \\(N\\) independent observations\n\\((y_i, x_i)_{i = 1, \\,2,\\, \\dots,\\,N}\\), and we would like to study the\ndependence of \\(Y\\) from \\(X\\). Of course we don‚Äôt know the true law, Eq.\n(1), but by a stroke of luck (or by a Taylor expansion\nargument) we make the correct initial guess that such dependence is\nlinear in \\(X\\). We are, however, unsure whether it would be appropriate\nto also include an intercept term in the fit. We thus establish the\nfollowing selective modeling procedure:\nFit a linear model with intercept term,\n\\(Y = mX + q + \\varepsilon\\).\nStop if the intercept estimate is significantly different from zero (say, at the level of 1-\\(\\sigma\\), \\(p\\text{-value}<32\\%\\)). Otherwise:\nFit a model with no intercept, \\(Y = mX + \\varepsilon\\).\nFinally, we use the last fitted model to construct a ‚Äúnaive 95%‚Äù\nconfidence interval \\((\\hat m_-, \\hat m_+)\\) for the slope \\(m\\).\nThis is defined by:\n\\[\n\\hat m_\\pm = \\hat m\\pm t_{0.975, \\,N-d} \\cdot \\hat \\sigma _\\hat m\\qquad (95\\%\\,\\text {C.L.}).\n\\tag{2}\n\\]\nHere \\(t_{0.975,\\, N-d}\\) is the 97.5%-quantile of the \\(t\\)-student\ndistribution with \\(N-d\\) degrees of freedom, and \\(d\\) is the number of\nestimated parameters, (\\(2\\) or \\(1\\), according to where we stopped in the\nmodeling procedure). \\(\\hat m\\) and \\(\\hat \\sigma _{\\hat m}\\) are the\nOrdinary Least Squares (OLS) estimates of the slope and its standard\ndeviation, respectively. These are the classical confidence intervals\nreported by the lm() function in R.\nAt a first glance, this procedure might look reasonable. After all, both\nintervals we may end up constructing do have a genuine 95% coverage probability,\nwhen constructed unconditionally‚Ä¶ and by selecting the ‚Äúbest‚Äù model we‚Äôre\nsupposedly choosing the ‚Äúbest‚Äù confidence interval. In spite of this qualitative\nargument, we inquire:\n‚Ä¶ does it work?\nNow, the question is: how often do the naive CIs (2)\ncover the true parameter \\(m\\) of Eq. (1)? The answer\nbetter be ‚Äúat least 95% of the times‚Äù for our confidence claim in Eq.\n(2) to be valid!\nWe can check the actual coverage of (2) through a simulation.\nHere I‚Äôll assume \\(m = \\sigma = 1\\), and that the\ndataset consists of \\(N=10\\) independent observations of \\(Y\\) at fixed points\n\\(X = (1, \\,2, \\,\\dots ,\\, 10)\\).\n\n\nm <- sigma <- 1  # True parameters\nx <- 1:10  # x covariate, assumed fixed\n\n\nThe following function generates observations of \\(Y\\) according to the distribution (1):\n\n\ngenerate_y <- function(x, m, sigma) {\n  eps <- rnorm(length(x), mean = 0, sd = sigma)\n  return(m * x + eps)\n  }\n\n\nFor example:\n\n\nset.seed(840)\nplot(x, generate_y(x, m, sigma), xlab = \"X\", ylab = \"Y\")\n\n\n\nBelow we generate \\(B=10^4\\) such \\((X,Y)\\) datasets, for each of which we fit a linear model according to the procedure specified above, and check how many\ntimes the true slope \\(m = 1\\) falls in the confidence interval defined by Eq. (2).\n\n\n# Simulation parameters\nB <- 1e4  # Number of replications\n\n# Preallocate logical vectors to be assigned for each replica - for efficiency. \nq_dropped <- logical(B)  # Was the intercept term 'q' dropped? \nm_covered <- logical(B)  # Was the true parameter 'm' covered?\n\n# Set seed for reproducibility\nset.seed(841)\n\n# Logging\ntime_start <- Sys.time()  \n\n# Start the simulation\nfor (b in 1:B) {\n  y <- generate_y(x, m, sigma)\n  \n  # Fit full model (including intercept 'q')\n  fit <- lm(y ~ x + 1)  \n  q_pval <- summary(fit)$coefficients[1, 4]\n  \n  # Is 'q' term \"significant\"? If not, drop 'q' and fit a simpler model\n  if (q_pval > 0.32)  { \n    q_dropped[[b]] <- TRUE\n    fit <- lm(y ~ x - 1) \n  } else {\n    q_dropped[[b]] <- FALSE\n  }\n  \n  # Construct CI for 'm',  using the selected model's fit\n  m_ci <- confint(fit, 'x', level = 0.95)\n  m_covered[[b]] <- m_ci[[1]] < m && m < m_ci[[2]]\n}\n\ntime_end <- Sys.time()\ncat(\"Done :) Took \", as.numeric(time_end - time_start), \" seconds.\")\n\nDone :) Took  12.12071  seconds.\n\nThe variable m_covered[[b]] is TRUE if the slope \\(m\\) fell in the\nnaive CI \\((m_-, m_+)\\) defined by Eq. (2) in the\nb-th replica of the simulation. Hence, the actual coverage fraction of\nthe CI is given by:\n\n\nmean(m_covered)  # Actual coverage of naive \"95%\" CIs.\n\n[1] 0.9172\n\n92%! If this difference from the nominal 95% coverage guarantee does not\nstrike you as enormous, think about it in these terms: the naive CIs\n(2) fail to cover the true parameter about 8% of the\ntimes; This is a relative +60% of failures with respect to an honest 95%\nCI.\nWhat‚Äôs going on\nWe can understand a bit better what‚Äôs happening here by decomposing the\ncoverage probability as follows:\n\\[\n\\text {Pr}(m \\in \\text{CI})  = \\text {Pr}(m \\in \\text{CI}_{q \\text{ dropped}}\\,\\vert\\,q \\text{ dropped})\\cdot \\text {Pr}(q \\text{ dropped}) +\\\\ +\\text {Pr}(m \\in \\text{CI}_{q  \\text{ kept}}\\,\\vert\\,q \\text{ kept})\\cdot \\text {Pr}(q \\text{ kept})\n\\tag{3}\n\\]\nThe right hand side of this equation shows how our selective modeling\nprocedure alters the probability \\(\\text{Pr}(m\\in \\text{CI})\\). There are\ntwo contributing factors here: the probability of dropping the intercept\nterm, and the covering probabilities of the CIs constructed in the two\ncases (\\(\\text{CI}_{q \\text{ dropped}}\\) and\n\\(\\text{CI}_{q \\text{ kept}}\\)). We can estimate all these\nprobabilities as:\n\n\nmean(q_dropped)  # Pr(q dropped)\n\n[1] 0.6782\n\nmean(m_covered[q_dropped])  # Pr(m covered | q dropped)\n\n[1] 0.9510469\n\nmean(m_covered[!q_dropped])  # Pr(m covered | q kept)\n\n[1] 0.845867\n\nThe first result directly follows from our procedure, which uses a\nhypothesis test with significance \\(\\alpha = 32\\%\\) to test the (true)\nnull hypothesis \\(q = 0\\). It is a bit harder but in fact possible to\nprove that4\n\\(\\text {Pr}(m \\in \\text{CI}_{q \\text{ dropped}}\\,\\vert\\,q \\text{ dropped}) = 95\\%\\),\nas the second estimate would seem to suggest. The third result is\nfinally what invalidates the naive coverage guarantee in Eq.\n(2).\nConcluding Remarks\nTo summarize:\nWe started with two linear models for \\(Y\\) vs.¬†\\(X\\), which were in fact both well-specified (that is, correct).\nWe stipulated to choose one of the two models by testing the null hypothesis \\(q = 0\\).\nAfter selection, we constructed \\(95\\%\\) confidence intervals for the slope\n\\(\\hat m\\) using the selected model, as if this had been fixed in advance.\nA simulation shows that such intervals have a true coverage probability of\n\\(\\approx 92\\%\\).\nThe mathematical explanation of the last result is provided by Eq. (3), while the (hopefully) plain English one in the introductory part of this post. I will conclude with a few parenthetical remarks.\nFirst, the selective procedure proposed here would likely hardly be applied in practice in such a simple situation5. However, one could easily think of a more complex scenario with multiple covariates, where eliminating redundant ones could turn out to be beneficial for interpretation (if not compulsory, if the number of covariates exceeds the sample size).\nSecond, in order to avoid cluttering the discussion with too much\ntechnicalities, I have deliberately chosen a quite special point in true-model space (\\(q = 0\\)). This implies that both fits with and without intercept estimate the same slope \\(m\\); this is a peculiar property of \\(q = 0\\), which would not be true in the general case \\(q \\in \\mathbb R\\). In general, we would have to carefully define the inferential targets for the \\(q=0\\) and \\(q \\in \\mathbb R\\) cases, in a differential manner.\nConclusion\nThat was all for today. In the next post, I will discuss some mathematical details\nregarding the formulation of the Selective Inference problem in model-building.\nFor those surviving down to the bottom of the funnel, my future plan is to\nreview some (valid) selective inference methods I found interesting, including:\nBenjamini-Yekutieli control of False Coverage Rate (Benjamini and Yekutieli 2005),\nPOSI bounds for marginal coverage (Berk et al. 2013),\nData Fission, an elegant generalization of good old data splitting (Leiner et al. 2021).\n‚Ä¶whatever cool stuff I may discover in the meantime.\nCiao!\n\n\n\nBenjamini, Yoav, and Daniel Yekutieli. 2005. ‚ÄúFalse Discovery Rate‚ÄìAdjusted Multiple Confidence Intervals for Selected Parameters.‚Äù Journal of the American Statistical Association 100 (469): 71‚Äì81.\n\n\nBerk, Richard, Lawrence Brown, Andreas Buja, Kai Zhang, and Linda Zhao. 2013. ‚ÄúValid Post-Selection Inference.‚Äù The Annals of Statistics, 802‚Äì37.\n\n\nIsidori, Gino, Davide Lancierini, Patrick Owen, and Nicola Serra. 2021. ‚ÄúOn the Significance of New Physics in b‚Üí S‚Ñì+ ‚Ñì- Decays.‚Äù Physics Letters B 822: 136644.\n\n\nLeiner, James, Boyan Duan, Larry Wasserman, and Aaditya Ramdas. 2021. ‚ÄúData Fission: Splitting a Single Data Point.‚Äù arXiv. https://doi.org/10.48550/ARXIV.2112.11079.\n\n\nShalizi, C. R. 2020. ‚ÄúPost-Model-Selection Inference.‚Äù 2020. http://bactra.org/notebooks/post-model-selection-inference.html.\n\n\nVrbik, Jan. 2020. ‚ÄúRegression Analysis (Lecture Notes).‚Äù 2020. http://spartan.ac.brocku.ca/~jvrbik/MATH3P82/notes.pdf.\n\n\nHere, in the ‚Äúextended family of models‚Äù, I‚Äôm also implicitly\naccounting for the multiplicity introduced by continuous model\nparameters and training parameters (also known as hyper-parameters).‚Ü©Ô∏é\nThe preferential method according to (Shalizi 2020), from which\nI borrowed the ‚Äúa-theoretical‚Äù description, and which I recommend as a starting point for literature review.‚Ü©Ô∏é\nThis is not to say that correctly accounting for Selective Inference is\nthe default in scientific practice. A relevant example from the field I come\nfrom (Particle Physics), is documented in this stimulating reference:\n(Isidori et al. 2021).‚Ü©Ô∏é\nI‚Äôm always amazed by the great deal of theory one can learn by\nrunning a dumb simulation, and trying to explain a posteriori what\nseems to be a too perfect result. Technically, this follows from the fact that the slope estimate\n\\(\\hat m\\) and residual sum of squares \\(\\text{RSS}\\) of the reduced\nmodel, and the \\(F\\)-statistic used to test \\(q = 0\\), are all\nindependent random variables under the same null hypothesis, here\ntrue by construction. All these facts are in turn consequences of\ngeneral theorems from linear model theory, see for example\n(Vrbik 2020, chap. 4)‚Ä¶ and, to be sure, it took me more than a single\nnight without sleep to figure all this out.‚Ü©Ô∏é\nAnd I‚Äôm actually not sure that, after properly taking into account Selective Inference, it would lead to a substantial gain in estimation accuracy, compared to simply fitting the possibly redundant model with intercept.‚Ü©Ô∏é\n",
    "preview": "posts/2022-10-18-posi/posi_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-11-14T19:30:52+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-11-13-kgrams-v012-released/",
    "title": "kgrams v0.1.2 on CRAN",
    "description": "kgrams: Classical k-gram Language Models in R.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2021-11-13",
    "categories": [
      "R"
    ],
    "contents": "\nSummary\nVersion v0.1.2 of my R package kgrams was just accepted by CRAN. This package provides tools for training and evaluating k-gram language models in R, supporting several probability smoothing techniques, perplexity computations, random text generation and more.\nShort demo\n\n\nlibrary(kgrams)\n# Get k-gram frequency counts from Shakespeare's \"Much Ado About Nothing\"\nfreqs <- kgram_freqs(kgrams::much_ado, N = 4)\n\n# Build modified Kneser-Ney 4-gram model, with discount parameters D1, D2, D3.\nmkn <- language_model(freqs, smoother = \"mkn\", D1 = 0.25, D2 = 0.5, D3 = 0.75)\n\n# Sample sentences from the language model at different temperatures\nset.seed(840)\nsample_sentences(model = mkn, n = 3, max_length = 10, t = 1)\n\n\n[1] \"i have studied eight or nine truly by your office [...] (truncated output)\"\n[2] \"ere you go : <EOS>\"                                                        \n[3] \"don pedro welcome signior : <EOS>\"                                         \n\nsample_sentences(model = mkn, n = 3, max_length = 10, t = 0.1)\n\n\n[1] \"i will not be sworn but love may transform me [...] (truncated output)\" \n[2] \"i will not fail . <EOS>\"                                                \n[3] \"i will go to benedick and counsel him to fight [...] (truncated output)\"\n\nsample_sentences(model = mkn, n = 3, max_length = 10, t = 10)\n\n\n[1] \"july cham's incite start ancientry effect torture tore pains endings [...] (truncated output)\"   \n[2] \"lastly gallants happiness publish margaret what by spots commodity wake [...] (truncated output)\"\n[3] \"born all's 'fool' nest praise hurt messina build afar dancing [...] (truncated output)\"          \n\nNEWS\nOverall Software Improvements\nThe package‚Äôs test suite has been greatly extended.\nImproved error/warning conditions for wrong arguments.\nRe-enabled compiler diagnostics as per CRAN policy (#19)\nAPI Changes\nverbose arguments now default to FALSE.\nprobability(), perplexity() and sample_sentences() are restricted to accept only language_model class objects as their model argument.\nNew features\nas_dictionary(NULL) now returns an empty dictionary.\nBug Fixes\nFixed bug causing .preprocess and .tknz_sent arguments to be ignored in process_sentences().\nFixed previously wrong defaults for max_lines and batch_size arguments in kgram_freqs.connection().\nAdded print method for class dictionary.\nFixed bug causing invalid results in dictionary() with batch processing and non-trivial size constraints on vocabulary size.\nOther\nMaintainer‚Äôs email updated\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-17T08:21:30+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-25-r-client-for-r-universe-apis/",
    "title": "R Client for R-universe APIs",
    "description": "Introducing W.I.P. {runiv}, an R package to interact with R-universe \nrepository APIs",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2021-07-25",
    "categories": [
      "R"
    ],
    "contents": "\nIntroduction\nFollowing my previous post on how to use your R-universe API to automatically generate a list of the packages on your R-universe, I started working on a simple R client to interact with such APIs.\nFor those who missed it, R-universe is a new project from rOpenSci that allows you to mantain a personal CRAN-like repository, which automatically syncs with the GitHub repositories hosting your projects.\nAmong other features, each repository has associated a RESTful API with which users can interact for managing and retrieving informations about packages in the repo. Quoting R-universe:\n\nThe package server provides REST APIs for managing package submissions and querying information about individual packages as well as on the repository level. These data can be accessed programmatically or displayed in a front-end dashboard.\n\n{runiv}\nSince this has already proved to be useful to me (and could hopefully be so also to others), I started playing around to implement an R client for R-universe APIs. The package is called runiv and the code is here. Up to now, only a small subset of the full API features are available. You can peek at the development version from GitHub, using:\nremotes::install_github(\"vgherard/runiv\")\nFor instance, the procedure for obtaining your packages DESCRIPTION outlined in my previous post is performed by:\n\n\ndf <- runiv::runiv_descriptions(\"vgherard\") # 'vgherard' is my R-universe name.\n\n\n\ndf is a dataframe containing all the entries of the DESCRIPTION files of my packages:\n\n\ndf[, c(\"Package\", \"Title\")]\n\n\n   Package                                             Title\n1      r2r                    R-Object to R-Object Hash Maps\n2   kgrams                  Classical k-gram Language Models\n3 scribblr                          A Notepad Inside RStudio\n4  gsample   Efficient Weighted Sampling Without Replacement\n5      sbo Text Prediction via Stupid Back-Off N-Gram Models\n6     fcci              Feldman-Cousins Confidence Intervals\n\ndf[1, \"Description\"] |> strtrim(60) |> paste(\"[...]\")\n\n\n[1] \"Implementation of hash tables (hash sets and hash maps) in R [...]\"\n\nConclusion\nI hope you find this useful. I have very little experience with web API R packages (this was another personal reason to tackle this), so that if you have any suggestion, or maybe want to collaborate on runiv, you are welcome to reach out to me through GitHub.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-13T16:53:50+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-21-automatically-resume-your-r-package-portfolio-using-the-r-universe-api/",
    "title": "Automatic resumes of your R-developer portfolio from your R-Universe",
    "description": "Create automatic resumes of your R packages using the R-Universe API.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2021-07-21",
    "categories": [
      "R"
    ],
    "contents": "\nHi R-bloggers üëã\nStarting from today, all posts from this blog in the R category will also appear on R-bloggers. I would like to thank Tal for aggregating my blog, and say ‚Äúhi!‚Äù to all R-bloggers readers. I‚Äôm a particle physicist with a passion for R, Statistics and Machine Learning. If you want to find out something more about me, you can take a look at my website, and links therein.\nIntroduction\nR-universe is a cool initiative from rOpenSci, which allows you to create your own CRAN-like repository. The latter is synced with the GitHub repositories (main or specific branches, or releases) associated to your R packages, so that using an R-universe is a very effortless way to organize and share your personal package ecosystem.\nIf you want to setup your own R-universe, follow the instructions in this blog post. In this post, I assume that you have created your own R-universe, and show you how to retrieve metadata on your packages using the R-universe API.\nRetrieving packages descriptions from your R-universe API\nOnce you will have it set up, your R-universe will be available at the URL your-user-name.r-universe.dev. For instance, mine is vgherard.r-universe.dev. From your R-universe home page, you can access the documentation of the API. We will use the command:\nGET /stats/descriptions\n    NDJSON stream with data from package DESCRIPTION files.\nThe JSON stream can be read with jsonlite, as follows:\n\n\ncon <- url(\"https://vgherard.r-universe.dev/stats/descriptions\")\npkgs <- jsonlite::stream_in(con)\n\n\n\n Found 6 records...\n Imported 6 records. Simplifying...\n\nThe result is a dataframe with alll the entries of your packages‚Äô DESCRIPTION file, e.g.:\n\n\npkgs[, c(\"Package\", \"Title\", \"Version\")]\n\n\n   Package                                             Title\n1      r2r                    R-Object to R-Object Hash Maps\n2   kgrams                  Classical k-gram Language Models\n3 scribblr                          A Notepad Inside RStudio\n4  gsample   Efficient Weighted Sampling Without Replacement\n5      sbo Text Prediction via Stupid Back-Off N-Gram Models\n6     fcci              Feldman-Cousins Confidence Intervals\n     Version\n1 0.1.1.9000\n2      0.1.0\n3 0.2.0.9000\n4      0.1.0\n5      0.5.0\n6      1.0.0\n\nI use this query on my personal website to automatically generate a resume of the packages available on my R-universe (this is combined with a GitHub Action scheduled workflow which periodically updates the Code section of my website). More precisely, I define an R string txt containing the Markdown code for my resume, and I inline it in R Markdown using the synthax `r `. This is the code I use on my website:\n\n\ntxt <- \"\"\nfor (i in seq_len(nrow(pkgs))) {\n  txt <- paste0(\n    txt, \n    \"### [`\", pkgs[i, \"Package\"], \"`](\", pkgs[i, \"RemoteUrl\"], \")\", \"\\n\",\n    \"[![CRAN status](https://www.r-pkg.org/badges/version/\", pkgs[i,\"Package\"],\n    \")](https://CRAN.R-project.org/package=\",pkgs[i, \"Package\"], \")\",\n    \"\\n\\n\",\n    \"*\", pkgs[i, \"Title\"], \".* \", pkgs[i, \"Description\"],\n    \"\\n\\n\"\n    )\n}\n\n\n\nand this is the output:\nr2r\n\nR-Object to R-Object Hash Maps. Implementation of hash tables (hash sets and hash maps) in R, featuring arbitrary R objects as keys, arbitrary hash and key-comparison functions, and customizable behaviour upon queries of missing keys.\nkgrams\n\nClassical k-gram Language Models. Tools for training and evaluating k-gram language models in R, supporting several probability smoothing techniques, perplexity computations, random text generation and more.\nscribblr\n\nA Notepad Inside RStudio. A project aware notepad inside RStudio, for taking quick project-related notes without distractions. RStudio addin.\ngsample\n\nEfficient Weighted Sampling Without Replacement. Sample without replacement using the Gumbel-Max trick (c.f. ).\nsbo\n\nText Prediction via Stupid Back-Off N-Gram Models. Utilities for training and evaluating text predictors based on Stupid Back-Off N-gram models (Brants et al., 2007, https://www.aclweb.org/anthology/D07-1090/).\nfcci\n\nFeldman-Cousins Confidence Intervals. Provides support for building Feldman-Cousins confidence intervals [G. J. Feldman and R. D. Cousins (1998) doi:10.1103/PhysRevD.57.3873].\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-13T16:53:50+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-06-r2r/",
    "title": "{r2r} now on CRAN",
    "description": "Introducing {r2r}, an R implementation of hash tables.",
    "author": [
      {
        "name": "vgherard",
        "url": {}
      }
    ],
    "date": "2021-07-06",
    "categories": [
      "R"
    ],
    "contents": "\nIntroduction\nMy package {r2r} (v0.1.1) has been accepted by CRAN, and is now available for download from the public repository.\nr2r\n\n   \nr2r provides a flexible implementation of hash tables in R, allowing for:\narbitrary R objects as keys and values,\narbitrary key comparison and hash functions,\ncustomizable behaviour (throw or return a default value) on missing key exceptions.\nInstallation\nYou can install the released version of r2r from CRAN with:\ninstall.packages(\"r2r\")\nand the development version from my R-universe repository, with:\ninstall.packages(\"r2r\", repos = \"https://vgherard.r-universe.dev\")\nUsage\n\n\nlibrary(r2r)\nm <- hashmap()\n\n# Insert and query a single key-value pair\nm[[ \"user\" ]] <- \"vgherard\"\nm[[ \"user\" ]]\n\n\n[1] \"vgherard\"\n\n# Insert and query multiple key-value pairs\nm[ c(1, 2, 3) ] <- c(\"one\", \"two\", \"three\")\nm[ c(1, 3) ]\n\n\n[[1]]\n[1] \"one\"\n\n[[2]]\n[1] \"three\"\n\n# Keys and values can be arbitrary R objects\nm[[ lm(mpg ~ wt, mtcars) ]] <- c(TRUE, FALSE, TRUE)\nm[[ lm(mpg ~ wt, mtcars) ]]\n\n\n[1]  TRUE FALSE  TRUE\n\nGetting help\nFor further details, including an introductory vignette illustrating the features of r2r hash maps, you can consult the r2r website. If you encounter a bug, want to suggest a feature or need further help, you can open a GitHub issue.\nComparison with hash\nCRAN package {hash} also offers an implementation of hash tables based on R environments. The two tables below offer a comparison between {r2r} and {hash} (for more details, see the benchmarks Vignette)\n\nTable 1: Features supported by {r2r} and {hash}.\nFeature\nr2r\nhash\nBasic data structure\nR environment\nR environment\nArbitrary type keys\nX\n\nArbitrary type values\nX\nX\nArbitrary hash function\nX\n\nArbitrary key comparison function\nX\n\nThrow or return default on missing keys\nX\n\nHash table inversion\n\nX\n\n\nTable 2: Performances of {r2r} and {hash} for basic hash table operations.\nTask\nComparison\nKey insertion\n{r2r} ~ {hash}\nKey query\n{r2r} < {hash}\nKey deletion\n{r2r} << {hash}\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-13T16:53:50+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-06-test-post/",
    "title": "Test post",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "vgherard",
        "url": {}
      }
    ],
    "date": "2021-07-06",
    "categories": [
      "Other"
    ],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-13T16:53:50+01:00",
    "input_file": {}
  }
]
