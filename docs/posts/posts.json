[
  {
    "path": "posts/2023-11-15-interpreting-the-likelihood-ratio-cost/",
    "title": "Interpreting the Likelihood Ratio cost",
    "description": "Analysis of infinite sample properties and comparison with cross-entropy loss.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-11-15",
    "categories": [
      "Forensic Science",
      "Bayesian Methods",
      "Information Theory",
      "Probability Theory",
      "R"
    ],
    "contents": "\nIntro\nDuring the last few months, I’ve been working on a machine learning algorithm with applications in Forensic Science, a.k.a. Criminalistics.\nIn this field, one common task for the data analyst is to present the trier-of-fact (the person or people who determine the facts in a legal proceeding) with a numerical assessment of the strength of the evidence provided by available data towards different hypotheses. In more familiar terms, the forensic expert is responsible of computing the likelihoods (or likelihood ratios) of data under competing hypotheses, which are then used by the trier-of-fact to produce Bayesian posterior probabilities for the hypotheses in question1.\nIn relation to this, forensic scientists have developed a bunch of techniques to evaluate the performance of a likelihood ratio model in discriminating between two alternative hypothesis. In particular, I have come across the so called Likelihood Ratio Cost, usually defined as:\n\\[\nC_{\\text{LLR}} = \\frac{1}{2N_1} \\sum _{Y_i=1} \\log(1+r(X_i) ^{-1})+\\frac{1}{2N_0} \\sum _{Y_i=0} \\log(1+r(X_i)), \\tag{1}\n\\]\nwhere we assume we have data consisting of \\(N_1+N_0\\) independent identically distributed observations \\((X_i,\\,Y_i)\\), with binary \\(Y\\); \\(N_1\\) and \\(N_0\\) stand for the number of positive (\\(Y=1\\)) and negative (\\(Y=0\\)) cases; and \\(r(X)\\) is a model for the likelihood ratio \\(\\Lambda(X) \\equiv \\frac{\\text{Pr}(X\\vert Y = 1)}{\\text{Pr}(X\\vert Y = 0)}\\).\nThe main reason for writing this note was to understand a bit better what it means to optimize Eq. (1), which does not look immediately obvious to me from its definition2. In particular: is the population minimizer of Eq. (1) the actual likelihood ratio? And in what sense is a model with lower \\(C_\\text{LLR}\\) better than one with a correspondingly higher value?\nThe short answers to these questions are: yes; and: \\(C_\\text{LLR}\\) optimization seeks for the model with the best predictive performance in a Bayesian inference setting with uninformative prior on \\(Y\\), assuming that this prior actually reflects reality (i.e. \\(\\text{Pr}(Y=1) = \\text{Pr}(Y=0) = \\frac{1}{2}\\)). The mathematical details are given in the rest of the post.\n\n\nCross-entropy with random weights\nWe start with a mathematical digression, which will turn out useful for further developments. Let \\(\\{(X_i,\\,Y_i)\\}_{i=1,\\,2,\\,\\dots,N}\\) be independent draws from a joint distribution, with binary \\(Y_i \\in \\{0,\\,1\\}\\). Given a function\n\\(w=w(\\boldsymbol Y)\\) that is symmetric in its arguments3, we define the random functional:\n\\[\n\\mathcal L_N^w[f] = -\\frac{1}{N}\\sum_{i=1} ^N \\left[w(\\boldsymbol Y)Y_i \\log(f(X_i))+ w({\\boldsymbol Y}^c)( Y_i^c) \\log(f(X_i)^c)\\right],\\tag{2}\n\\]\nwhere \\(f=f(X)\\) is any function satisfying \\(f(X)\\in [0,\\,1]\\) for all \\(X\\), and we let \\(q^c = 1-q\\) for any number \\(q \\in [0,\\,1]\\). Notice that for \\(w(\\boldsymbol{Y}) \\equiv 1\\), this is just the usual cross-entropy loss.\nWe now look for the population minimizer of (2), i.e. the function \\(f_*\\) that minimizes the functional \\(f \\mapsto \\mathbb E(\\mathcal L _N ^w [f])\\)4. Writing the expectation as:\n\\[\n\\mathbb E(\\mathcal L _N ^w [f]) = -\\frac{1}{N}\\sum _{i=1} ^N \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y)\\vert X_i)\\cdot \\log(f(X_i))+E(Y_i^c\\cdot w(\\boldsymbol Y ^c)\\vert X_i)\\cdot \\log(f^c(X_i))\\right],\n\\]\nwe can easily see that \\(\\mathbb E(\\mathcal L _N ^w [f])\\) is a convex functional with a unique minimum given by:\n\\[\nf_*(X_i) = \\frac{1}{1+r(X_i)^{-1}},\\quad r_*(X_i) = \\dfrac{E(Y_i\\cdot w(\\boldsymbol Y)\\vert X_i)}{E(Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)}.\\tag{3}\n\\]\nThe corresponding expected loss is:\n\\[\n\\mathbb E(\\mathcal L _N ^w [f_*]) = \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal H(f_*(X_i))\\right],\n\\]\nwhere \\(\\mathcal H(p) = -p \\log (p) -(1-p) \\log(1-p)\\) is the entropy of a binary random variable \\(Z\\) with probability \\(p = \\text{Pr}(Z=1)\\) (the index \\(i\\) in the previous expression can be any index, since data points are assumed to be identically distributed).\nBefore looking at values of \\(f\\) other than \\(f_*\\), we observe that the previous expectation can be succintly expressed as:\n\\[\n\\mathbb E(\\mathcal L _N ^w [f_*]) = k \\cdot H^\\prime(Y\\vert X),\n\\]\nwhere\n\\[\nk = \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c))\\tag{4}\n\\]\nand \\(H'(Y\\vert X)\\) is the conditional entropy of \\(Y\\vert X\\) with respect to a different probability measure \\(\\text{Pr}^\\prime\\), defined by:\n\\[\n\\text{Pr}^\\prime(E) = t \\cdot \\text {Pr}(E \\vert Y = 1) + (1-t)\\cdot \\text {Pr}(E \\vert Y = 0), \\tag{5}\n\\]\nwhere \\(t=\\text{Pr}^\\prime(Y=1)\\in [0,\\,1]\\) is fixed by the requirement5:\n\\[\n\\dfrac{\\text {Pr}^\\prime (Y=1)}{\\text{Pr}^\\prime (Y=0)}=\\dfrac{\\text {Pr} (Y=1)}{\\text{Pr} (Y=0)}\\cdot\\dfrac{\\mathbb E(w(\\boldsymbol Y)\\vert \\sum _i Y_i >0)}{\\mathbb E(w(\\boldsymbol Y^c)\\vert \\sum _i Y_i^c >0)}. \\tag{6}\n\\]\nIn terms of \\(\\text{Pr}^\\prime\\), the population minimizers \\(f_*\\) and \\(r_*\\) in Eq. (3) can be simply expressed as:\n\\[\nr_*(X)=\\dfrac{\\text {Pr}^\\prime(Y=1\\vert X)}{\\text {Pr}^\\prime(Y=0\\vert X)},\\qquad f_*(X)=\\text {Pr}^\\prime(Y=1\\vert X). \\tag{7}\n\\]\nIf now \\(f\\) is an arbitrary function, we have:\n\\[\n\\begin{split}\n\\mathbb E(\\mathcal L _N ^w [f]) - \\mathbb E(\\mathcal L _N ^w [f_*]) &= \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot \\mathcal D(f_*(X_i)\\vert \\vert f(X_i))\\right]\n&= k\\cdot D(\\text{Pr}^\\prime\\vert \\vert \\text{Pr}^\\prime _f)\n\\end{split}\n\\]\nwhere \\(\\mathcal D(p\\vert \\vert q) = p \\log (\\frac{p}{q}) + (1-p) \\log (\\frac{1-p}{1-q})\\), and \\(D(\\text{Pr}^\\prime\\vert \\vert \\text{Pr}^\\prime _f)\\) is the Kullback-Liebler divergence between the measure \\(\\text{Pr}^\\prime\\) and the measure \\(\\text{Pr}^\\prime _f\\) defined by:\n\\[\n\\text{Pr}^\\prime _f(Y = 1\\vert X)=f(X),\\qquad \\text{Pr}^\\prime _f(X)=\\text{Pr}^\\prime(X)\n\\]\n(notice that \\(\\text {Pr} ^{\\prime} _{f_*} \\equiv \\text{Pr} ^{\\prime}\\) by definition).\nFinally, suppose that \\(X = g(\\widetilde X)\\) for some random variable \\(\\widetilde X\\), and define the corresponding functional:\n\\[\n\\widetilde{\\mathcal L} _N^w[\\widetilde f]  = -\\frac{1}{N}\\sum_{i=1} ^N \\left[w(\\boldsymbol Y)Y_i \\log(\\widetilde f(\\widetilde X))+ w({\\boldsymbol Y}^c)( Y_i^c) \\log(\\widetilde f(\\widetilde X)^c)\\right].\n\\]\nThen \\(\\mathcal L _N ^w [f] = \\widetilde{\\mathcal L} _N^w[f \\circ g]\\). If \\(\\widetilde f _* =\\) is the population minimizer of \\(\\widetilde{\\mathcal L} _N^w\\), it follows that \\(\\mathbb E (\\widetilde{\\mathcal L} _N^w[\\widetilde f _*]) \\leq \\mathbb E(\\mathcal L _N ^w [f_*])\\).\nPutting everything together, we can decompose the expected loss for a function \\(f=f(X)\\), where \\(X= g(\\widetilde X)\\), in the following suggestive way:\n\\[\n\\begin{split}\n\\mathbb E(\\mathcal L _N ^w [f]) &= (L_N ^w)_\\text{min}+(L_N ^w)_\\text{proc} +(L_N ^w)_\\text{missp},\\\\\n(L_N ^w)_\\text{min}&\\equiv\\mathbb E(\\widetilde{\\mathcal L} _N^w[{\\widetilde f} _*])  \\\\ &=\n\\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert \\widetilde X _i)\\cdot \\mathcal H({\\widetilde f} _*(\\widetilde X _i))\\right]\\\\\n&=k\\cdot H^\\prime(Y\\vert \\widetilde X),\\\\\n(L_N ^w)_\\text{proc}&\\equiv\\mathbb E(\\mathcal L _N ^w [f_*]-\\widetilde{\\mathcal L} _N^w[\\phi_*])  \\\\& =\n\\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot  \\mathcal H(f_*(X_i))\n\\right]- (L_N ^w)_\\text{min}\\\\\n& = k\\cdot I^\\prime(Y; \\widetilde X\\vert X),\\\\\n(L_N ^w)_\\text{missp} & \\equiv \\mathbb E(\\mathcal L _N ^w [f]) - \\mathbb E(\\mathcal L _N ^w [f_*]) \\\\&= \\mathbb E\\left[ \\mathbb E(Y_i\\cdot w(\\boldsymbol Y) + Y_i^c\\cdot w(\\boldsymbol Y^c)\\vert X_i)\\cdot  \\mathcal  D(f_*(X_i)\\vert \\vert f(X_i))\\right]\\\\ &=k\\cdot D(\\text {Pr}^\\prime\\vert \\vert \\text {Pr}^\\prime _f),\n\\end{split} \\tag{8}\n\\]\nwhere \\(k\\) is defined in Eq. (4). In the equation for \\((L^w _N)_\\text{proc}\\) we introduced the conditional mutual information (with respect to the measure \\(\\text{Pr}^\\prime\\)), that satisfies (Cover and Thomas 2006):\n\\[\nI(\\widetilde X;Y\\vert X) = I(\\widetilde X,Y)-I(X,Y) = H(Y\\vert X)-H(Y\\vert \\widetilde X).\n\\]\nThe three components in Eq. (8) can be interpreted as follows: \\((L_N ^w)_\\text{min}\\) represents the minimum expected loss achievable, given the data available \\(\\widetilde X\\); \\((L_N ^w)_\\text{proc}\\) accounts for the information lost in the processing transformation \\(X=g(\\widetilde X)\\); finally \\((L_N ^w)_\\text{missp}\\) is due to misspecification, i.e. the fact that the model \\(f(X)\\) for the true posterior probability \\(f_*(X)\\) is an approximation.\nAll the information-theoretic quantities (and their corresponding operative interpretations hinted in the previous paragraph) make reference to the measure \\(\\text{Pr}^\\prime\\) defined by Eqs. (5) and (6). This is merely the result of altering the proportion of positive (\\(Y=1\\)) and negative (\\(Y=0\\)) examples in the \\(X\\)-\\(Y\\) joint distribution by a factor dictated by the weight function \\(w\\) - while keeping conditional distributions such as \\(X\\vert Y\\) unchanged.\nA familiar case: cross-entropy loss\nFor \\(w(\\boldsymbol {Y}) = 1\\), the functional \\(\\mathcal {L} _{N} ^{w}[f]\\)\ncoincides with the usual cross-entropy loss6:\n\\[\n\\text{CE}[f] = -\\frac{1}{N}\\sum_{i=1} ^N \\left[Y_i \\log(f(X_i))+ (1-Y_i) \\log(1-f(X_i))\\right].\\tag{9}\n\\]\nFrom Eq. (6) we see that the measure \\(\\text{Pr}^{\\prime}\\)\ncoincides with the original \\(\\text{Pr}\\), so that by Eq. (3)\nthe population minimizer of (9) is\n\\(f_{*}(X) = \\text{Pr}(Y=1\\vert X)\\) (independently of sample size). Since \\(k = 1\\) (cf. Eq. (4)), the decomposition (8) reads:\n\\[\n\\begin{split}\n\\mathbb E(\\text{CE} [f]) &= (\\text{CE})_\\text{min}+(\\text{CE})_\\text{proc} +(\\text{CE})_\\text{missp},\\\\\n(\\text{CE})_\\text{min}&=H(Y\\vert \\widetilde X),\\\\\n(\\text{CE})_\\text{proc}&= I(Y; \\widetilde X\\vert X),\\\\\n(\\text{CE})_\\text{missp} &=D(\\text {Pr}\\vert \\vert \\text {Pr} _{f}),\n\\end{split} \\tag{10}\n\\]\nwhere conditional entropy \\(H\\), mutual information \\(I\\) and relative entropy \\(D\\) now simply refer to the original measure \\(\\text{Pr}\\).\nThe Likelihood Ratio Cost\nThe quantity \\(C_{\\text{LLR}}\\) defined in Eq. (1) can be put in the general form (2), if we let \\(f(X) = (1+r(X)^{-1})^{-1}\\) and7:\n\\[\nw(\\boldsymbol Y) = \\left(\\dfrac{2}{N}\\sum _{i = 1}^{N}Y_j \\right)^{-1}\n\\]\nIn what follows, I will consider a slight modification of the usual\n\\(C_\\text{LLR}\\), defined by the weight function:\n\\[\nw(\\boldsymbol Y) = \\dfrac{1}{2(N-1)}\\sum _{i = 1}^{N}(1-Y_j).\n\\]\nThis yields Eq. (1) multiplied by \\(\\dfrac{N_1N_0}{N(N-1)}\\), which I will keep denoting as \\(C_\\text{LLR}\\), with a slight abuse of notation.\nWe can easily compute8:\n\\[\n\\dfrac{\\text {Pr}^\\prime (Y=1)}{\\text{Pr}^\\prime (Y=0)}=1, \\tag{11}\n\\]\nso that, by Eq. (3), the population minimizer of\n\\(C_\\text{LLR}\\) is:\n\\[\nr_*(X) = \\Lambda (X),\\quad f_*(X)=\\dfrac{1}{1+\\Lambda(X)^{-1}},\n\\]\nwhere \\(\\Lambda(X)\\) denotes the likelihood-ratio of \\(X\\), schematically:\n\\[\n\\Lambda(X)\\equiv \\dfrac{\\text{Pr}(X\\vert Y = 1)}{\\text{Pr}(X\\vert Y = 0)}.\n\\]\nThe constant \\(k\\) in Eq. (4) is:\n\\[\nk = \\text{Pr}(Y = 1)\\text{Pr}(Y = 0)=\\text{Var}(Y)\n\\]\nThe general decomposition (8) becomes:\n\\[\n\\begin{split}\n\\mathbb E(C_\\text{LLR} [f]) &= (C_\\text{LLR})_\\text{min}+(C_\\text{LLR})_\\text{proc} +(C_\\text{LLR})_\\text{missp},\\\\\n(C_\\text{LLR})_\\text{min}&=\\text{Var}(Y)\\cdot H^{\\prime}(Y\\vert \\widetilde X),\\\\\n(C_\\text{LLR})_\\text{proc}&= \\text{Var}(Y)\\cdot I^{\\prime}(Y; \\widetilde X\\vert X),\\\\\n(C_\\text{LLR})_\\text{missp} &=\\text{Var}(Y)\\cdot D^{\\prime}(\\text {Pr}\\vert \\vert \\text {Pr} _{f}),\n\\end{split} \\tag{10}\n\\]\nwhere \\(\\text{Pr}^\\prime\\) is now given by (11).\nDiscussion\nThe table below provides a comparison between cross-entropy and likelihood-ratio cost,\nsummarizing the results from previous sections.\n\nCross-entropy\nLikelihood Ratio Cost\n\\(f_*(X)\\)\n\\(\\text{Pr}(Y = 1\\vert X)\\)\n\\((1+\\Lambda(X)^{-1})^{-1}\\)\n\\(r_*(X)\\)`\nPosterior odds ratio\nLikelihood ratio\nMinimum Loss\n\\(H(Y\\vert \\widetilde X)\\)\n\\(\\text{Var}(Y) \\cdot H^\\prime(Y\\vert \\widetilde X)\\)\nProcessing Loss\n\\(I(Y; \\widetilde X\\vert X)\\)\n\\(\\text{Var}(Y) \\cdot I^\\prime(Y; \\widetilde X\\vert X)\\)\nMisspecification Loss\n\\(D(f_*\\vert\\vert f)\\)\n\\(\\text{Var}(Y) \\cdot D^\\prime(f_*\\vert\\vert f)\\)\nReference measure\n\\(\\text{Pr}\\)\n\\(\\text{Pr}^{\\prime} = \\frac{\\text{Pr}(\\cdot \\vert Y = 1)+\\text{Pr}(\\cdot \\vert Y = 0)}{2}\\)\nThe objective of \\(C_\\text{LLR}\\) is found to be the likelihood ratio, as\nterminology suggests. The interpretation of model selection according to\n\\(C_\\text{LLR}\\) minimization turns out to be slightly more involved, compared to\ncross-entropy, which we first review.\nSuppose we are given a set of predictive models \\(\\{\\mathcal M_i\\}_{i\\in I}\\),\neach of which consists of a processing transformation, \\(\\widetilde X \\mapsto X\\),\nand an estimate of the posterior probability \\(\\text{Pr}(Y = 1\\vert X)\\).\nWhen the sample size \\(N \\to \\infty\\), cross-entropy minimization will almost\ncertainly select the model that minimizes\n\\(I(Y; \\widetilde X\\vert X) + D(f_*\\vert \\vert f)\\). Following standard\nInformation Theory arguments, we can interpret this model as the statistically\noptimal compression algorithm for \\(Y\\), assuming \\(X\\) to be available at both\nthe encoding and decoding ends.\nThe previous argument carries over mutatis mutandi to \\(C_\\text{LLR}\\)\nminimization, with an important qualification: optimal average compression is\nnow achieved for data distributed according to a different probability measure\n\\(\\text{Pr}'(\\cdot) = \\frac{1}{2}\\text {Pr}(\\cdot\\vert Y = 1) + \\frac{1}{2}\\text {Pr}(\\cdot\\vert Y = 0)\\). In particular, according to \\(\\text{Pr}'\\),\nthe likelihood ratio coincides with the posterior odds ratio, and\n\\((1+\\Lambda(X)^{-1})^{-1}\\) coincides with posterior probability, which clarifies\nwhy we can measure differences from the true likelihood-ratio through the\nKullback-Liebler divergence.\nThe measure \\(\\text{Pr}'\\) is not just an abstruse mathematical construct:\nit is the result of balanced sampling from the original distribution, i.e.\ntaking an equal number of positive and negative cases9. If the \\((X,\\,Y)\\) distribution is already balanced,\neither by design or because of some underlying symmetry in the data generating\nprocess, our analysis implies that likelihood-ratio cost and cross-entropy\nminimization are essentially equivalent for \\(N\\to \\infty\\). In general, with\n\\(\\text{Pr} (Y=1) \\neq \\text{Pr} (Y=0)\\), this is not the case10.\nThe fact that \\(C_\\text{LLR}\\) seeks for optimal predictors according to the balanced measure \\(\\text{Pr}'\\) is, one could argue, not completely crazy from the point of view of forensic science, where “\\(Y\\in\\{0,1\\}\\)” often stands for a sort verdict (guilty vs. not guilty, say). Indeed, optimizing\nwith respect to \\(\\text{Pr}^\\prime\\) means that our predictions are designed to be optimal in a world in which the verdict could be a priori \\(Y=0\\) or \\(Y=1\\) with equal probability - which is what an unbiased trier-of-fact should ideally assume. Minimizing \\(C_\\text{LLR}\\), we guard ourselves against any bias\nthat may be implicit in the training dataset, extraneous to the \\(X\\)-\\(Y\\)\nrelation and not explicitly modeled, a feature that may be regarded as desirable from a legal standpoint.\nSimulated example\nIn general, the posterior odd ratio and likelihood ratio differ only by a\nconstant, so it is reasonable to try to fit the same functional form to both of\nthem. Let us illustrate with a simulated example of this type the differences\nbetween cross-entropy and \\(C_{\\text{LLR}}\\) optimization mentioned in the\nprevious Section.\nSuppose that \\(X \\in \\mathbb R\\) has conditional density:\n\\[\n\\phi(X\\vert Y) = (2\\pi\\sigma _Y^2)^{-\\frac{1}{2}} \\exp(-\\frac{(X-\\mu_Y)^2}{2\\sigma _Y^2})\n\\]\nand \\(Y\\) has marginal probability \\(\\text{Pr}(Y = 1) = \\pi\\). The true likelihood-ratio and posterior odds ratio are respectively given by:\n\\[\n\\begin{split}\n\\Lambda (X) &\n    \\equiv \\frac{\\phi(X\\vert Y=1)}{\\phi(X\\vert Y=0)}\n    = e ^ {a X^2 + bX +c},\\\\\n\\rho (X) &\n    \\equiv \\frac{\\text{Pr}(Y = 1\\vert X)}{\\text{Pr}(Y = 0\\vert X)}\n    = e ^ {a X ^ 2 + bX +c+d},\n\\end{split}\n\\]\nwhere we have defined:\n\\[\na  \\equiv \\dfrac{\\sigma _1 ^2 -\\sigma_0 ^2}{2\\sigma _0 ^2\\sigma_1 ^2},\\quad\nb  \\equiv \\mu _1 - \\mu _0, \\quad\nc  \\equiv \\dfrac{\\mu_0^2}{2\\sigma_0^2} -\\dfrac{\\mu_1 ^2}{2\\sigma _1^2}+\\ln(\\frac{\\sigma _0 }{\\sigma _1 }),\\quad\nd  \\equiv \\ln (\\frac {\\pi}{1-\\pi}) .\n\\]\nSuppose that we fit an exponential function \\(r(X)=e^{mX +q}\\) to \\(\\Lambda(X)\\) by\nlikelihood-ratio cost minimization, and similarly \\(r'(X)=e^{m'X+q'}\\) to\n\\(\\rho(X)\\) by cross-entropy minimization11. Due to the previous discussion, one could reasonably expect the results of the two procedure to differ in some way, which is demonstrated below by simulation.\nThe chunk of R code below defines the function and data used for the simulation.\nIn particular, I’m considering a heavily unbalanced case\n(\\(\\text{Pr}(Y = 1) = 0.1\\%\\)) in which negative cases give rise to a sharply\nlocalized \\(X\\) peak around \\(X=0\\) (\\(\\mu _0 = 0\\), \\(\\sigma_0 = .25\\)),\nwhile the few positive cases give rise to a broader signal centered at \\(X=1\\)\n(\\(\\mu _1 = 1\\), \\(\\sigma _1 = 1\\)).\n\n\nShow code\n\n# Tidyverse facilities for plotting\nlibrary(dplyr)\nlibrary(ggplot2) \n\n# Loss functions\nweighted_loss <- function(par, data, w) {\n  m <- par[[1]]\n  q <- par[[2]]\n  x <- data$x\n  y <- data$y\n  \n  z <- m * x + q\n  p <- 1 / (1 + exp(-z))\n  \n  -mean(y * w(y) * log(p) + (1-y) * w(1-y) * log(1-p))\n}\n\ncross_entropy <- function(par, data) \n  weighted_loss(par, data, w = \\(y) 1)\n\ncllr <- function(par, data) \n  weighted_loss(par, data, w = \\(y) mean(1-y))\n\n\n# Data generating process\nrxy <- function(n, pi = .001, mu1 = 1, mu0 = 0, sd1 = 1, sd0 = 0.25) { \n  y <- runif(n) < pi\n  x <- rnorm(n, mean = y * mu1 + (1-y) * mu0, sd = y * sd1 + (1-y) * sd0)\n  data.frame(x = x, y = y)\n}\npi <- formals(rxy)$pi\n\n\n# Simulation\nset.seed(840)\ndata <- rxy(n = 1e6)\npar_cllr <- optim(c(1,0), cllr, data = data)$par\npar_cross_entropy <- optim(c(1,0), cross_entropy, data = data)$par\npar_cross_entropy[2] <- par_cross_entropy[2] - log(pi / (1-pi))\n\n\n# Helpers to extract LLRs from models\nllr <- function(x, par)\n  par[1] * x + par[2] \n\nllr_true <- function(x) {\n  mu1 <- formals(rxy)$mu1 \n  mu0 <- formals(rxy)$mu0 \n  sd1 <- formals(rxy)$sd1\n  sd0 <- formals(rxy)$sd0\n    \n  a <- 0.5 * (sd1 ^2 - sd0 ^2) / (sd1 ^2 * sd0 ^2)\n  b <- mu1 / (sd1^2) - mu0 / (sd0^2)\n  c <- 0.5 * (mu0^2 / (sd0^2) - mu1^2 / (sd1^2)) + log(sd0 / sd1)\n  a * x * x + b * x + c\n}\n\n\nSo, what do our best estimates look like? The plot below shows the best fit\nlines for the log-likelihood ratio from \\(C_{\\text{LLR}}\\) minimization (in solid red) and cross-entropy minimization (in solid blue). The true log-likelihood ratio parabola is the black line. Also shown are the \\(\\text{LLR}=0\\) line (in dashed red) and the \\(\\text{LLR}=\\ln(\\frac{1-\\pi}{\\pi})\\) (in\ndashed blue), which are the appropriate Bayes thresholds for classifying a\ndata point as positive (\\(Y=1\\)), assuming data comes from a balanced and unbalanced distribution, respectively.\n\n\nShow code\n\nggplot() + \n  geom_function(fun = \\(x) llr(x, par_cllr), color = \"red\") + \n  geom_function(fun = \\(x) llr(x, par_cross_entropy), color = \"blue\") +\n  geom_function(fun = \\(x) llr_true(x), color = \"black\") +\n  geom_hline(aes(yintercept = 0), linetype = \"dashed\", color = \"red\") +\n    geom_hline(aes(yintercept = -log(pi / (1-pi))), \n               linetype = \"dashed\", color = \"blue\") +\n    ylim(c(-10,10)) + xlim(c(-1, 2)) +\n  xlab(\"X\") + ylab(\"Log-Likelihood Ratio\")\n\n\n\nThe reason why the lines differ is that they are designed to solve a different predictive problem: as we’ve argued above, minimizing \\(C_\\text{LLR}\\) looks for the best \\(Y\\vert X\\) conditional probability estimate according to the balanced\nmeasure \\(\\text{Pr}'\\), whereas cross-entropy minimization does the same for\nthe original measure \\(\\text{Pr}\\). This is how data looks like under the two measures (the histograms are stacked - in the unbalanced case, positive examples are invisible on the linear scale of the plot):\n\n\nShow code\n\ntest_data <- bind_rows(\n  rxy(n = 1e6, pi = 0.5) |> mutate(type = \"Balanced\", llr_thresh = 0),\n  rxy(n = 1e6) |> mutate(type = \"Unbalanced\", llr_thresh = -log(pi / (1-pi)))\n  )\n\ntest_data |> \n  ggplot(aes(x = x, fill = y)) + \n  geom_histogram(bins = 100) +\n  facet_grid(type ~ ., scales = \"free_y\") +\n  xlim(c(-2, 4))\n\n\n\nThese differences are reflected in the misclassification rates of the resulting classifiers defined by \\(\\hat Y(X)=I(\\text{LLR}(X)>\\text{threshold})\\), where the appropriate threshold is zero in the balanced case, and \\(\\ln(\\frac{1-\\pi}{\\pi})\\) in the unbalanced case. According to intuition, we see that the \\(C_\\text{LLR}\\) optimizer beats the cross-entropy optimizer on the balanced sample, while performing\nsignificantly worse on the unbalanced one.\n\n\nShow code\n\ntest_data |>\n  mutate(\n    llr_cllr = llr(x, par_cllr),\n    llr_cross_entropy = llr(x, par_cross_entropy),\n    llr_true = llr_true(x)\n    ) |>\n  group_by(type) |>\n  summarise(\n    cllr = 1 - mean((llr_cllr > llr_thresh) == y),\n    cross_entropy = 1 - mean((llr_cross_entropy > llr_thresh) == y),\n    true_llr = 1 - mean((llr_true > llr_thresh) == y)\n    )\n\n# A tibble: 2 × 4\n  type           cllr cross_entropy true_llr\n  <chr>         <dbl>         <dbl>    <dbl>\n1 Balanced   0.166         0.185    0.140   \n2 Unbalanced 0.000994      0.000637 0.000518\n\nFinal remarks\nOur main conclusion in a nutshell is that \\(C_\\text{LLR}\\) minimization is\nequivalent, in the infinite sample limit, to cross-entropy minimization on a\nbalanced version of the original distribution. We haven’t discussed what happens\nfor finite samples where variance starts to play a role, affecting the\nefficiency of loss functions as model optimization and selection criteria.\nFor instance, for a well specified model of likelihood ratio, how do the\nconvergence properties of \\(C_{\\text{LLR}}\\) and cross-entropy estimators compare\nto each other? I expect that answering questions like this would require a much\nmore in-depth study than the one performed here (likely, with simulation playing\na central role).\n\n\n\nBrümmer, Niko, and Johan du Preez. 2006. “Application-Independent Evaluation of Speaker Detection.” Computer Speech & Language 20 (2): 230–75. https://doi.org/https://doi.org/10.1016/j.csl.2005.08.001.\n\n\nCover, Thomas M., and Joy A. Thomas. 2006. Elements of Information Theory 2nd Edition (Wiley Series in Telecommunications and Signal Processing). Hardcover; Wiley-Interscience.\n\n\nThis is how I understood things should theoretically work, from discussions with friends who are actually working on this field. I have no idea on how much day-to-day practice comes close to this mathematical ideal, and whether there exist alternative frameworks to the one I have just described.↩︎\nThe Likelihood Ratio Cost was introduced in (Brümmer and du Preez 2006). The reference looks very complete, but I find its notation and terminology so unfamiliar that I decided to do my own investigation and leave this reading for a second moment.↩︎\nThat is to say, \\(w(Y_{\\sigma(1)},\\,Y_{\\sigma(2)},\\dots,\\,Y_{\\sigma(N)})=w(Y_1,\\,Y_2,\\dots,\\,Y_N)\\) for any permutation \\(\\sigma\\) of the set \\(\\{1,\\,2,\\,\\dots,\\,N\\}\\).↩︎\nNota bene: the function \\(f\\) is here assumed to be fixed, whereas the randomness in the quantity \\(L _N ^w [f]\\) only comes from the paired observations \\(\\{(X_i,\\,Y_i)\\}_{i=1,\\,2,\\,\\dots,N}\\).↩︎\nNotice that, due to symmetry \\(\\mathbb E(w(\\boldsymbol Y)\\vert \\sum _i Y_i >0) = \\mathbb E(w(\\boldsymbol Y)\\vert Y_1 = 1)\\), which might be easier to compute.↩︎\nHere and below I relax a bit\nthe notation, as most details should be clear from context.↩︎\nThe quantity \\(w(\\boldsymbol Y)\\) is not defined when all \\(Y_i\\)’s are zero, as the right-hand\nside of Eq. (1) itself. In this case, we make the convention \\(w(\\boldsymbol Y) = 0\\).↩︎\nFor the original loss in Eq. (1), without the modification discussed above, the result would have been\n\\(\\dfrac{\\text {Pr}^\\prime (Y=1)}{\\text{Pr}^\\prime (Y=0)}=\\dfrac{1-\\text {Pr}(Y=0)^N}{1-\\text {Pr}(Y=1)^N}.\\)↩︎\nFormally, given an i.i.d.\nstochastic process \\(Z_i = (X_i,\\,Y_i)\\), we can define a new stochastic process\n\\(Z_i ^\\prime = (X_i^\\prime,\\,Y_i^\\prime)\\) such that\n\\(Z_i ^\\prime = Z_{2i - 1}\\) if \\(Y_{2i-1}\\neq Y_{2i}\\), and \\(Z_i ^\\prime = \\perp\\)\n(not defined) otherwise. Discarding \\(\\perp\\) values, we obtain an i.i.d.\nstochastic process whose individual observations are distributed according to\n\\(\\text{Pr}^\\prime\\).↩︎\nThere is another\ncase in which \\(C_{\\text{LLR}}\\) and cross-entropy minimization converge to the\nsame answer as \\(N\\to \\infty\\): when used for model selection among a class of\nmodels for the likelihood or posterior odds ratio that contains their correct\nfunctional form.↩︎\nThis is just logistic regression. It could be a reasonable approximation if \\(\\sigma_0 ^2\\approx \\sigma_1 ^2\\), which however I will assume below to be badly violated.↩︎\n",
    "preview": "posts/2023-11-15-interpreting-the-likelihood-ratio-cost/interpreting-the-likelihood-ratio-cost_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-11-03-conditional-probability/",
    "title": "Conditional Probability",
    "description": "Notes on the formal definition of conditional probability.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-11-03",
    "categories": [
      "Probability Theory",
      "Measure Theory"
    ],
    "contents": "\nLet \\((\\Omega, \\,\\mathcal E,\\,P)\\) be a probability space, and let\n\\(X\\colon \\Omega \\to \\Omega _X\\) be a random variable with target space\n\\((\\Omega _X, \\mathcal X)\\). We denote the corresponding push-forward measure on\n\\(\\mathcal X\\) by \\(X_*P\\), so that:\n\\[\n(X_*P)(B)= P(X^{-1}(B))\n\\]\nfor all \\(B\\in \\mathcal X\\). A measurable function\n\\(f\\colon \\Omega _X \\to \\mathbb R\\) is integrable with respect to \\(X_*P\\)\nif and only if \\(f\\circ X\\) is integrable with respect to \\(P\\), in which case1:\n\\[\n\\intop _\\mathcal X f\\,\\text d(X_*P) = \\intop _\\mathcal \\Omega (f \\circ X)\\,\\text dP.\n\\]\nNow, given an arbitrary event \\(E\\in \\mathcal E\\) define\n\\((X_*P)_E(A)=P(E\\cap X^{-1}(A))\\). Then \\((X_*P)_E\\) is a measure on\n\\(\\mathcal X\\) which is clearly dominated by \\(X_*P\\), and there exists a\nRadon-Nikodym derivative\n\\(\\frac{\\text d (X_*P)_E}{\\text d (X_*P)} \\in L_1(X_*P)\\). We define the\nconditional probability of event \\(E\\) with respect to the random variable \\(X\\) as\nthe random variable:\n\\[\nP(E\\vert X)\\equiv \\frac{\\text d (X_*P)_E}{\\text d (X_*P)}.\n\\]\nThe intuition behind this definition comes from the tautology (given the\ndefinition in terms of Radon-Nikodym derivative):\n\\[\nP(E \\cap (X\\in A)) = \\intop _{A} P(E\\vert X)\\,\\text d(X_*P).\n\\]\nOn one hand, from elementary probability theory, one would expect any\nsensible definition of conditional probability to satisfy this theorem.\nOn the other hand, the theorem univocally identifies \\(P(E\\vert X)\\) as the\nRadon-Nikodym derivative \\(\\frac{\\text d (X_*P)_E}{\\text d (X_*P)}\\), modulo a set of \\(X_*P\\) measure zero.\nIt is fairly easy to verify the following properties of conditional probability:\nCountable additivity. For any finite or countable family \\((E_i)_{i\\in I}\\) of\ndisjoint events, \\(E_i \\cap E_j = \\emptyset\\), we have:\n\\[\nP(\\cup _{i\\in I} E_i \\vert X = x) = \\sum _{i \\in I}P(E_i \\vert X = x)\n\\]\nfor almost all \\(x\\in \\Omega_X\\).\nPositivity. For any event \\(E\\) we have \\(P(E \\vert X=x) \\geq 0\\) for almost all\n\\(x \\in \\Omega\\).\nNormalization. \\(P(\\Omega \\vert X = x) = 1\\) for almost all \\(x \\in \\Omega\\).\nThis, however, does not generally imply that \\(P(\\cdot \\vert X = x)\\) is a\nprobability measure for almost all \\(x\\in \\Omega_X\\)2.\nFunctions \\(\\nu \\colon \\mathcal E \\times \\Omega _X \\to \\mathbb R^+\\) such that\n\\(\\nu(\\cdot, x)\\) is a measure for all \\(x\\in \\Omega _X\\), and\n\\(\\nu (E,\\cdot)\\) is \\(\\mathcal X\\)-measurable for all \\(E\\in \\mathcal E\\) are called\nrandom measures. If \\(\\nu\\) satisfies\n\\[\nP(E \\cap (X\\in A)) = \\intop _{A} \\nu (E,\\cdot)\\,\\text d(X_*P)\n\\]\n(or, equivalently, if \\(\\nu (E,\\cdot)\\) is a version of\n\\(\\frac{\\text d (X_*P)_E}{\\text d (X_*P)}\\)) for all \\(E\\in \\mathcal E\\), \\(\\nu\\) is\ncalled a regular conditional probability for the random variable \\(X\\). If the\nspace \\((\\Omega,\\, \\mathcal E)\\) is regular enough (e.g. if it is a Borel space)\none can prove that a regular conditional probability exists for any random\nvariable \\(X\\), see e.g. (Kallenberg 1997).\nIf \\(X = \\chi _A \\colon \\Omega \\to \\{0,1\\}\\), where \\(A\\in \\mathcal E\\)\nhas positive probability \\(0<P(A)<1\\), we can easily compute:\n\\[\nP(E\\vert \\chi _A) = \\chi _A\\cdot \\frac{P(E\\cap A)}{P(A)} + (1-\\chi _A)\\cdot \\frac{P(E\\cap A^c)}{P(A^c)}\n\\]\nIn particular, \\(P(E\\vert A) \\equiv P(E\\vert \\chi _A = 1)\\) agrees with the\nusual elementary definition of conditional probability.\nMore generally, if \\(X = \\text {id} _\\Omega\\), where the target space is equipped with a sub-\\(\\sigma\\)-algebra \\(\\mathcal F \\subseteq \\mathcal E\\), we have:\n\\[\nP(E\\vert \\mathcal F)\\equiv \\frac{\\text d (P\\vert _\\mathcal F)_E}{\\text d (P\\vert _\\mathcal F)},\n\\]\nwhich is sometimes taken as the definition of conditional probability with respect to a sub-\\(\\sigma\\)-algebra. When \\(\\mathcal F\\) is the \\(\\sigma\\)-algebra generated by a finite or countable partition \\(\\mathcal A = (A_i)_{i\\in I}\\) of\n\\(\\mathcal \\Omega\\) such that \\(P(A_i)>0\\) for all \\(i\\in I\\), we find:\n\\[\nP(E\\vert \\mathcal A)=\\sum _{i\\in I} \\frac{P(E\\cap A_i)}{P(A_i)}\\chi _{A_i},\n\\]\nagain in agreement with elementary definitions.\nFinally, if \\(X\\colon \\Omega \\to \\mathbb R\\) is a real-valued random variable, where \\(\\mathbb R\\) is equipped with the Borel \\(\\sigma\\)-algebra, \\(X_*P\\)\ncoincides with the Stieltjes measure generated by the cumulative distribution\nfunction \\(P_X\\) of \\(X\\). Denoting \\(P(E\\vert X)(x) \\equiv P(E\\vert X = x)\\), we may\nwrite:\n\\[\nP(E \\cap (X \\in B))=\\intop _B P(E\\vert X=x) \\,\\text dP_X(x).\n\\]\nand, in particular:\n\\[\nP(E)=\\intop _\\mathbb R P(E\\vert X=x) \\,\\text dP_X(x).\n\\]\n\n\n\nKallenberg, Olav. 1997. Foundations of Modern Probability. Vol. 2. Springer.\n\n\n\nThese claims can be proved by a standard argument using approximations by simple\nfunctions.↩︎\nFor instance, denoting by\n\\(N_E = \\{x \\in \\Omega_X \\vert P(E\\vert X = x) \\geq 0\\}\\), positivity implies that\n\\((X_*P)(N_E)=0\\). However, there’s no guarantee that\n\\(\\cup _{E\\in \\mathcal E} N_E\\) is also a measure zero set (and in fact it does\nnot need to be measurable, since the union is generally uncountable).↩︎\n",
    "preview": {},
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-10-31-prefix-free-codes/",
    "title": "Prefix-free codes",
    "description": "Generalities about prefix-free (a.k.a. instantaneous) codes",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-10-31",
    "categories": [
      "Information Theory",
      "Entropy",
      "Probability Theory"
    ],
    "contents": "\nLet \\(\\mathbb X\\) be a finite alphabet and denote by\n\\(\\mathbb X ^* = \\coprod _{k = 0} ^{\\infty} \\mathbb X ^k\\) the set of strings of\nsymbols from \\(\\mathbb X\\). A binary code on \\(\\mathbb X\\) is a function\n\\(f \\colon \\mathbb X \\to \\{0,\\,1\\}^*\\). This is usually extended to a function\n\\(f^* \\colon \\mathbb X ^* \\to \\{0,\\,1\\}^*\\) as follows:\n\\[\nf^* (x_1 \\,x_2\\,\\cdots x_n) = f(x_1) f(x_2)\\cdots f(x_n)\n\\]\nA code is said to be:\nNon-singular if \\(f\\) is injective.\nUniquely decodable if \\(f^*\\) is injective.\nPrefix-free if \\(x' \\neq x\\) implies that \\(f(x^\\prime) \\neq f(x)s\\) for any \\(s\\)\nin \\(\\{0,1\\}^*\\).\nFor example:\n\\[\na \\mapsto 0,\\quad b\\mapsto 00\n\\]\nis a non-singular but not uniquely decodable code for the alphabet\n\\(\\mathbb X = \\{a,\\,b\\}\\), while the code:\n\\[\na \\mapsto 0,\\quad b\\mapsto 01\n\\]\nis uniquely decodable, but not prefix-free. Finally, the assignments:\n\\[\na \\mapsto 0, \\quad b \\mapsto 10, \\quad c \\mapsto 110, \\quad d\\mapsto1110,\\quad\\cdots\n\\]\nshow that there exist prefix-free codes for any finite or countable alphabet.\nThe importance of prefix-free codes lies in the fact that they allow for\nreal-time decoding, as soon as the string corresponding to a symbol is received\n(which is why they are also called “instantaneous codes”) 1. Binary prefix-free codes can also be interpreted as representing sequences of “Yes-No” questions that univocally identify the\nelements of \\(\\mathbb X\\).\nAn important property satisfied by all uniquely decodable binary codes, and in\nparticular by prefix-free codes, is the Kraft-McMillan inequality:\n\\[\n\\sum _{x\\in \\mathbb X} 2 ^{-L(x)} \\leq 1\n\\]\nwhere \\(L (x)\\) is the length of the code for \\(x\\). A converse is also true: for\nany set of positive integers\n\\((\\ell _{i})_{1\\leq i\\leq N}\\) satisfying the Kraft-McMillan inequality, there\nexists a prefix-free code over \\(\\mathbb X = \\{1,\\,2,\\,\\dots,\\,N\\}\\) such that\n\\(\\ell _i = L(i)\\).\nThis allows to immediately prove the entropy bound for the expected length\nof uniquely decodable codes. Given a probability distribution \\(p\\) over\n\\(\\mathbb X\\), we have:\n\\[\n\\begin{split}\n\\mathbb E(L(X))&=\\sum _{x\\in \\mathbb X} p(x) L(x) \\\\\n                             &=-\\sum _{x\\in \\mathbb X} p(x) \\log _2(2^{-L(x)}) \\\\\n                             &=-\\sum _{x\\in \\mathbb X} p(x) \\log _2(p(x))-\\sum _{x\\in \\mathbb X} p(x) \\log _2(\\frac{2^{-L(x)}}{p(x)})\n\\end{split}\n\\]\nThe first term is recognized as the entropy (in bits) of \\(X\\), \\(H_2(X)\\), whereas\nthe second term can be bounded using the Jensen and Kraft-McMillan inequalities:\n\\[\n-\\sum _{x\\in \\mathbb X} p(x) \\log _2(\\frac{2^{-L(x)}}{p(x)}) \\geq -\\log _2\\left(\\sum _{x\\in \\mathbb X} 2^{-L(x)} \\right) \\geq 0.\n\\]\nWe obtain:\n\\[\n\\mathbb E (L(X)) \\geq H_2(X)\n\\]\nFurthermore, noticing that the positive integers\n\\(\\ell _i = \\lceil \\log _2\\frac{1}{p(x_i)} \\rceil\\) satisfy the Kraft-McMillan\ninequality, we can immediately construct a prefix-free code (the Shannon-Fano\ncode) for which \\(L(x_i) = \\ell _i\\). For this code:\n\\[\n\\mathbb E (L(X)) \\leq H_2(X) + 1.\n\\]\n\nThe decoding\nalgorithm works as follows: given a binary string\n\\(y_1y_2\\cdots y_M = f^*(x_1 x_2 \\cdots x_N)\\) we start reading the substrings\n\\(y_1y_2 \\cdots y_k\\) until we find a match with some code \\(s \\in \\text{Im}(f)\\),\nwhich is the code of the first symbol \\(x_1\\) of the original sequence.\nWe remove this substring and start reading again, to find the code of the second symbol \\(x_2\\),and so on and so forth. This procedure can obviously be implemented\nin an online setting.↩︎\n",
    "preview": {},
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-07-24-ab-tests-and-repeated-checks/",
    "title": "AB tests and repeated checks",
    "description": "False Positive Rates under repeated checks - a simulation study using R.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-07-27",
    "categories": [
      "AB testing",
      "Sequential Hypothesis Testing",
      "Frequentist Methods",
      "Statistics",
      "R"
    ],
    "contents": "\nIntro\n\n“How is the experiment going?”\n\nAlso:\n\n“Do we already see something?”\n\nAnd my favorite one:\n\n“Did we already hit significance, or do we need more data?”\n\nIf you have dealt with experiments with relatively high outcome expectations,\nyou will likely have received (or perhaps asked yourself) similar questions\nfrom time to time.\nIn many data analysis contexts, including but not limited to for-profit ones,\nresearchers are always trying to come up with positive results as fast as\nthey can. Therefore, it is not at all surprising to see questions such as the\nones above regularly arise during the course of an experiment. This is natural\nand not a problem per se. What I want to highlight and quantify in this post\nis how, if not done carefully, such “real-time” monitoring schedules can\nseriously invalidate data analysis - by inflating false positive and false\nnegative rates.\nGenerally speaking, repeated and ad-hoc checks lead to problems of\nselective/simultaneous inference (a topic which I have touched in\nother places in this blog).\nAvoiding them is not the only valid solution - if you want to learn about some\nproper method you may give a look into Sequential Hypothesis Testing,\na topic that I may explore in future posts. Here my goal is to understand\nthe consequences of naive repeated checking, which can be easily found out\nthrough simulation.\nWhat’s the matter with repeated checks?\nTo understand why problems can arise, recall that the classical\nFrequentist framework 1 operates by providing\na priori guarantees (bounds) on the probabilities of 2:\nA false positive outcome in the absence of any signal: rejecting the null\nhypothesis when this is actually true.\nA false negative outcome in the presence of some (well-defined) signal.\nThe a priori nature of these guarantees means that they are stipulated before\nrunning the experiment and assuming a certain experimental schedule 3.\nThis implies that any departure from the original schedule can in principle\ninvalidate the claimed False Positive Rate (FPR) and False Negative Rate (FNR).\nFor instance, the most basic experimental schedule\n(actually the one implicitly assumed by virtually all sample size calculators\n) is:\nCollect data until reaching a prefixed sample size.\nRun an hypothesis test (with a prefixed significance threshold for claiming a\nsignal).\nCommon examples of departures from the original schedule include:\nRunning several tests on partial data (before reaching the established sample\nsize), to look for an early signal.\nStopping the experiment beforehand, because partial data doesn’t show any\nsignal.\nProlonging the experiment after reaching the established sample size, because\nthere’s a “hint” to a signal, but the significance threshold was not reached.\nIn what follows, I will focus on the first behavior, whose result is to inflate\nthe FPR. Again, there are various ways to perform repeated checks while keeping\nthe FPR under control, but that’s not the focus of this post. Instead, I want to\nunderstand how FPR is affected when the same test is repeated several times on\npartial data.\nExample\nLet me illustrate the idea with an imaginary marketing experiment. Suppose\nyou are optimizing an advertising campaign, say you want to test whether a new\nad design performs better than the existing one in terms of click through rates.\nYou start sending batches of two thousands ads4 to randomized\nusers, half using the new design and half using the old one.\nIf the new design does actually perform better, you want to fully switch to it\nas soon as possible, so that after each batch send, you compare the click\nthrough rates of all ads sent so far, with the idea of switching\nas soon as a statistically significant improvement is observed.\nConcretely, you propose to do the following:\nAt each step, calculate the click through rates for the new and old designs.\nCompute a \\(p\\)-value for the hypothesis test5 that tests whether the new design leads\nto an higher click through rate than the old one.\nIf the \\(p\\)-value is smaller than a certain fixed threshold \\(\\alpha\\), stop the\nexperiment and declare the new design as the winner.\nIf no \\(p\\)-value smaller than \\(\\alpha\\) is observed after a certain number \\(n\\)\nof iterations, stop the experiment and declare the old design as the winner.\nNow, the question is: how often would the above procedure declare the new\ndesign as the winner, if it doesn’t truly perform better than the old one?\n(i.e. what is the FPR of the whole procedure?)\nSimulation\nTo compute the FPR, we assume that both the new and old designs have in fact the\nsame click through rate \\(p = 10 \\%\\). The following function generates a\nsequence of \\(n\\) consecutive \\(p\\)-values, computed as described above, that one\ncould observe under these circumstances:\n\n\ngenerate_p_values <- function(n = 28,      # maximum number of iterations\n                              size = 1e3,  # ad sends per batch\n                              p = 0.1      # true common click through rate\n                              ) \n  {\n  successes_a <- cumsum( rbinom(n = n, size = size, prob = p) )  # clicks old ad\n  successes_b <- cumsum( rbinom(n = n, size = size, prob = p) )  # clicks new ad\n  \n  sapply(1:n, \\(k) {\n    prop.test(\n      x = c(successes_a[k], successes_b[k]), \n      n = k * size * c(1, 1), \n      alternative = \"greater\",\n      )$p.value\n  })\n}\n\n\nFor instance:\n\n\nset.seed(999)\n( p_example <- generate_p_values(n = 5) )\n\n[1] 0.4704229 0.3932333 0.1669308 0.2219066 0.2592812\n\nThe function below evaluates such a sequence of \\(p\\)-values with a fixed\nthreshold \\(\\alpha\\):\n\n\nevaluate_p_values <- function(p, alpha = 0.05, checkpoints = seq_along(p)) {\n  p <- p[checkpoints]\n  as.logical(cumsum(p < alpha))\n}\n\n\nFor instance, with \\(\\alpha = 20\\%\\), the sequence above would lead to a\n(false) positive result, which would be claimed at the third check. Output\nlooks as follows:\n\n\nevaluate_p_values(p_example, alpha = 0.2)\n\n[1] FALSE FALSE  TRUE  TRUE  TRUE\n\nLet me now simulate a large number of such “experiments”. I will fix\n\\(\\alpha = 5\\%\\), a popular choice:\n\n\nset.seed(840)\nsim_data <- replicate(1e4, generate_p_values(n = 100) |> evaluate_p_values())\n\n\nThe result is a matrix whose columns are logical vectors such as the one above:\n\n\nsim_data[,1]\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [11] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [21] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [31] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [41] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [51] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [71] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [81] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [91] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n(a true negative result). Hence, the averages of this matrix rows provide the\nfalse positive rates after \\(n\\) checks:\n\n\nfpr <- rowMeans(sim_data)\nplot(fpr, type = \"l\", xlab = \"Checks\", ylab = \"False Positive Rate\")\nabline(h = 0.05, col = \"red\", lty = \"dashed\")\n\n\n\nThe curve above shows how the FPR depends on the number of checks performed,\naccording to the procedure described in the previous section. For a single\ncheck, this coincides with FPR of an individual binomial test6. However, allowing for repeated checks, we\nsee that the overall FPR steadily increases with number of checks. With \\(n = 3\\)\nchecks, the FPR is already close to \\(10 \\%\\), almost twice the nominal FPR of\neach individual test:\n\n\nfpr[3]\n\n[1] 0.0929\n\nWith \\(n \\approx 40\\) checks, the FPR is about \\(25 \\%\\), the same FPR of\nan experiment that involves tossing a coin twice, declaring it biased if the\nresult is two consecutive “tails”.\n\n\nfpr[40]\n\n[1] 0.2471\n\nHere we are assuming that data is re-checked after the arrival of every single\nbatch, but there are of course infinite alternative possibilities. For instance,\nthe plot below shows what happens when checks are performed after\nthe collection of \\(n = 1, \\,4, \\,16, \\,64\\) batches of data (at each checkpoint,\nthe expected size of statistical fluctuations is reduced by a factor of \\(2\\)).\n\n\nShow code\n\ncheckpoints <- c(1, 4, 16, 64)\n\nset.seed(840)\nfpr2 <- replicate(1e4, \n          generate_p_values(n = 64) |> \n            evaluate_p_values(checkpoints = checkpoints)\n          ) |>\n  rowMeans()\n\nplot(fpr2, \n     type = \"b\", \n     xlab = \"Checks\", \n     ylab = \"False Positive Rate\", \n     xaxt = \"n\"\n     )\n\nabline(h = 0.05, col = \"red\", lty = \"dashed\")\naxis(1, at = seq_along(checkpoints))\naxis(3, at = seq_along(checkpoints), labels = paste(checkpoints, \"K\"))\nmtext(\"Sample Size\", side = 3, line = 2)\n\n\n\nAs a third possible variation, we may think of applying different \\(p\\)-value\nthresholds at different checks (a scheme that can be actually made to work in\npractice, see for instance the Wikipedia article on the Haybittle–Peto boundary).\nThe following plot illustrates this, assuming three (equally spaced) checks\nafter the collection of \\(n = 1,\\,2,\\,3\\) data batches, using the significance\nthresholds \\(\\alpha = 0.01, \\,0.025, \\,0.05\\), respectively.\n\n\nShow code\n\nset.seed(840)\n\nalpha <- c(0.01, 0.025, 0.05)\n\nfpr3 <- replicate(1e5, \n          generate_p_values(n = 3) |> \n            evaluate_p_values(alpha = alpha)\n          ) |>\n  rowMeans()\n\nplot(fpr3, \n     type = \"b\", \n     xlab = \"Checks\", \n     ylab = \"False Positive Rate\", \n     xaxt = \"n\"\n     )\n\nabline(h = alpha[3], col = \"red\", lty = \"dashed\")\nabline(h = alpha[2], col = \"blue\", lty = \"dashed\")\naxis(1, at = seq_along(fpr3))\naxis(3, at = seq_along(fpr3), labels = alpha)\nmtext(\"p-value threshold\", side = 3, line = 2)\n\n\n\nConclusions\nThis post illustrated quantitatively how the performance of repeated checks\nduring the process of data collection can affect the overall False Positive Rate\nof an experimental analysis. The code provided above can be easily adapted to\nsimulate other types of experiments and schemes for interim checks.\nA question that may possibly arise is: should I really care? You could argue\nthat what I’ve shown here represents a simple trade-off between FPR on one side,\nFNR and efficiency (speed) in detection of a signal on the other.\nMy answer is a resounding yes, irrespective of whether you are running\nexperiments for purely scientific or utilitaristic purposes. If you are unable\nto characterize (at least approximately) the FPR and FNR of your analysis,\nthe whole point of running a formal test looks very dubious to me. You may as\nwell simply collect some data and draw an educated guess.\nOther story is if you are able to tell in advance how interim\nchecks affect FPR/FNR, and use this knowledge to optimize your analysis\nstrategy. This note provides some clues on how to do so.\n\nI move within this framework because it is the only\none I’m reasonably comfortable with, and for which I have a decent understanding\nof the decision dynamics that follow from it. That said, I suspect that also\nBayesian hypothesis testing can be affected by the kind of issues discussed\nhere, although perhaps in a less transparent way, due to working with formal a\nposteriori probabilities.↩︎\nThe statistical jargon\nused to indicate these two types of errors, and the corresponding a priori\nguarantees on their probabilities, sounds very mysterious to me\n(Type I/II errors, size and power…). I like to think in terms of\n“False Positive” and “False Negative” rates, which is the same thing.↩︎\nThis is\ngenerally true, also in the aforementioned sequential settings. In that case,\nthe difference is that the schedule takes into account that continuous and/or\ninterim checks will be performed.↩︎\nThe actual numbers in this\nexample may be totally unrealistic, but that’s beside the point.↩︎\nTechnically, this would be a\ntwo-sample, one-sided binomial test.↩︎\nThe fact that\nthis is not exactly equal to \\(\\alpha\\), but in fact slightly smaller, is due to\nthe discreteness of the underlying binomial distributions. The \\(p\\)-value of the\nbinomial test is defined in such a way to satisfy\n\\(\\text{Pr}(p < \\alpha)\\leq \\alpha\\).↩︎\n",
    "preview": "posts/2023-07-24-ab-tests-and-repeated-checks/ab-tests-and-repeated-checks_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-07-11-testing-functional-specification-in-linear-regression/",
    "title": "Testing functional specification in linear regression",
    "description": "Some options in R, using the `{lmtest}` package.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-07-11",
    "categories": [
      "Statistics",
      "Model Misspecification",
      "Regression",
      "Linear Models",
      "R"
    ],
    "contents": "\nAnother one from the series on “misspecified regression models” (started with Model Misspecification and Linear Sandwiches).\nIntro\nLately I’ve been messing around with the {lmtest} R package, a nice collection of hypothesis tests for classical linear model assumptions: linearity (of course) and heteroskedasticity (\\(X\\)-independence of the conditional variance).\nJust to clarify, here the relevant “linearity” assumption is that the conditional mean \\(\\mathbb E (Y\\vert X)\\) is given by a linear combination of known functions \\(f_i\\) of \\(X\\):\n\\[\n\\mathbb E (Y\\vert X) = \\sum _{i = 1}^p \\alpha_if_i(X),\n\\]\nTesting “linearity” (or, as the title goes, “functional specification”) refers to testing that the chosen set of functions \\(\\{f_{i}\\}_{i=1,\\dots,p}\\) provide a valid description of the data generating process.\nFirst attempt: residual autocorrelation\nMy initial intuition was that it should be possible to test functional specification through the following procedure:\nPerform linear regression with the specified functional form.\nOrder the residuals according to the corresponding values of \\(X\\)1.\nTest for serial correlation (e.g. performing a Durbin-Watson test, lmtest::dwtest) on the series of ordered residuals.\nThe idea is quite simple: if residuals exhibit some systematic pattern when\nplotted against \\(X\\), then for close values of \\(X\\), residuals should also tend to be close, leading to a positive correlation. For example:\n\n\nset.seed(840)\nx <- rnorm(1e2)\ny <- x^3 + rnorm(length(x))\nplot(x, y)\nabline(lm(y ~ x))\n\n\n\nThis, I suspect, is the reason why functions such as lmtest::dwtest() have an\norder.by argument which precisely allows to sort residuals before performing the test.\nUnfortunately, it turns out that such a method is not only sensitive to functional misspecification, but also to heteroskedasticity - as one can quickly verify by running a simulation using lmtest::dwtest().\nThe overall idea is interesting, and works for homoskedastic noise, but the limitation to constant variance may be a bit too stringent. For this reason I turned to a second method, which also allows to take into account the\npossibility of heteroskedastic noise.\nSecond attempt: RESET + Heteroskedastic Consistent variance estimates\nThe idea of RESET tests (see ?lmtest::resettest()) is also quite simple:\nif the linear model is correct, there should be relatively little gain in adding additional non-linear functions of the original covariates to the fit’s formula.\nThe statistical significance of these model adjustments can be tested through a\nstandard \\(Z\\)-test (or \\(F\\)-test, for multiple adjustments at once), with an\nimportant\ncatch: the covariance matrix of regression coefficients used in these tests can\nbe chosen to be robust to heteroskedasticity (see Model Misspecification and Linear Sandwiches).\nThe code that follows illustrates this procedure with an example dataset. The following section contains a more in-depth simulation study of the property of the RESET\ntest.\n\n\nfit_cars <- lm(dist ~ speed, data = cars)\nwith(data = cars, plot(speed, dist))\nabline(fit_cars)\n\n\n\n\n\nlmtest::resettest(fit_cars, \n                  type = \"regressor\", \n                  power = 2,\n                  vcov = sandwich::vcovHC\n                  )\n\n\n    RESET test\n\ndata:  fit_cars\nRESET = 2.32, df1 = 1, df2 = 48, p-value = 0.1344\n\nUnfortunately, the output of lmtest::resettest does not include the results of the extended fit, which can be useful to understand the impact of the omitted covariates on the overall model picture (independently of the RESET \\(p\\)-value under the null hypothesis). 2\nIn order to get some insight on the effect of misspecification, we need to\nmanually perform the RESET fit and make the relevant comparisons:\n\n\nfit_cars_sq <- lm(dist ~ speed + I(speed*speed), data = cars)\nwith(data = cars, plot(speed, dist))\nabline(fit_cars)\nlines(x = cars$speed, y = fitted(fit_cars_sq), col = \"blue\")\n\n\n\nRESET + HC vcov: a simulation study\nWe consider a univariate regression problem, with a regressor \\(X \\sim \\mathcal N (0,1)\\), a and a response \\(Y\\). We will consider three ground truth distributions for \\(Y\\) given \\(X\\):\n\\[\n\\begin{split}\n\\text{T1}:& \\qquad Y=\\frac{1}{5}X+Z\\\\\n\\text{T2}:& \\qquad Y=\\frac{1}{5}X + \\vert X \\vert Z\\\\\n\\text{T3}:& \\qquad Y=\\frac{1}{5}X^3 + Z\n\\end{split}\n\\]\nwhere \\(Z\\sim \\mathcal N (0,1)\\) is independent from \\(X\\). We will study,\nthrough simulation, the \\(p\\)-value distribution of the RESET test for linear\nregression based on the model \\(Y = q+m X + \\varepsilon\\), where \\(q\\) and \\(m\\) are\nunknown coefficients, and \\(\\epsilon\\) is a noise term with unknown variance.\nIt follows that the model is correctly specified with respect to \\(\\text{T1}\\), has functional misspecification with respect to \\(\\text{T3}\\), and potentially noise misspecification3 with respect to \\(\\text{T2}\\), if we model variance as being independent of \\(X\\).\nData will consist of independent samples \\((X_i, Y_i)\\) from the joint distribution of \\(X\\) and \\(Y\\). To facilitate simulation, we define some helpers in the code chunk below.\n\n\nShow code\n\n#' Helper to generate data with prescribed: \n#' * Regressor distribution: `x`\n#' * Response conditional mean: `f`\n#' * Response conditional noise: `eps` \ndgp_fun <- function(x, f, eps) {\n  function(n) {\n    .x <- x(n)\n    data.frame(x = .x, y = f(.x) + eps(.x))\n  }\n}\n\n#' Helper to simulate results of linear regression, with prescribed:\n#' * Data generating process: `dgp`\n#' * Sample size of simulated datasets: `n`\n#' * Summary function (e.g. p-value of RESET test): `summarize_fun`\nlm_simulate <- function(dgp, n, summarize_fun, nsim, simplify) {\n  replicate(nsim, {\n    data <- dgp(n)\n    fit <- lm(y ~ x, data)\n    summarize_fun(fit)\n  }, simplify = simplify)\n} \n\n#' Helper to perform RESET test on a `lm` fit object, and plot the p-value\n#' distribution. The estimator for regression coefficients variance-covariance\n#' matrix can be set through the `vcov` argument.\nreset_pvalue <- function(\n    dgp, n,  # Data generating process params\n    power = 2:3, type = \"regressor\", vcov = sandwich::vcovHC,  # RESET params\n    nsim = 1e3  # Simulation params\n    ) \n{\n  summarize_fun <- function(fit)\n    lmtest::resettest(fit, power = power, type = type, vcov = vcov)$p.value\n  \n  p <- lm_simulate(\n    dgp = dgp, \n    n = n, \n    summarize_fun = summarize_fun, \n    nsim = nsim,\n    simplify = TRUE\n    )\n  \n  return(data.frame(\n    p = p,\n    dgp = deparse(substitute(dgp)),\n    n = n,\n    vcov = deparse(substitute(vcov)),\n    nsim = nsim\n  ))\n  \n}\n\n\nFurthermore, we will use:\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nfor plotting.\nData generating processes\nThe data generating processes can be defined as follows:\n\n\ndgp_t1 <- dgp_fun(\n  x = rnorm,\n  f = \\(x) 0.2 * x,\n  eps = \\(x) rnorm(length(x))\n)\n\ndgp_t2 <- dgp_fun(\n  x = rnorm,\n  f = \\(x) 0.2 * x,\n  eps = \\(x) abs(x) * rnorm(length(x))\n)\n\ndgp_t3 <- dgp_fun(\n  x = rnorm,\n  f = \\(x) 0.2 * x^3,\n  eps = \\(x) rnorm(length(x))\n)\n\n\nData generated according to these three distributions looks as follows:\n\n\nShow code\n\nbind_rows(\n  tibble(dgp_t1(100), dgp = \"dgp_t1\"),\n  tibble(dgp_t2(100), dgp = \"dgp_t2\"),\n  tibble(dgp_t3(100), dgp = \"dgp_t3\"),\n  ) |>\n  ggplot(aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = F) +\n    facet_grid(~ dgp)\n\n\n\nRESET \\(p\\)-value distributions\nThe RESET \\(p\\)-value cumulative distributions for the three ground truths \\(\\text{T1}\\),\n\\(\\text{T2}\\) and \\(\\text{T3}\\) are shown below\n4. The \\(y\\) coordinates of these\nplots can be interpreted as follows:\nFor the ground truths \\(\\text{T1}\\) and \\(\\text{T2}\\), \\(y\\) represents\nthe false positive rate (or Type I Error Rate) in rejecting the null hypothesis\n“no functional misspecification” at a given size of the test\n\\(x\\). For a valid \\(p\\)-value, these curves should lie on or below the straight\nline \\(y = x\\).\nFor the ground truth \\(\\text{T3}\\), \\(y\\) represents the Power (or one minus the\nType II Error Rate) in detecting functional misspecification at a given size\n\\(x\\). High values correspond to high sensitivity.\n\n\nShow code\n\nsim_data <- dplyr::bind_rows(\n  reset_pvalue(dgp = dgp_t1, n = 10, vcov = sandwich::vcovHC),\n  reset_pvalue(dgp = dgp_t1, n = 100, vcov = sandwich::vcovHC),\n  reset_pvalue(dgp = dgp_t1, n = 1000, vcov = sandwich::vcovHC),\n  reset_pvalue(dgp = dgp_t1, n = 10000, vcov = sandwich::vcovHC),\n  reset_pvalue(dgp = dgp_t1, n = 10, vcov = stats::vcov),\n  reset_pvalue(dgp = dgp_t1, n = 100, vcov = stats::vcov),\n  reset_pvalue(dgp = dgp_t1, n = 1000, vcov = stats::vcov),\n  reset_pvalue(dgp = dgp_t1, n = 10000, vcov = stats::vcov),\n  \n  reset_pvalue(dgp = dgp_t2, n = 10, vcov = sandwich::vcovHC),\n  reset_pvalue(dgp = dgp_t2, n = 100, vcov = sandwich::vcovHC),\n  reset_pvalue(dgp = dgp_t2, n = 1000, vcov = sandwich::vcovHC),\n  reset_pvalue(dgp = dgp_t2, n = 10000, vcov = sandwich::vcovHC),\n  reset_pvalue(dgp = dgp_t2, n = 10, vcov = stats::vcov),\n  reset_pvalue(dgp = dgp_t2, n = 100, vcov = stats::vcov),\n  reset_pvalue(dgp = dgp_t2, n = 1000, vcov = stats::vcov),\n  reset_pvalue(dgp = dgp_t2, n = 10000, vcov = stats::vcov),\n  \n  reset_pvalue(dgp = dgp_t3, n = 10, vcov = sandwich::vcovHC),\n  reset_pvalue(dgp = dgp_t3, n = 100, vcov = sandwich::vcovHC),\n  reset_pvalue(dgp = dgp_t3, n = 1000, vcov = sandwich::vcovHC),\n  reset_pvalue(dgp = dgp_t3, n = 10000, vcov = sandwich::vcovHC),\n  reset_pvalue(dgp = dgp_t3, n = 10, vcov = stats::vcov),\n  reset_pvalue(dgp = dgp_t3, n = 100, vcov = stats::vcov),\n  reset_pvalue(dgp = dgp_t3, n = 1000, vcov = stats::vcov),\n  reset_pvalue(dgp = dgp_t3, n = 10000, vcov = stats::vcov)\n)\n\nsim_data |>\n  mutate(n_label = paste(\"n\", n, sep = \" = \")) |>\n  ggplot(aes(p, color = vcov)) + \n    stat_ecdf() +\n    scale_color_discrete(\"vcov\") + \n    scale_x_continuous(\"p-value\", labels = scales::percent) + \n    scale_y_continuous(\"Empirical CDF\", labels = scales::percent) +\n    geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n    facet_grid(n_label ~ dgp, ) +\n    ggtitle(\n      \"p-value distribution of RESET test\",\n      paste(\"nsim\", max(sim_data$nsim), sep = \" = \")\n      )\n\n\n\nThe plots illustrate qualitatively the behavior of the RESET test with and\nwithout the vcov correction for noise heteroskedasticity. Various remarks:\nThe test with the standard stats::vcov estimator is sensitive not only to\npure functional misspecification (\\(\\text{T3}\\)), but also to pure heteroskedastic\nnoise (\\(\\text{T2}\\)).\nThe sandwich::vcovHC estimator leads to an asymptotically correct Type I\nError Rate in the \\(\\text{T2}\\) case, but to a somewhat lower sensitivity (with\nrespect to stats::vcov) in the \\(\\text{T3}\\) case.\nWe need to keep in mind that sandwich::vcovHC only provides\nasymptotically correct variance-covariance estimates. Thus, for small \\(n\\), the\n\\(p\\)-value distribution of the RESET test using the sandwich::vcovHC can also\nbe distorted (even in the perfectly specified case \\(\\text{T1}\\)).\nConclusions\nThis post explained how to perform model validation checks that are sensitive to functional misspecification, but relatively robust to heteroskedasticity.\nThe general idea is to extend the original model, allowing for more general functional forms in the conditional mean of the response, and test whether such extension significantly improves the fit. The catch is that, when performing the latter test, we need to somehow keep into account the possibility of heteroskedastic noise.\nThis idea is readily implemented with RESET tests for linear models: one can simply use a variance-covariance estimator for regression coefficients that is robust to heteroskedasticity. In R, this can be achieved with a single line of code, using lmtest::resettest(vcov = sandwich::vcovHC).\nWith some effort, one may be able to generalize such a procedure to any parametric model fitted by Maximum Likelihood Estimation, since a sandwich estimator is available also in this more general case (see e.g. the presentation of sandwich estimators in this paper by D.A. Freedman).\n\nHere I’m implicitly assuming that we have a single \\(X\\), but a similar logic should also apply to multivariate regression.↩︎\nWith enough data, the RESET test would likely test positive for a variety of misspecifications, but that doesn’t mean that such misspecification are necessarily relevant from a modeling perspective. Here, for instance, a large coefficient for \\(\\text{(speed)}^2\\) with a \\(Z\\)-score of two \\(\\sigma\\)s could be more worrying than a minuscule coefficient with a \\(Z\\)-score of five \\(\\sigma\\)s.↩︎\nSometimes also referred to as “second order misspecification”.↩︎\nThe code is a bit unelegant 😬 but it works.↩︎\n",
    "preview": "posts/2023-07-11-testing-functional-specification-in-linear-regression/testing-functional-misspecification-in-linear-regression_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-06-14-sum-and-ratio-of-independent-random-variables/",
    "title": "Sum and ratio of independent random variables",
    "description": "Sufficient conditions for independence of sum and ratio.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-06-14",
    "categories": [
      "Mathematics",
      "Probability Theory"
    ],
    "contents": "\nLet \\(X\\) and \\(Y\\) be two continuous independent random variables, with joint\ndensity \\(f_{XY}(x,y)=f_X(x)f_Y(y)\\). Define:\n\\[\ns = x+y, \\qquad r = x/y,\n\\]\nwith inverse transformation given by:\n\\[\ny = \\frac{s}{1+r},\\qquad x = \\frac{rs}{1+r}.\n\\]\nThe Jacobian of the \\((x,y) \\mapsto (s,r)\\) transformation is:\n\\[\n\\left|\\dfrac{\\partial (s,r)}{\\partial(x,y)}\\right|= \\dfrac{(1+r)^2}{s}.\n\\]\nHence the joint density of \\(S = X+Y\\) and \\(R = X/Y\\) is given by:\n\\[\nf_{SR}(s,r) = f(x,y)\\left|\\dfrac{\\partial (x,y)}{\\partial(s,r)}\\right|=f_X(\\frac{rs}{1+r})f_Y(\\frac{s}{1+r})\\frac{s}{(1+r)^2}.\n\\]\nThe necessary and sufficient condition for this to factorize into a product,\n\\(f_{SR}(s,r)\\equiv f_S(s)f_R(r)\\), is that \\(f_X(x)f_Y(y) = g_S(s)g_R(r)\\)\nfor some functions \\(g_S\\) and \\(g_R\\).\nThis is true for all functions \\(f_X\\) and \\(f_Y\\) from the family:\n\\[\n\\phi(t) = \\text{const} \\times  t^\\alpha e^{-\\beta t}.\n\\]\nThis includes some important special cases:\nThe \\(\\chi ^2\\) distribution (\\(\\alpha = \\frac{\\nu}{2}-1,\\,\\beta = \\frac{1}{2}\\)).\nThe exponential distribution: \\(\\alpha = 0,\\,\\beta >0\\).\nThe “homogeneous” distribution: \\(\\beta = 0\\) (restricted to the appropriate\ndomain).\nThe uniform distribution: \\(\\alpha = \\beta = 0\\).\n\n\n\n",
    "preview": {},
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-07-fishers-randomization-test/",
    "title": "Fisher's Randomization Test",
    "description": "Notes and proofs of basic theorems",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-06-07",
    "categories": [
      "Statistics",
      "Frequentist Methods",
      "Causal Inference"
    ],
    "contents": "\nLet \\(N\\in \\mathbb N\\) be fixed, and let:\n\\(\\mathbf Y(1),\\,\\mathbf Y(0)\\in \\mathbb R ^N\\) be random vectors, with\ncomponents \\(Y_i(1),Y_i(0)\\in \\mathbb R\\),\n\\(\\mathbf Z\\) be a random vector with components \\(Z_i \\in \\{0,1\\}\\), independent\nfrom \\(\\mathbf Y(1)\\) and \\(\\mathbf Y(0)\\) above,\n\\(\\mathbf Y = \\mathbf Z\\times \\mathbf Y(1)+(1-\\mathbf Z)\\times \\mathbf Y(0)\\) (multiplication is component-wise).\nGiven a scalar function \\(t = t(\\mathbf Z, \\,\\mathbf Y)\\in \\mathbb R\\), define:\n\\[\nP(t,\\mathbf Z, \\mathbf Y)=\\sum _{\\mathbf Z '}\\text{Pr}_\\mathbf Z(\\mathbf Z')\\cdot I(t(\\mathbf Z',\\mathbf Y)\\geq t(\\mathbf Z,\\mathbf Y)),\n\\]\nwhere \\(\\text{Pr}_\\mathbf Z(\\cdot)\\) is the marginal distribution of treatment assignments.\nTheorem. If \\(\\mathbf Y(0) = \\mathbf Y(1)\\) then:\n\\[\n\\text{Pr}(P(t,\\mathbf Z,\\mathbf Y)\\leq \\alpha) \\leq \\alpha.\n\\]\nProof. Let \\(\\mathbf Z'\\) be distributed according to\n\\(\\text{Pr}_\\mathbf Z(\\cdot)\\), and define\n\\(\\mathbf Y' = \\mathbf Z'\\times \\mathbf Y(1)+(1-\\mathbf Z')\\times \\mathbf Y(0)\\). Given \\(t_0\\in \\mathbb R\\), we observe that:\n\\[\n\\text {Pr}(t(\\mathbf Z',\\mathbf Y')\\geq t_0 \\,\\vert\\,\\mathbf Y(0),\\,\\mathbf Y(1)) = \\sum _{\\mathbf Z '}\\text{Pr}_\\mathbf Z(\\mathbf Z')\\cdot I(t(\\mathbf Z',\\mathbf Y')\\geq t_0).\n\\]\nNow, if \\(\\mathbf Y(0) = \\mathbf Y(1)\\), we have \\(t(\\mathbf Z',\\mathbf Y') = t(\\mathbf Z',\\mathbf Y)\\), so that we may replace \\(\\mathbf Y'\\) with \\(\\mathbf Y\\)\nin the RHS of the previous equation. If, moreover, we choose \\(t_0= t(\\mathbf Z , \\mathbf Y)\\) we obtain:\n\\[\nP(t, \\mathbf Z, \\mathbf Y) = \\text {Pr}(t(\\mathbf Z',\\mathbf Y')\\geq t(\\mathbf Z,\\mathbf Y) \\,\\vert\\,\\mathbf Y(0),\\mathbf Y(1)).\n\\]\nIn other words, \\(P(t,\\mathbf Z, \\mathbf Y)\\) is a conditional \\(p\\)-value.\nTherefore:\n\\[\n\\text{Pr}(P(t,\\mathbf Z,\\mathbf Y)\\leq \\alpha \\,\\vert\\,\\mathbf Y(0),\\mathbf Y(1)) \\leq \\alpha.\n\\]\nSince this is valid for any value of \\(\\mathbf Y (0)\\) and \\(\\mathbf Y(1)\\), the thesis follows.\nIn the usual setting of causal inference, we interpret:\n\\(Z_i\\) as the treatment assignment for the \\(i\\)-th statistical unit, \\(Z_i = 0,1\\)\nstanding for “treatment” and “control”, respectively.\n\\(Y_i(1)\\) and \\(Y_i(0)\\) as the potential outcomes for the \\(i\\)-th unit under treatment and control, respectively.\n\\(Y_i\\) as the observed outcome for the \\(i\\)-th unit.\n\\(t(\\cdot)\\) as a test statistic used to test the null hypothesis \\(\\mathbf Y(1)= \\mathbf Y (0)\\).\n\\(P(t,\\mathbf Z,\\mathbf Y)\\) is the randomization \\(p\\)-value of \\(t(\\mathbf Z, \\mathbf Y)\\) in a Fisher Randomization Test.\nFisher’s “sharp” null hypothesis is an equality between random variables, the potential outcomes. Typical examples for the distribution \\(\\text{Pr}_\\mathbf Z(\\cdot)\\) are:\nCompletely Randomized Experiment (CRE):\n\\[\n\\text{Pr}_\\mathbf Z (\\mathbf Z) = \\begin{cases}\n\\binom N {n_1} ^{-1} & \\sum _{i=1}^N Z_i =n_1, \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n\\]\nBernoulli Randomized Experiment (BRE):\n\\[\n\\text{Pr}_\\mathbf Z (\\mathbf Z) = \\prod _{i=1} ^N \\pi^{Z_i}(1-\\pi)^{1-Z_i}.\n\\]\nAn example of test statistic is the difference in means between the treatment\nand control group, that can be written:\n\\[\nt(\\mathbf Z , \\mathbf Y) = \\sum_i c_i Y_i,\\qquad c_i=\\frac{Z_i}{\\sum _iZ_i} - \\frac{1-Z_i}{\\sum _i(1-Z_i)}.\n\\]\n\n\n\n",
    "preview": {},
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-06-07-p-values-and-measure-theory/",
    "title": "p-values and measure theory",
    "description": "Self-reassurance that p-value properties don't depend on regularity \nassumptions on the test statistic.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-06-07",
    "categories": [
      "Probability Theory",
      "Measure Theory",
      "Frequentist Methods",
      "Statistics"
    ],
    "contents": "\nLet \\((\\Omega, \\mathcal E, \\text{Pr})\\) be a probability space, where \\(\\Omega\\) is the\nspace of random outcomes, \\(\\mathcal E\\) the \\(\\sigma\\)-algebra of measurable\nevents, and \\(P\\) the probability measure.\nGiven a random variable \\(T\\colon \\Omega \\to \\mathbb R\\), define \\(p_T\\colon \\Omega \\to \\left[0,1\\right]\\) as:\n\\[\np_T(\\omega) = \\text{Pr}(\\{\\omega'\\in \\Omega\\,\\vert\\, T(\\omega')\\geq T(\\omega)\\})\n\\]\nTheorem. \\(p_T\\) is measurable and\n\\(\\text{Pr}(p_T\\leq \\alpha) \\leq \\alpha\\) for all \\(\\alpha \\in \\left[0,1\\right]\\).\nEquality holds if and only if there exists a\nsequence \\(\\{\\omega_n\\}_{n\\in \\mathbb N}\\) such that\n\\(p_T(\\omega_n) \\leq \\alpha\\), and \\(p_T(\\omega _n)\\to \\alpha\\) as \\(n \\to \\infty\\).\nProof. Let \\(\\alpha\\in\\left[0,1\\right]\\), and denote:\n\\[\nE_T(\\omega) = \\{\\omega'\\in \\Omega\\,\\vert\\, T(\\omega')\\geq T(\\omega)\\},\n\\]\nso that \\(p_T(\\omega) = \\text{Pr}(E_T(\\omega))\\).\nAssume first that there exists \\(\\omega_\\alpha \\in p_T^{-1}(\\alpha)\\), that is to\nsay \\(\\text{Pr}(E_T(\\omega)) = \\alpha\\). We can show that:\n\\[\nN_T(\\omega_\\alpha) = \\{\\omega \\vert p_T(\\omega) \\leq \\alpha\\} \\backslash E_T(\\omega_\\alpha)\n\\]\nis a measurable, zero probability set, which proves the thesis for this\nparticular case. As a matter of fact, for any \\(\\omega \\in \\Omega\\), if\n\\(p_T(\\omega)\\leq \\alpha\\) and \\(T(\\omega) <T(\\omega_\\alpha)\\), then we must have:\n\\[\n\\text{Pr}(\\{\\omega'\\in \\Omega\\,\\vert\\,\n                                                                T(\\omega_\\alpha)>T(\\omega')\\geq T(\\omega)\\}\n                                                                ) = p_T(\\omega) - \\alpha=0.\n\\]\nIf \\(t_* = \\inf_{p_T(\\omega)\\leq \\alpha}T(\\omega)\\) and\n\\(\\{a _n\\}_{n \\in \\mathbb N}\\) is a sequence in \\(\\Omega\\) such that\n\\(T(a _n)\\to t_*\\) as \\(n\\to \\infty\\), then:\n\\[\nN_T(\\omega _\\alpha) \\subseteq \\cup _n \\{\\omega'\\in \\Omega\\,\\vert\\,\n                                                                T(\\omega_\\alpha)>T(\\omega')\\geq T(a_n)\\},\n\\]\nthe right hand side being a probability zero set.\nIf \\(p_T^{-1}(\\alpha)\\) is empty, let\n\\(\\alpha^* = \\sup _{p_T(\\omega)\\leq \\alpha}p(\\omega)\\), and let\n\\(\\{b _n\\}_{n\\in \\mathbb N}\\) be a sequence in \\(\\Omega\\) such that\n\\(p_T(b_n)\\to \\alpha^*\\) as \\(n\\to \\infty\\). Then:\n\\[\n\\{\\omega \\vert p_T(\\omega) \\leq \\alpha\\}=\n\\{\\omega \\vert p_T(\\omega) \\leq \\alpha^*\\}=\n\\cup _n \\{\\omega \\vert p_T(\\omega) \\leq p_T(b_n)\\},\n\\]\nso that, from the particular case proved earlier, we have:\n\\[\n\\text{Pr}(p_T \\leq \\alpha) = \\lim _{n \\to \\infty} \\text{Pr}(p_T \\leq p_T(b_n)) \\leq \\lim _{n \\to \\infty} p_T(b_n) = \\alpha ^* \\leq \\alpha,\n\\]\nas was to be proved.\n\n\n\n",
    "preview": {},
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-05-20-linear-regression-with-autocorrelated-noise/",
    "title": "Linear regression with autocorrelated noise",
    "description": "Effects of noise autocorrelation on linear regression. Explicit formulae and a simple simulation.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-05-25",
    "categories": [
      "Statistics",
      "Regression",
      "Time Series",
      "Linear Models",
      "Model Misspecification",
      "R"
    ],
    "contents": "\nConsider two time series \\(Y_t\\) and \\(X_t\\) such that:\n\\[\nY_t =  X_t \\cdot \\beta+\\eta_t                               \\tag{1}\n\\]\nwhere \\(\\eta_t\\) is \\(\\text{AR}(1)\\) noise:\n\\[\n\\eta_{t+1} = \\alpha \\eta_t + \\epsilon_t, \\qquad \\epsilon _t \\sim \\mathcal N(0,\\sigma^2_0)                                                               \\tag{2}\n\\]\nBy iteration of (2), we see that \\(\\eta_t\\) has gaussian unconditional distribution:\n\\[\n\\eta_t \\sim \\mathcal N (0, \\sigma ^2),\\qquad \\sigma^2 \\equiv \\frac{\\sigma^2_0}{1-\\alpha ^2}                             \\tag{3}\n\\]\nso that individual observations of \\((X_t,\\,Y_t)\\) are distributed according to a perfectly specified linear model.\nThis does not mean that, given observational data \\(\\{(X_t,\\,Y_t)\\}_{t = 1,\\,2,\\,\\dots,\\,T}\\), we are allowed to make standard linear model assumptions to perform valid inference on the parameters \\(\\beta\\) and \\(\\sigma\\) of Eqs. (1) and (3). Since the noise terms \\(\\eta _t\\) are not independent draws from a single distribution, but are rather autocorrelated, the usual OLS variance estimate under linear model assumptions will be biased, as we show below 1.\nIt is fairly easy to work out the consequences of autocorrelation. Suppose, more generally, that the error term \\(\\eta _t\\) is a stationary time series with unconditional mean \\(\\mathbb E(\\eta_t)=0\\) and unconditional variance \\(\\text{Var}(\\eta _t)=\\sigma ^2\\). The OLS estimate of \\(\\beta\\) is2:\n\\[\n\\hat \\beta =(\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T\\mathbf Y=\\beta + (\\mathbf X^T\\mathbf X)^{-1} \\mathbf X^T \\mathbf{η}, \\tag{4}\n\\]\nwhich is unbiased since \\(\\mathbb E (\\mathbf{η}) = 0\\). The estimate of the noise variance \\(\\sigma ^2\\), on the other hand:\n\\[\n\\begin{split}\n\\hat \\sigma ^2  & = \\frac{(\\mathbf Y - \\mathbf X\\hat \\beta)^T(\\mathbf Y - \\mathbf X\\hat \\beta)}{N-p}= \\frac{\\mathbf{η}^T(\\mathbf 1-\\mathbf H)\\mathbf{η} }{N-p} \\\\\n\\mathbb E (\\hat \\sigma ^2) & = \\dfrac{\\text {Tr}\\left[(\\mathbf 1- \\mathbb E(\\mathbf H))\\cdot  \\text {Cor}(\\mathbf{η})\\right]}{N-p}\\sigma ^2                     \n\\end{split}\n\\]\nwhere \\(\\mathbf H = \\mathbf X(\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T\\) as usual, and we have used the fact that \\(\\mathbb {V}( \\mathbf{η} ) = \\sigma ^2 \\cdot \\text {Cor}(\\mathbf{η})\\) (since each \\(\\eta_t\\) has the same unconditional variance \\(\\sigma ^2\\)). Hence the \\(\\hat \\sigma ^2\\) OLS estimate is biased if \\(\\text{Cor}(\\mathbf{η})\\neq \\mathbf 1\\).\nSimilarly, the variance-covariance matrix of the OLS \\(\\hat \\beta\\) estimator is:\n\\[\n\\mathbb V (\\hat \\beta) = \\mathbb E\\left[(\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T\\text {Cor}(\\mathbf{η})\\mathbf X (\\mathbf X^T\\mathbf X)^{-1} \\right]\\sigma^2\n\\]\nwhereas its OLS estimate is:\n\\[\n\\hat {\\mathbb V} (\\hat \\beta) = (\\mathbf X^T\\mathbf X)^{-1} \\hat \\sigma ^2\n\\]\nwhich is biased for \\(\\text{Cor}(\\mathbf{η})\\neq \\mathbf 1\\).\nEven though the variance estimators are themselves biased, the biases could still vanish in the asymptotic limit. This is the case for \\(\\hat \\sigma ^2\\), as we can see by rewriting:\n\\[\n\\dfrac{\\mathbb E (\\hat \\sigma ^2)}{\\sigma ^2}-1 = -\\dfrac{1}{{N-p}}\\text {Tr}\\left[\\mathbb E(\\mathbf H)^T\\cdot(\\text {Cor}(\\mathbf{η})-\\mathbf 1)\\cdot \\mathbb E(\\mathbf H)\\right]                      \n\\]\nwhere we have used the projector properties of \\(\\mathbf H\\) to recast the trace in terms of a symmetric operator. In principle, nothing prevents the operator above to have \\(O(N)\\) eigenvalues, which would make the \\(\\hat \\sigma ^2\\) estimator asymptotically biased3. In realistic cases, one expects the correlations \\(\\text{Cor}(\\eta_t,\\eta_{t'})\\) to decay exponentially with \\(\\vert t - t'\\vert\\) 4 , in which case the trace is bounded to be of \\(O(p)\\), and \\(\\mathbb E(\\hat \\sigma ^2) \\to \\sigma ^2\\) as \\(N\\to \\infty\\).\nFor \\(\\hat {\\mathbb V} (\\hat \\beta)\\) things are not so favorable. It is enough to consider a special case of a plain intercept term: \\(X=1\\). In this case, we find with some manipulations:\n\\[\n\\begin{split}\n\\mathbb V (\\hat \\beta) &= \\frac{\\sigma ^2}{N}\\left(1+\\frac{1}{N}\\sum _{t\\neq t'} \\text{Cor}(\\eta_t,\\eta_{t'})\\right),\\\\\n\\mathbb E(\\hat {\\mathbb V} (\\hat \\beta)) & = \\frac{\\sigma ^2}{N}\\left(1-\\frac{1}{N(N-1)}\\sum _{t\\neq t'} \\text{Cor}(\\eta_t,\\eta_{t'})\\right)\n\\end{split}\n\\]\nSince \\(\\sum _{t\\neq t'}\\text{Cor}(\\eta_t,\\eta_{t'})=O(N)\\), we see that:\n\\[\n\\lim _{N\\to \\infty} \\dfrac{\\mathbb E(\\hat {\\mathbb V} (\\hat \\beta))}{\\mathbb V(\\hat \\beta)}\\neq 1\n\\]\nwhich amounts to say that \\(\\hat {\\mathbb V} (\\hat \\beta)\\) is asymptotically biased5.\nIllustration\nThe (foldable) block below defines helpers to simulate the results of linear regression on data generated according to \\(Y_t = f(X_t) + \\eta _t\\). These are the same functions used in my previous post on misspecification and sandwich estimators - slightly adapted to the current case.\n\n\nShow code\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nrxy_fun <- function(rx, f, reps) {\n  res <- function(n) {\n    x <- rx(n)  # X has marginal distribution 'rx'\n    y <- f(x) + reps(x)  # Y has conditional mean 'f(x)' and noise 'reps(x)'\n    return(tibble(x = x, y = y))  \n  }\n  return(structure(res, class = \"rxy\"))\n}\n\nplot.rxy <- function(x, N = 1000, seed = 840) {\n  set.seed(seed)\n  \n  ggplot(data = x(N), aes(x = x, y = y)) +\n    geom_point(alpha = 0.3) + \n    geom_smooth(method = \"lm\", se = FALSE)\n}\n\nlmsim <- function(rxy, N = 100, vcov = stats::vcov, B = 1e3, seed = 840) \n{ \n  set.seed(seed)\n  \n  res <- list(\n    coef = matrix(nrow = B, ncol = 2), \n    vcov = vector(\"list\", B),\n    sigma2 = numeric(B)\n    )\n  colnames(res$coef) <- c(\"(Intercept)\", \"x\")\n  class(res) <- \"lmsim\"\n                \n  for (b in 1:B) {\n    .fit <- lm(y ~ ., data = rxy(N))\n    res$coef[b, ] <- coef(.fit)  # Store intercept and slope in B x 2 matrix\n    res$vcov[[b]] <- vcov(.fit)  # Store vcov estimates in length B list.\n    res$sigma2[[b]] <- sigma(.fit) ^ 2\n  }\n  \n  return(res)\n}\n\nprint.lmsim <- function(x) \n{\n  cat(\"Simulation results:\\n\\n\")\n  cat(\"* Model-trusting noise variance:\\n \")\n  print( mean(x$sigma2) )\n  cat(\"* Model-trusting vcov of coefficient estimates:\\n\")\n  print( avg_est_vcov <- Reduce(\"+\", x$vcov) / length(x$vcov) )\n  cat(\"\\n* Simulation-based vcov of coefficient estimates:\\n\")\n  print( emp_vcov <- cov(x$coef))\n  cat(\"\\n* Ratio (Model-trusting / Simulation):\\n\")\n  print( avg_est_vcov / emp_vcov )\n  return(invisible(x))\n}\n\n\nWe simulate linear regression on data generated according to:\n\\[\n\\begin{split}\nY_t &= 1 + X_t+\\eta_t,\\\\\nX_{t+1} &= 0.4 \\cdot X_t+Z^X_t,\\\\\n\\eta _{t+1} &= \\frac{1}{\\sqrt 2}\\eta _t +Z^\\eta_t\\\\\n\\end{split}\n\\]\nwhere \\(Z^{X,\\eta}_t\\sim \\mathcal N(0,1)\\). The noise \\(\\eta_t\\) is \\(\\text{AR}(1)\\), and results in the unconditional variance of the corresponding linear model \\(\\text{Var} (\\eta _t) = 2\\), twice the conditional variance \\(\\text{Var}(\\eta _{t+1}\\vert \\eta _t)=\\mathbb E(Z_t ^2)=1\\).\n\n\nrxy_01 <- rxy_fun(\n  rx = \\(n) 1 + arima.sim(list(order = c(1,0,0), ar = 0.4), n = n),\n  f = \\(x) 1 + x,\n  reps = \\(x) arima.sim(\n    list(order = c(1,0,0), ar = 1/sqrt(2)), \n    n = length(x) \n    )\n)\n\nplot(rxy_01)\n\n\n\nFrom the simulation below, we see that with \\(N=100\\) serial observations, \\(\\mathbb E(\\hat \\sigma ^2)\\) is relatively close to \\(\\sigma ^2 = 2\\), but the \\(\\mathbb E(\\hat {\\mathbb V} (\\hat \\beta))\\) grossly underestimates all entries (as can be seen from the last line of the output of lmsim() below).\n\n\nlmsim(rxy_01, N = 100)\n\nSimulation results:\n\n* Model-trusting noise variance:\n [1] 1.870606\n* Model-trusting vcov of coefficient estimates:\n            (Intercept)           x\n(Intercept)  0.03583159 -0.01663739\nx           -0.01663739  0.01659708\n\n* Simulation-based vcov of coefficient estimates:\n            (Intercept)           x\n(Intercept)  0.15486131 -0.02665435\nx           -0.02665435  0.02978162\n\n* Ratio (Model-trusting / Simulation):\n            (Intercept)         x\n(Intercept)   0.2313786 0.6241905\nx             0.6241905 0.5572928\n\nTo correctly estimate \\(\\mathbb V (\\hat \\beta)\\), we could try using the “autocorrelation-consistent” sandwich estimator sandwich::vcovHAC() 6. It turns out that, even with a relatively simple example like the present one, the sample size required for the HAC estimator’s bias to die out is unreasonably large (see below). With such large samples, one can probably obtain much better results by leaving out some data for model building, performing inference on the remaining data with a proper time-series model.\n\n\nlmsim(rxy_01, vcov = sandwich::vcovHAC, N = 100)\n\nSimulation results:\n\n* Model-trusting noise variance:\n [1] 1.870606\n* Model-trusting vcov of coefficient estimates:\n            (Intercept)           x\n(Intercept)  0.08787795 -0.02323242\nx           -0.02323242  0.02339146\n\n* Simulation-based vcov of coefficient estimates:\n            (Intercept)           x\n(Intercept)  0.15486131 -0.02665435\nx           -0.02665435  0.02978162\n\n* Ratio (Model-trusting / Simulation):\n            (Intercept)         x\n(Intercept)   0.5674623 0.8716182\nx             0.8716182 0.7854329\n\n\n\nlmsim(rxy_01, vcov = sandwich::vcovHAC, N = 500)\n\nSimulation results:\n\n* Model-trusting noise variance:\n [1] 1.974131\n* Model-trusting vcov of coefficient estimates:\n             (Intercept)            x\n(Intercept)  0.023032270 -0.005723968\nx           -0.005723968  0.005684149\n\n* Simulation-based vcov of coefficient estimates:\n             (Intercept)            x\n(Intercept)  0.029600757 -0.005993161\nx           -0.005993161  0.006152216\n\n* Ratio (Model-trusting / Simulation):\n            (Intercept)         x\n(Intercept)   0.7780973 0.9550834\nx             0.9550834 0.9239189\n\n\n\nlmsim(rxy_01, vcov = sandwich::vcovHAC, N = 1000)\n\nSimulation results:\n\n* Model-trusting noise variance:\n [1] 1.98033\n* Model-trusting vcov of coefficient estimates:\n             (Intercept)            x\n(Intercept)  0.011878079 -0.002771484\nx           -0.002771484  0.002849089\n\n* Simulation-based vcov of coefficient estimates:\n             (Intercept)            x\n(Intercept)  0.015085291 -0.002844716\nx           -0.002844716  0.002855522\n\n* Ratio (Model-trusting / Simulation):\n            (Intercept)         x\n(Intercept)   0.7873948 0.9742566\nx             0.9742566 0.9977471\n\n\nFor the linear model assumptions to hold, the \\((X_t,\\,Y_t)\\) pairs should come from independent realizations of the same time series, which is of course not the type of data we are usually presented with.↩︎\nAs usual we stack observations vertically in the \\(\\mathbf X\\) and \\(\\mathbf Y\\) matrices.↩︎\nFor an extreme case, suppose that \\(\\mathbf X = \\mathbf e\\) (no covariate except for an intercept term), and let the noise term be \\(\\eta _t = Z_0 + Z_t\\), where \\(Z_0\\) and \\(\\{Z_t\\}_{t=1,2,\\dots,T}\\) are independent \\(Z\\)-scores. One can easily see that, in this setting, \\(\\text {Cor}(\\eta) = \\frac{1}{2}(\\mathbf 1+\\mathbf e \\mathbf e^T )\\) and \\(\\text{Tr}(\\cdots) \\approx \\frac{N}{2}\\).↩︎\nFor instance, for the \\(\\text{AR}(1)\\) noise of Eq. (2), we have \\(\\text{Cor}(\\eta_t, \\eta_{t'})= \\alpha ^{\\vert t - t'\\vert}\\).↩︎\nThe difference \\(\\mathbb E(\\hat {\\mathbb V} (\\hat \\beta))-\\mathbb V(\\hat \\beta)\\) decays as \\(O(N^{-1})\\), which is of the same order of the estimation target \\(\\mathbb V (\\hat \\beta)\\). Not sure I’m using standard terminology here.↩︎\nDisclaimer: I haven’t read any theory about the HAC estimator, so I may be misusing it here, but I would have expected it to work relatively well on such an “easy” example. For illustrations on how to use sandwich estimators for first- and second-order linear model misspecification, you can read this post of mine.↩︎\n",
    "preview": "posts/2023-05-20-linear-regression-with-autocorrelated-noise/linear-regression-with-autocorrelated-noise_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-05-14-model-misspecification-and-linear-sandwiches/",
    "title": "Model Misspecification and Linear Sandwiches",
    "description": "Being wrong in the right way. With R excerpts.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-05-14",
    "categories": [
      "Statistics",
      "Regression",
      "Linear Models",
      "Model Misspecification",
      "R"
    ],
    "contents": "\nIntroduction\nTraditional linear models, such as the output of the R function lm(), are\noften loaded with a set of strong assumptions. Take univariate regression:\n\\[\nY = q+mX+\\varepsilon. \\tag{1}\n\\]\nThis equation assumes that:\nThe conditional mean \\(\\mathbb E(Y\\vert X) = q + mX\\), a linear function\nof \\(X\\).\nThe conditional variance \\(\\mathbb {V}(Y \\vert X)=\\mathbb{V}(\\varepsilon\\vert X)\\)\nis independent of \\(X\\).\nThe conditional distribution \\(Y\\vert X\\) is gaussian.\nIn a set of measurements \\(\\left\\{\\left(X_i,Y_i\\right)\\right\\}_{i = 1,\\, \\dots, \\,N}\\), \\(Y_i\\) and the set \\(\\left\\{ X_j, Y_j\\right\\} _{j\\neq i}\\) are conditionally independent of each other, given the value of the corresponding regressor \\(X_i\\).1\nThe last assumption is satisfied in many practical situations, and we will take it here for granted2. What happens when the first\nthree assumptions are violated (that is “frequently” to “almost always”,\ndepending on context)?\nA comprehensive discussion is provided by (Buja et al. 2019). These authors show that:\nIf the conditional mean \\(\\mathbb E (Y \\vert X)\\) is not linear (“first order misspecification”), then the Ordinary Least Squares (OLS) regression\ncoefficients \\(\\hat \\beta\\) consistently estimate:\n\\[\n\\beta \\equiv \\text{arg } \\min _{\\beta^\\prime} \\mathbb E((Y-X\\beta^\\prime)^2) \\tag{2}\n\\]\nwhich can be thought as the “best linear approximation of the response”3.\nBoth non-linearity in the sense of the previous point, and \\(X\\)-dependence in\n\\(\\mathbb{V}(Y \\vert X)\\) (“second order misspecification”) affect the sampling\ndistribution of \\(\\hat \\beta\\) and, in particular, \\(\\mathbb{V}(\\hat \\beta)\\),\nwhich is the relevant quantity for inference in the large-sample limit.\nBoth problems can be efficiently addressed through the so-called “sandwich” estimators for the covariance matrix of \\(\\hat \\beta\\) (White 1980), whose consistency is robust to both type of misspecification.\nDetails can be found in the mentioned reference. The rest of the post\nillustrates with examples how to compute “sandwich” estimates in R, and why\nyou may want to do so.\nFitting misspecified linear models in R\nThe {sandwich} package\n(available on CRAN) provides estimators for the regression coefficients’\nvariance-covariance matrix \\(\\mathbb V (\\hat \\beta)\\) that are robust to first\nand second order misspecification. These can be readily used with lm objects,\nas in the example below:\n\n\nfit <- lm(mpg ~ wt, data = mtcars)\n\nstats::vcov(fit)  # standard vcov (linear model trusting estimate)\n\n            (Intercept)        wt\n(Intercept)    3.525484 -1.005693\nwt            -1.005693  0.312594\n\nsandwich::vcovHC(fit)  # sandwich vcov (model-robust estimate)\n\n            (Intercept)         wt\n(Intercept)    5.889249 -1.7418581\nwt            -1.741858  0.5448011\n\nIt is important to note that both functions stats::vcov() and\nsandwich::vcovHC() employ the same point estimates of regression coefficients\nto compute \\(\\mathbb V (\\hat \\beta)\\):\n\n\nfit\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\nThe difference between these functions lies in the different assumptions they\nmake on the linear model residuals, which leads to different estimates\nfor \\(\\mathbb{V}(\\hat \\beta)\\).\nEffects of misspecification\nThis section illustrates some consequences of model misspecification through\nsimulation. The examples use:\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nFor convenience, we define some helpers to be used in the following\nexamples. The function below returns random generators for the generic additive\nerror model \\(Y = f(X) + \\varepsilon\\), where the distribution of the noise term\n\\(\\varepsilon\\) may in general depend on \\(X\\). Both \\(X\\) and \\(Y\\) are assumed here\nand below to be 1-dimensional.\n\n\nrxy_fun <- function(rx, f, reps) {\n  res <- function(n) {\n    x <- rx(n)  # X has marginal distribution 'rx'\n    y <- f(x) + reps(x)  # Y has conditional mean 'f(x)' and noise 'reps(x)'\n    return(tibble(x = x, y = y))  \n  }\n  return(structure(res, class = \"rxy\"))\n}\n\nplot.rxy <- function(x, N = 1000, seed = 840) {\n  set.seed(seed)\n  \n  ggplot(data = x(N), aes(x = x, y = y)) +\n    geom_point(alpha = 0.3) + \n    geom_smooth(method = \"lm\", se = FALSE)\n}\n\n\nThe following function simulates fitting the linear model y ~ x over multiple\ndatasets generated according to a function rxy().\n\n\nlmsim <- function(rxy, N = 100, vcov = stats::vcov, B = 1e3, seed = 840) \n{ \n  set.seed(seed)\n  \n  res <- list(coef = matrix(nrow = B, ncol = 2), vcov = vector(\"list\", B))\n  colnames(res$coef) <- c(\"(Intercept)\", \"x\")\n  class(res) <- \"lmsim\"\n                \n  for (b in 1:B) {\n    .fit <- lm(y ~ ., data = rxy(N))\n    res$coef[b, ] <- coef(.fit)  # Store intercept and slope in B x 2 matrix\n    res$vcov[[b]] <- vcov(.fit)  # Store vcov estimates in length B list. \n  }\n  \n  return(res)\n}\n\nprint.lmsim <- function(x) \n{\n  cat(\"Simulation results:\\n\\n\")\n  cat(\"* Model-trusting vcov (average of vcov estimates):\\n\")\n  print( avg_est_vcov <- Reduce(\"+\", x$vcov) / length(x$vcov) )\n  cat(\"\\n* Simulation-based vcov (vcov of coefficient estimates):\\n\")\n  print( emp_vcov <- cov(x$coef))\n  cat(\"\\n* Ratio (1st / 2nd):\\n\")\n  print( avg_est_vcov / emp_vcov )\n  return(invisible(x))\n}\n\n\nThe print method defined above shows a comparison of the covariance matrices\nobtained by:\nAveraging variance-covariance estimates from the various simulations, and\nTaking the variance-covariance matrix of regression coefficients obtained\nin the simulations.\nThe first one can be considered a “model-trusting” estimate (where the actual\n“model” is specified by the vcov argument of lmsim(), i.e. stats::vcov and\nsandwich::vcovHC for the traditional and sandwich estimates, respectively).\nThe second one is a model-free simulation-based estimate of the true\n\\(\\mathbb{V}(\\hat \\beta)\\). The comparison between the two4\nprovides a measure of the asymptotic bias of the model-trusting estimate.\nExample 1: First order misspecification\n\\[\nY = X ^ 2 + \\varepsilon,\\quad X \\sim \\text{Unif} (0,1),\\qquad \\varepsilon \\sim \\mathcal N (0,0.01)\n\\]\n\n\nrxy_01 <- rxy_fun(\n  rx = runif,\n  f = \\(x) x^2,\n  reps = \\(x) rnorm(length(x), sd = .01)\n  )\n\n\nIn this model, \\(\\mathbb E (Y \\vert X)\\) is not linear in \\(X\\)\n(first order misspecification), but the remaining assumptions of the linear\nmodel hold. This is how a typical linear fit of data generated from this model\nlooks like:\n\n\nplot(rxy_01, N = 300)\n\n\n\nHere the effect of misspecification on the variance-covariance model trusting\nestimates is to underestimate true covariance values\n(by a factor as large as 40%!):\n\n\nlmsim(rxy_01)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n              (Intercept)             x\n(Intercept)  0.0002277348 -0.0003417356\nx           -0.0003417356  0.0006833282\n\n* Simulation-based vcov (vcov of coefficient estimates):\n              (Intercept)             x\n(Intercept)  0.0003367876 -0.0005662584\nx           -0.0005662584  0.0011488351\n\n* Ratio (1st / 2nd):\n            (Intercept)         x\n(Intercept)   0.6761971 0.6034976\nx             0.6034976 0.5948009\n\nThis is fixed by the sandwich::vcovHC() estimators:\n\n\nlmsim(rxy_01, vcov = sandwich::vcovHC)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n              (Intercept)             x\n(Intercept)  0.0003475834 -0.0005732957\nx           -0.0005732957  0.0011443449\n\n* Simulation-based vcov (vcov of coefficient estimates):\n              (Intercept)             x\n(Intercept)  0.0003367876 -0.0005662584\nx           -0.0005662584  0.0011488351\n\n* Ratio (1st / 2nd):\n            (Intercept)         x\n(Intercept)    1.032055 1.0124276\nx              1.012428 0.9960916\n\nExample 2: Second order misspecification\n\\[\nY = X + \\varepsilon,\\quad X \\sim \\text{Unif} (0,1),\\qquad \\varepsilon \\sim \\mathcal N (0,X)\n\\]\n\n\nrxy_02 <- rxy_fun(\n  rx = runif,\n  f = \\(x) x,\n  reps = \\(x) rnorm(length(x), sd = x)\n  )\n\nplot(rxy_02, N = 300)\n\n\n\nThis model is first-order consistent, but second-order misspecified (variance is\nnot independent of \\(X\\)). The effects on vcov model-trusting estimates is\nmixed: some covariances are underestimated, some are overestimated.\n\n\nlmsim(rxy_02)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n            (Intercept)           x\n(Intercept)  0.01344466 -0.02014604\nx           -0.02014604  0.04008595\n\n* Simulation-based vcov (vcov of coefficient estimates):\n             (Intercept)           x\n(Intercept)  0.005456494 -0.01417346\nx           -0.014173461  0.04834196\n\n* Ratio (1st / 2nd):\n            (Intercept)         x\n(Intercept)    2.463974 1.4213920\nx              1.421392 0.8292164\n\nAgain, this large bias is corrected by the sandwich estimator:\n\n\nlmsim(rxy_02, vcov = sandwich::vcovHC)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n             (Intercept)           x\n(Intercept)  0.005637138 -0.01451506\nx           -0.014515056  0.04909868\n\n* Simulation-based vcov (vcov of coefficient estimates):\n             (Intercept)           x\n(Intercept)  0.005456494 -0.01417346\nx           -0.014173461  0.04834196\n\n* Ratio (1st / 2nd):\n            (Intercept)        x\n(Intercept)    1.033106 1.024101\nx              1.024101 1.015653\n\nExample 3: sample size effects\nThe sandwich estimators only become unbiased in the large sample\nlimit. For instance, in our previous Example 1, the sandwich covariance\nestimates require sample sizes of \\(N \\approx 50\\) or larger, in order for their\nbias to be relatively contained (\\(\\lesssim 10\\%\\)). With a small sample size:\n\n\nlmsim(rxy_01, N = 10, vcov = sandwich::vcovHC)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n             (Intercept)           x\n(Intercept)  0.008253143 -0.01356350\nx           -0.013563503  0.02691423\n\n* Simulation-based vcov (vcov of coefficient estimates):\n             (Intercept)            x\n(Intercept)  0.005084963 -0.008573385\nx           -0.008573385  0.017136158\n\n* Ratio (1st / 2nd):\n            (Intercept)        x\n(Intercept)    1.623049 1.582048\nx              1.582048 1.570611\n\nFor such small sample sizes, however, one should probably also keep into account the bias in the point estimate \\(\\hat \\beta\\) itself, so that the bias in the variance \\(\\mathbb V (\\hat \\beta)\\) becomes a kinda second-order problem.\nExample 4: variance underestimation and overestimation\nAccording to the heuristics of (Buja et al. 2019), the linear model trusting\nvariances \\(\\mathbb V (\\hat \\beta)_{ii}\\) tend to underestimate (overestimate) the\ntrue variances:\nIn the presence of non-linearity, when the strong deviations from linearity\nare far away from (close to) the center of the regressor distribution.\nIn the presence of heteroskedasticity, when the regions of high variance are\nfar away from the (close to) the center of the regressor distribution.\nWe illustrate the second case. Consider the following two models:\n\\[\nY = X + \\varepsilon,\\quad X \\sim \\text{Unif} (0,1),\\qquad \\varepsilon \\sim \\mathcal N (0,\\vert X-\\frac{1}{2}\\vert )\n\\]\n\n\nrxy_04a <- rxy_fun(\n  rx = runif,\n  f = \\(x) x,\n  reps = \\(x) rnorm(length(x), sd = abs(0.5 - x))\n  )\n\nplot(rxy_04a)\n\n\n\n\\[\nY = X + \\varepsilon,\\quad X \\sim \\text{Unif} (0,1),\\qquad \\varepsilon \\sim \\mathcal N (0,\\frac{1}{2}-\\vert X-\\frac{1}{2}\\vert )\n\\]\n\n\nrxy_04b <- rxy_fun(\n  rx = runif,\n  f = \\(x) x,\n  reps = \\(x) rnorm(length(x), sd = 0.5 - abs(0.5 - x))\n  )\n\nplot(rxy_04b)\n\n\n\nIn agreement with the heuristics, we have, for the first model:\n\n\nlmsim(rxy_04a)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n             (Intercept)            x\n(Intercept)  0.003326042 -0.004989057\nx           -0.004989057  0.009977552\n\n* Simulation-based vcov (vcov of coefficient estimates):\n             (Intercept)            x\n(Intercept)  0.005390525 -0.009154439\nx           -0.009154439  0.018296535\n\n* Ratio (1st / 2nd):\n            (Intercept)         x\n(Intercept)   0.6170162 0.5449878\nx             0.5449878 0.5453247\n\nand, for the second model:\n\n\nlmsim(rxy_04b)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n             (Intercept)            x\n(Intercept)  0.003420946 -0.005150512\nx           -0.005150512  0.010300847\n\n* Simulation-based vcov (vcov of coefficient estimates):\n             (Intercept)            x\n(Intercept)  0.001590907 -0.001503471\nx           -0.001503471  0.003131620\n\n* Ratio (1st / 2nd):\n            (Intercept)        x\n(Intercept)    2.150312 3.425748\nx              3.425748 3.289303\n\nIt is interesting to notice that, far away from the large-sample limit, the\nsandwich estimates also have a bias (as discussed in the previous example),\nbut the bias leads to an overestimate of \\(\\mathbb V (\\hat \\beta)\\)\nin both cases5:\n\n\nlmsim(rxy_04a, N = 10, vcov = sandwich::vcovHC)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n            (Intercept)          x\n(Intercept)  0.07714254 -0.1302820\nx           -0.13028198  0.2595908\n\n* Simulation-based vcov (vcov of coefficient estimates):\n            (Intercept)          x\n(Intercept)  0.05560994 -0.0957307\nx           -0.09573070  0.1947398\n\n* Ratio (1st / 2nd):\n            (Intercept)        x\n(Intercept)    1.387208 1.360922\nx              1.360922 1.333013\n\nlmsim(rxy_04b, N = 10, vcov = sandwich::vcovHC)\n\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n            (Intercept)           x\n(Intercept)  0.05301354 -0.07223407\nx           -0.07223407  0.13959714\n\n* Simulation-based vcov (vcov of coefficient estimates):\n            (Intercept)           x\n(Intercept)  0.02725563 -0.03408101\nx           -0.03408101  0.06735272\n\n* Ratio (1st / 2nd):\n            (Intercept)        x\n(Intercept)    1.945049 2.119481\nx              2.119481 2.072628\n\nConclusions\nSandwich estimators provide valid inference for parameter covariances and\nstandard errors in misspecified linear regression settings.\nThese model-robust tools are available in R through\n{sandwich} (which also provides\nmethods for more general glm objects).\nFor fairly large datasets, this model-robust approach can be coupled with data\nsplitting, leading to a modeling procedure which I’m finding to be quite solid\nand versatile in practice:\nPerform data exploration and model selection on a separate portion of data.\nThis is to avoid biasing inferential results with random selective procedures.\nOnce a reasonable model is found, fit the model on the remaining data,\nadopting robust covariance estimates for model parameters.\nThis works very well with independent data for which a (generalized) linear\nmodel can provide a useful parametric description. Generalizations may be\ndiscussed in a separate post.\n\n\n\nBuja, Andreas, Lawrence Brown, Richard Berk, Edward George, Emil Pitkin, Mikhail Traskin, Kai Zhang, and Linda Zhao. 2019. “Models as Approximations i.” Statistical Science 34 (4): 523–44.\n\n\nWhite, Halbert. 1980. “A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.” Econometrica: Journal of the Econometric Society, 817–38.\n\n\n\nThis is already somewhat implicit in the representation (1), that\nmodels \\(Y\\) and \\(X\\) as single random variables. The reason for stating this condition in an apparently convoluted way, rather than a simpler “data points \\((X_i,Y_i)\\) are independent draws from the same joint distribution”, is that this formulation includes cases where the \\(X_i\\)’s are not independent, cf. the following note.↩︎\nThere are of course important exceptions, like time series or spatial data. Noteworthy, our formulation of strict linear model assumptions can also cover some cases of temporal or spatial dependence in the regressors \\(X_i\\), provided that such dependence is not reflected on \\(Y_i \\vert X_i\\).↩︎\nAccording to an \\(L_2\\) loss criterion.↩︎\nI use an element-wise ratio,\nin order to avoid confusion from the different scales involved in the various\nentries of \\(\\mathbb V (\\hat \\beta)\\).↩︎\n\nI don’t know whether this result (that sandwich estimates are, at worst,\noverestimates) is a general one.↩︎\n",
    "preview": "posts/2023-05-14-model-misspecification-and-linear-sandwiches/misspecification-and-linear-sandwiches_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-05-12-consistency-and-bias-of-ols-estimators/",
    "title": "Consistency and bias of OLS estimators",
    "description": "OLS estimators are consistent but generally biased - here's an example.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-05-12",
    "categories": [
      "Statistics",
      "Regression",
      "Linear Models",
      "Model Misspecification"
    ],
    "contents": "\nGiven random variables \\(Y\\colon \\Omega \\to \\mathbb R\\) and\n\\(X\\colon \\Omega \\to \\mathbb R ^{p}\\) defined on an event space \\(\\Omega\\), denote:\n\\[\n\\beta = \\arg \\min _{\\beta ^\\prime } \\mathbb E[(Y-X \\beta^\\prime )^2]= \\mathbb E(X^TX)^{-1}\\mathbb E(X^TY), \\tag{1}\n\\]\nso that \\(X \\beta\\) is the best linear predictor of \\(Y\\) in terms of \\(X\\) (\\(X\\) is\nregarded as a row vector).\nLet \\((\\textbf Y, \\textbf X)\\) be independent samples from the joint \\(XY\\)\ndistribution, with independent observations stacked vertically in \\(N \\times 1\\)\nand \\(N \\times p\\) matrices respectively, as customary. Then the usual Ordinary\nLeast Squares (OLS) estimator of \\(\\beta\\) is given by:\n\\[\n\\hat \\beta = \\arg \\min _{\\beta ^\\prime}(\\textbf Y - \\textbf X \\beta ^\\prime)^2=(\\textbf X^T\\textbf X)^{-1} \\textbf X^T \\textbf Y. \\tag{2}\n\\]\nThis is a consistent, but generally biased estimator of \\(\\beta\\).\nComparing Eqs. (1) and (2), consistency follows immediately\nfrom the law of large numbers and continuity. In order to show that\n\\(\\mathbb E (\\hat \\beta) \\neq \\beta\\) in general, it is sufficient to provide an\nexample.\nConsider, for instance (example adapted from D.A. Freedman):\n\\[\nX \\sim \\mathcal N (0, 1),\\qquad Y=X(1+aX^2)\n\\]\nRecalling that \\(\\mathbb E (X^4) = 3\\) for the standard normal, we have:\n\\[\n\\beta = 1+3a,\n\\]\nwhere we have ignored a potential intercept term (which would vanish here, since\n\\(\\mathbb E (Y) = 0\\)). To compute \\(\\mathbb E (\\hat \\beta)\\), we use the identity\n\\(\\frac{e^{-z}}{z} = \\intop _1 ^\\infty \\text d t\\, e ^{-zt}\\) to rewrite this\nexpected value as:\n\\[\n\\begin{split}\n\\mathbb E (\\hat \\beta) & =  (2 \\pi)^{-N/2}\n    \\intop \\text d\\textbf X \\,e^{-\\sum _j X_i ^2 /2}\n                                    \\dfrac{\\sum _i X_i^2(1+aX_i^2)}{\\sum _i X_i^2} = \\frac{N}{2}\\intop_1 ^\\infty \\text d t\\,I(t) \\\\\nI(t)                                     & \\equiv (2 \\pi)^{-N/2} \\intop \\text d\\textbf X\\,\n                                                        e^{-t \\sum _j X_j ^2 /2}X_1^2(1+aX_1^2)\n\\end{split}\n\\]\nThe inner integral can be computed easily:\n\\[\nI(t) = t^{-\\frac{N}{2}}(\\frac{1}{t}+a\\frac{3}{t^2})\n\\]\nand we eventually find:\n\\[\n\\mathbb E (\\hat \\beta) = 1+3 a\\frac{N}{N+2}\n\\]\nThe bias is thus given by:\n\\[\n\\beta - \\mathbb E (\\hat \\beta) = \\frac{6a}{N+2}\n\\]\nThis vanishes linearly, in agreement with the fact that\n\\(\\sqrt N (\\hat \\beta - \\beta )\\) converges in probability to a gaussian with\nzero mean and finite variance (which requires the bias to be \\(o(N^{-1/2})\\)).\n\n\n\n",
    "preview": {},
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-05-01-magic-piggy-bank/",
    "title": "Bayes, Neyman and the Magic Piggy Bank",
    "description": "Compares frequentist properties of credible intervals and confidence \nintervals in a gambling game involving a magic piggy bank.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-05-01",
    "categories": [
      "Statistics",
      "Confidence Intervals",
      "Frequentist Methods",
      "Bayesian Methods"
    ],
    "contents": "\nIntro\nFrequentist and Bayesian approaches to statistical inference are motivated by\ndifferent interpretations of the concept of probability.\nThese philosophical differences can, at times, shadow the comparably important\noperational differences between the two frameworks, whose methods proceed, at\nthe end of the day, from the same mathematical theory.\nFrom the purely operational point of view, the\nquestion “Bayesian or Frequentist?” can (and should) be answered by\nobjective criteria, rather than subjective opinions. As one could\nexpect, the answer is in general neither “Frequentist” nor “Bayesian”,\nbut rather “It depends”.\nTo illustrate this, I will discuss an hypothetical game that revolves around\nreporting measurements and correctly quantifying uncertainty. As we shall see,\nthe winning strategies can be either Frequentist or Bayesian in spirit,\ndepending on a variation of the actual rules of the game.\nReporting measurements\nAll scientific measurements come with an associated uncertainty, which can be\nexpressed in the form of an interval that is supposed to contain the object\nof measurement. In the Frequentist and Bayesian\nframeworks, these intervals are traditionally dubbed Confidence and Credible\nintervals, respectively. While, superficially, these can be both characterized\nas “covering the true value with probability \\(p\\)”, the word “probability”\nhas quite different connotations in the two cases, and confusing them can lead\nto irrational thought or, as in the imaginary game described below, financial\nruin.\nMagic Piggy Bank\nThere are two players, called the Bookmaker and the Gambler, that compete\nagainst each other in a gambling game1. The interactions between these two players are mediated by\nthe Magic Piggy Bank, a magic creature that acts as a sort of referee.\nThe Magic Piggy Bank contains infinite biased coins, and knows the probability\n\\(\\Theta\\) of giving “tails” for each one of them.\nA single iteration of the game proceeds as follows:\nThe Magic Piggy Bank ejects 2 a biased coin and gives it to the Bookmaker.\nThe Bookmaker can flip the coin an arbitrary number of times, to produce an\nestimate of \\(\\Theta\\), in the form of an interval \\(I\\). This must be accompanied\nby a payout, that is a number \\(p\\in \\left(0,1\\right)\\), for bets on the event\n\\(\\Theta \\in I\\). The resulting \\(I\\) and\n\\(p\\), together with the original data \\(X=(n_\\text{tosses}, n_\\text{tails})\\) from\nthe Bookmaker’s experiments, are reported to the Magic Piggy Bank.\nThe Magic Piggy Bank communicates the payout \\(p\\) to the Gambler, and reveals\nsome additional information. What particular information is revealed depends\non the variant of the game being played (see descriptions below).\nBased on the information received, the Gambler can choose to bet either\nin favor or against \\(\\Theta \\in I\\). When betting in favor, the Gambler pays \\(p\\)\nto the Bookmaker, who returns back \\(1\\) if \\(\\Theta \\in I\\) obtains. When betting\nagainst, the Bookmaker pays \\(p\\) to the Gambler, who returns back \\(1\\) if\n\\(\\Theta \\in I\\) obtains.\nThe Magic Piggy Bank reveals all data (\\(X\\), \\(I\\), \\(\\Theta\\)) to both players\nand the scores are settled.\nAs to the third step, we will consider three variants of the game:\nThe Magic Piggy Bank tells the Gambler the results of the Bookmaker’s tosses\n\\(X=(n_\\text{tosses}, n_\\text{tails})\\), as well as the actual interval \\(I\\).\nThe Magic Piggy Bank tells the Gambler the true value of \\(\\Theta\\).\nThe Gambler is given no additional information beyond the established payout\n\\(p\\).\nProblem\nSuppose that the Bookmaker and Gambler are forced to play indefinitely.\nWhat are the best strategies for these two players, according to\nthe three different variants A, B, and C described above3?\nAnalysis\nOne can readily verify that the Gambler’s gain (or, equivalently, the\nBookmaker’s loss) in a single iteration of the game is given by:\n\\[\nG=b\\cdot (\\chi_I (\\Theta)-p) \\tag{1}\n\\]\nwhere, \\(b\\) is equal to \\(\\pm 1\\) if the Gambler bets in favor or against,\nrespectively, and:\n\\[\n\\chi _I (\\Theta) = \\begin{cases}\n1 & \\Theta \\in I \\\\\n0 & \\Theta \\notin I\n\\end{cases} \\tag{2}\n\\]\nThe expected gain is given by:\n\\[\n\\mathbb E (G) = \\intop \\text{d}P(\\Theta,X) \\,b\\cdot (\\chi_I (\\Theta)-p), \\tag{3}\n\\]\nwhere \\(\\text{d} P(\\Theta, X)\\) denotes the joint probability measure of \\(\\Theta\\)\nand \\(X\\).\nLet’s now examine in detail the three different variants (A, B, C) of the game\ndescribed above.\nVariant A\nIn the first variant of the game, the Gambler is given the same information as the\nBookmaker. In particular, the choice to bet in favor or against, represented by\nthe sign \\(b\\), cannot depend on \\(\\Theta\\) (which the Gambler doesn’t know), and we\ncan rewrite the expected gain (3) as4:\n\\[\n\\begin{split}\n\\mathbb E (G) &= \\intop \\text{d}P(X) \\,b\\cdot \\intop \\text{d}P(\\Theta \\vert X) \\,(\\chi_I (\\Theta)-p) \\\\\n& = \\intop \\text{d}P(X) \\,b \\cdot \\left(\\text {Pr}(\\Theta \\in I \\vert X)-p\\right)\n\\end{split} \\tag{4}\n\\]\nwhere we have used the fact that, for any random variable \\(Y\\) and set \\(E\\), the\nfollowing relation holds:\n\\[\n\\mathbb E (\\chi _E (Y)) = \\text{Pr}(Y \\in E).\n\\]\nNow, since both \\(X\\) and \\(I\\) are known to the Gambler, the latter is (at least in\nprinciple) able to compute:\n\\[\nb_A \\equiv \\text{sgn}\\left(\\text {Pr}(\\Theta \\in I \\vert X)-p\\right) \\tag{5}\n\\]\nIn practice, in order to compute (5), the Gambler would need\nto know the overall distribution \\(\\pi (\\Theta)\\) of the coins \\(\\Theta\\) extracted\nfrom the Magic Piggy Bank, but this is something that can be accurately\nestimated in the long run, since the actual values of \\(\\Theta\\) are revealed\nat the end of each iteration 5.\nPlugging Eq. (5) into Eq. (4), we find:\n\\[\n\\mathbb E (G) = \\intop \\text{d}P(X) \\left|\\text {Pr}(\\Theta \\in I \\vert X)-p\\right|\\quad\\text{(Variant A)}   \\tag{6}.\n\\]\nComparing with (4), it is clear that\n(6) is the maximum expected gain, for any choice\nof \\(b\\). In other words, the choice \\(b_A\\) in Eq. (5) is an\noptimal one.\nFinally, from the Bookmaker’s point of view,\nEq. (6) represents a sure loss in the long run,\nthat can only be avoided by enforcing:\n\\[\n\\text {Pr}(\\Theta \\in I \\vert X)=p \\quad \\text{(Variant A)} \\tag{7}\n\\]\nIn order to ensure this, the Bookmaker needs to know as well the overall\ncoins’ distribution \\(\\pi (\\Theta)\\), and the same remarks made above for the\nGambler apply here.\nEquation (7) defines what is known as a\nBayesian credible interval.\nVariant B\nWe now consider the second variant of the rules, where the Gambler is told the\ntrue value of \\(\\Theta\\), but does not know the details of the Bookmaker’s\nmeasurement, except for the established payout \\(p\\). Using a reasoning similar\nto the previous section we rewrite:\n\\[\n\\begin{split}\n\\mathbb E (G) &= \\intop \\text{d}P(\\Theta) \\,b\\cdot \\intop \\text{d}P(X \\vert\\Theta) \\,(\\chi_I (\\Theta)-p) \\\\\n& = \\intop \\text{d}P(\\Theta) \\,b \\cdot \\left(\\text {Pr}(\\Theta \\in I \\vert \\Theta)-p\\right)\n\\end{split} \\tag{8}\n\\]\nand define6:\n\\[\nb_B \\equiv \\text{sgn}\\left(\\text {Pr}(\\Theta \\in I \\vert \\Theta)-p\\right)\\quad(\\text{Variant B}) \\tag{9}\n\\]\nwhich is easily shown to be the optimal betting strategy for the Gambler in the\npresent setting. In the long run, this sign can be accurately estimated by\nmodeling the conditional mean of \\(\\chi _I (\\Theta) - p\\) (as a function of\n\\(\\Theta\\) and \\(p\\)).\nIf the Gambler bets according to (9), the Bookmaker is forced\nto set payouts according to:\n\\[\n\\text {Pr}(\\Theta \\in I \\vert \\Theta)=p\\quad(\\text{Variant B}), \\tag{10}\n\\]\nin order to avoid a certain loss.\nEquation (10) defines what is known as a\nFrequentist confidence interval.\nVariant C\nIn the last case, the Gambler has no extra information beyond the payout \\(p\\),\nand the expected gain reduces to:\n\\[\n\\mathbb E (G)=b\\cdot \\left(\\text{Pr}(\\Theta \\in I)-p\\right),\\tag{11}\n\\]\nwhere \\(\\text{Pr}(\\Theta \\in I)\\) is the unconditional probability that \\(I\\) covers\n\\(\\Theta\\). The optimal betting choice is:\n\\[\nb_C \\equiv \\text{sgn}\\left(\\text {Pr}(\\Theta \\in I)-p\\right)\\quad(\\text{Variant C}) \\tag{12}\n\\]\nwhich forces the Bookmaker to set payouts according to:\n\\[\n\\text {Pr}(\\Theta \\in I)=p\\quad(\\text{Variant C}). \\tag{13}\n\\]\nThis is, by the way, satisfied by both the Bayesian and Frequentist intervals,\ndue to Eqs. (7) and (10), respectively.\nSummary of results\nProvided access to the same data used by the Bookmaker to\nproduce the interval \\(I\\) (Variant A), a rational Gambler would bet in favor of\n\\(\\Theta \\in I\\) if the probability of this event\nconditional to the observed the data is greater than the payout \\(p\\)\n(Eq. (5)).\nOn the other hand, given true value of \\(\\Theta\\) (Variant B), the optimal choice\nfor a Gambler is to bet on \\(\\Theta \\in I\\) if the probability of this event\nconditional to the ground truth is greater than \\(p\\)\n(Eq. (9)).\nFinally, in the lack of any of this information (Variant C), the most rational\nchoice is simply to bet on \\(\\Theta \\in I\\) if this event occurs more frequently\nthan \\(p\\) (Eq. (12)).\nWhen playing against the first two types of players, in order to avoid a certain\nloss, the Bookmaker must produce Bayesian credible intervals (Variant A) or\nFrequentist confidence intervals (Variant B). In the remaining case (Variant C),\nthe Bookmaker can either produce Bayesian or Frequentist intervals7.\nConclusions\nWhen I first learned about Bayesian and Frequentist inference, I remember most\ndiscussions were focused on the philosophical differences between these two\nschools of thought. There was little to no mention about the actual mathematical\nproperties of the constructs prescribed by the two formalisms, which made the\nchoice between “Bayesian” or “Frequentist” look like a mere matter of\ncommitting to one particular view.\nTechnically, what I did here was to compare the frequentist properties of\ncredible intervals and confidence intervals. I’m sure the literature,\nincluding the pedagogical one, is full of examples like this, and better ones8. With no pretense of originality, I believe that including more examples\nof this kind in the usual presentations can be beneficial to students and\npractitioners, and perhaps help them out of the ugly black-box of orthodoxy.\n\nThe introduction of bets as an expedient\nto operationally define subjective probabilities is historically due to the\nItalian mathematician\nBruno de Finetti.\nThe statistical analysis of the game proposed below can be given a Frequentist\ninterpretation.↩︎\nReaders are free to imagine this process in the way they find more convenient.↩︎\nWe assume that both\nplayers know from the outset which variant of the game they are playing to.↩︎\n\nWe denote (with some abuse of notation) by\n\\(\\text{d}P(\\Theta \\vert X)\\) the conditional probability measure of \\(\\Theta\\)\nconditioned on \\(X\\).↩︎\nIn the Bayesian spirit of (5),\nthe Gambler could for instance estimate \\(\\pi(\\Theta)\\) through Bayesian updates\nof a Dirichlet prior.↩︎\nNoteworthy, the random quantity in this equation is \\(I\\),\nwhereas \\(\\Theta\\) is regarded as fixed. This is in stark contrast with\nEq. (5), where \\(X\\) and \\(I\\) were fixed, and \\(\\Theta\\) was\nrandom.↩︎\n\nThere are, in fact, infinitely many more ways to produce intervals with the\nunconditional coverage property Eq. (13).↩︎\n\nI see that Jaynes (the father of the Maximum Entropy foundation of\nStatistical Mechanics, among other things) has a full essay paper on\nConfidence Intervals vs. Bayesian Intervals, which I haven’t read -\nthe abstract sounds a bit loaded to me, but it’s probably definitely worth to\nread.↩︎\n",
    "preview": {},
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-10-correlation-without-causation/",
    "title": "Correlation Without Causation",
    "description": "*Cum hoc ergo propter hoc*",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2023-03-30",
    "categories": [
      "Statistics"
    ],
    "contents": "\nIt is part of common knowledge that correlation does not require causation.\nAbsence of causation, say between a condition \\(p\\) and an effect \\(q\\), means that the realization of \\(p\\) has no influence on the presence of \\(q\\). If this is the case,\na statistical correlation between \\(p\\) and \\(q\\) can still be present, if the realization of \\(p\\) modifies our state of information about \\(q\\).\nAs an example, let \\(X,Y\\) be two conditionally independent binary random variables, with a common probability \\(\\Theta\\) of evaluating to one. Think, for instance,\nof a machine that produces pairs of identical biased coins, with a probability of tails \\(\\Theta\\).\nIf \\(\\Theta\\) is equal to a given value \\(\\theta\\), the joint probability distribution of \\(X\\) and \\(Y\\) is:\n\\[\n\\text {Pr}(X=x,Y=y\\vert \\Theta = \\theta) = B(x;\\theta)B(y;\\theta), \\tag{1}\n\\]\nwhere \\(B(z; \\theta) = \\theta ^z (1 - \\theta) ^ {1-z}\\).\nWhether or not this provides a satisfying probabilistic description of experiments on \\(X\\) and \\(Y\\) depends on context.\nFrom a frequentist point of view, if \\(\\Theta\\) is fixed once and for all, the right hand side of Eq. (1) correctly describes the experimental outcomes of \\(X\\) and \\(Y\\) for some value of \\(\\theta\\). On the other hand, if \\(\\Theta\\) can change from experiment to experiment in a random fashion, and we do not observe its values \\(\\theta\\), we clearly cannot use Eq. (1) as it stands, as its usage requires knowing \\(\\theta\\).\nFinally, from a bayesian’s point of view, if \\(\\Theta\\) is fixed but unknown, Eq. (1) does not describe our state of knowledge about \\(X\\) and \\(Y\\), because it assumes unavailable information (\\(\\Theta = \\theta\\)).\nIn the last two cases, what we’re actually after is the unconditional probability:\n\\[\n\\text{Pr}(X=x,\\,Y=y)=\\intop\\,\\text{d}P_\\Theta(\\theta) \\,\\text{Pr}(X=x,Y=y\\vert\\Theta = \\theta)\n\\tag{2}\n\\]\nwhere \\(\\text{d}P_\\Theta(\\theta)\\) can be regarded either as the actual probability distribution of \\(\\Theta\\) (in a frequentist framework) or as a subjective prior distribution (in a bayesian framework).\nPlugging Eq. (1) into (2), we find:\n\\[\n\\begin{split}\n\\text{Pr}(X=1,\\,Y=1) & = \\mathbb E(\\Theta)^2+\\text{Var}(\\Theta)\\\\\n\\text{Pr}(X=1,\\, Y=0)&=\\mathbb E(\\Theta)-\\mathbb E(\\Theta)^2-\\text{Var}(\\Theta)\\\\\n\\text{Pr}(X=0,\\, Y=1)&=\\mathbb E(\\Theta)-\\mathbb E(\\Theta)^2-\\text{Var}(\\Theta)\\\\\n\\text{Pr}(X=0,\\,Y=0) & = \\mathbb (1-\\mathbb E(\\Theta))^2+\\text{Var}(\\Theta) \\\\\n\\end{split}\n\\]\nIn particular, we have:\n\\[\n\\dfrac{\\text{Pr}(Y = 1 \\vert\\, X = 1)}{\\text {Pr}(Y=1)} = 1+\\frac{\\text{Var}(\\Theta)}{\\mathbb{E}(\\Theta)^2},\n\\tag{3}\n\\]\nwhich means that, unconditionally, \\(X\\) and \\(Y\\) are not independent, but in fact positively correlated1.\nObservations of this kind apply, mutatis mutandis, in many practical situations. For instance if we were modeling the time series of new visitors to a website, we could reasonably assume that the number of yesterday’s new visitors does not influence the number of today’s ones (if individual visitors are unlikely to interact with each other). Yet, it would be wrong to assume, and easy to disprove, that these two numbers are by themselves statistically independent, because yesterday’s new visitors carry useful background information on today’s potential new visitors.\nThe bottom line of the post is that lack of causation does not imply lack of correlation, which is logically equivalent to the original motto… but, for some strange reason, I find easier to forget.\n\nHere I’m using the word correlation in a loose sense, as in the popular motto.↩︎\n",
    "preview": {},
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-07-posi-2/",
    "title": "How to get away with selection. Part II: Mathematical Framework",
    "description": "Mathematicals details on Selective Inference, model misspecification and coverage guarantees.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2022-11-25",
    "categories": [
      "Statistics",
      "Selective Inference",
      "Model Misspecification"
    ],
    "contents": "\nIntroduction\nIn a previous post I introduced the problem of\nSelective Inference and illustrated, in a simplified setting, how selection\ngenerally affects the coverage of confidence intervals - when they are both\nselected and constructed using the same data. While the example was\n(hopefully) helpful to build some intuition, in order to discuss\n“How to get away with selection” in a comprehensive manner we need to make a\nfew clarifications. In particular, we need to answer the following questions:\nWhat is the target of our Selective Inference?\nWhat statistical properties would we like our inferences to have?\nSearching through the literature, I realized there exist a bunch of variations\non these two themes, which give rise to different mathematical formalisms.\nSpecifying these points is mandatory for any further discussion, so my main goal\nhere is to present these different points of view and explain some of their\npros and cons.\nMathematical Framework\nRegression and parameter estimation\nIn order to avoid getting carried away with too much abstraction, I will focus\non a specific type of problem, that is parameter estimation in regression. As\nfar as I can tell, this represents no serious loss in generality, and most of\nthe notions I’m going to outline would carry over to more general problems in a\nstraightforward manner.\nBroadly speaking, the goal of regression is to understand the dependence of a\nset of random variables \\(Y\\) from another set of random variables \\(X\\). More precisely,\nwe’re interested in the conditional probability distribution of \\(Y\\),\nconditioned on the observation of \\(X\\), which can always be represented as:\n\\[\nY = f(X)+\\varepsilon,\\qquad \\mathbb E(\\varepsilon|X)\\equiv 0.\n\\tag{1}\n\\]\nwhere \\(f(X) = \\mathbb E(Y|X)\\) is the conditional mean of \\(Y|X\\), and \\(\\varepsilon\\)\nis a random variable with vanishing conditional mean, sometimes called the “error term”.\nParameter estimation means that we have (somehow) chosen functional forms for\nthe conditional mean and for the probability distribution of the error term,\nand we want to provide estimates for the parameters defining these two functions.\nEnter selection\nNow, in many applications we actually don’t have much insight about the correct\nfunctional form \\(f\\), nor of the distribution of the error term \\(\\varepsilon\\).\nGiven a dataset of experimental observations of \\(Y\\) and \\(X\\), we are thus faced\nwith two tasks:\nSelection. Choose an adequate model \\(\\hat M = (\\hat f,\\,\\hat \\varepsilon)\\)\nfor the true \\(f\\) and \\(\\varepsilon\\), usually from a (more or less) pre-specified\nfamily of initial guesses \\(\\mathcal M =\\{(f_i,\\varepsilon_i)\\}_i\\), using a\n(more or less) pre-specified criterion.\nPost-Selection Inference. Perform inference with the chosen model. In the\nstudy case we’re considering, this amounts to provide confidence intervals for\nmodel parameters.\nIt is, of course, the need to use the same data for both tasks which gives rise\nto complications.\nInferential target\nWe now come to the first question raised in the Introduction, regarding the\nnature of the inferential target. And now more concretely:\nwhat are the true values of the parameters we’re trying to estimate?\nOne can appreciate that the answer necessarily depends on how we consider the\nfinal output of the modeling procedure:\n(Model Trusting) As the true data generating process, or\n(Assumption Lean) As an approximation of the (partially or totally\nunknown) data generating process, chosen in a data-driven fashion within\na family of initial guesses \\(\\mathcal M\\).\nAccording to the first interpretation, there’s no room for ambiguity: the\ntargets of our estimates should clearly be the true parameter values,\nwhose definition does not depend on any modeling choice. The second\ninterpretation, on the other hand, leaves a certain amount of\nfreedom in this respect. Here, I will follow the point of view advocated by\n(Berk et al. 2013), according to which the target parameters\nare those providing the best approximation1 to the true data generating\nprocess, according to the functional form chosen in the selection stage.\nI believe both positions have their merits and flaws, and which one is more\nappropriate largely depends on context. In a reductionist field like\nHigh Energy Physics, whose eventual goal is to explain the fundamental laws of\nNature, the Model Trusting point of view is usually taken,\nand with good reason. When studying more emergent phenomena, on the other hand,\nthe quest for fundamental laws is often meaningless (or at best wishful\nthinking), and the Assumption Lean standpoint looks more reasonable. In any\ncase, here the differences are not merely philosophical ones, as the two\ninterpretations give rise to different mathematical formalisms.\nIn the following posts, I will be mostly focusing on the Assumption Lean\npoint of view. In my opinion, this has two big advantages2:\nConceptual: Inferences have a well-defined meaning even when the model is\nmisspecified3 - which, apart from quite particular cases (see above),\naccounts for the great majority of cases encountered by data analysts in\nthe practice.\nMathematical: It allows to reduce the problem of selective inference to\nthat of simultaneous inference (more on this below).\nFor the latter type of problems, the theory of\nmultiple testing\nreadily provides at least conservative bounds.\nNotions of coverage\nIn addition to the conceptual distinction about the interpretation of the\nselected model, there is also a technical distinction regarding the type\nof coverage guarantees that selective confidence intervals should be endowed\nwith (this is the concrete version of the second question posed in the\nIntroduction).\nHere are some of the notions of coverage I’ve come across:\nMarginal coverage over the selected parameters. We bound at level \\(\\alpha\\)\nthe probability that our procedure constructs any non-covering confidence\ninterval for model parameters \\(\\beta_i\\). Denote by \\(\\widehat M\\) the selected model\nand, with abuse of notation, the corresponding set of selected parameters.\nIf \\(\\widehat{\\text{CI}}_i\\) are the confidence intervals for parameters \\(\\beta _i\\), we\nrequire:\n\\[\n\\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\in \\widehat M) \\geq 1-\\alpha\n\\tag{2}\n\\]\nConditional coverage over the selected parameters. We bound at level\n\\(\\alpha\\) the conditional\nprobability of constructing a non-covering confidence interval, conditioned on\nthe outcome of selection \\(\\widehat M\\). If \\(m\\) is the selected model, we require:\n\\[\n\\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\in m|\\,\\widehat M=m) \\geq 1-\\alpha\n\\tag{3}\n\\]\nFalse Coverage Rate. We bound at level \\(q\\)4 the expected fraction of\nnon-covering confidence intervals out of all intervals constructed:\n\\[\n\\mathbb E \\left( \\dfrac{|i \\in \\widehat M \\colon \\ \\beta_i \\in \\widehat{\\text{CI}}_i|}{|\\widehat M|} \\right)\n\\geq1-q\n\\tag{4}\n\\]\nwhere \\(|S|\\) denotes the cardinality of a set \\(S\\).\nNotice that the random variables in the previous equations are \\(\\widehat M\\) and\n\\(\\widehat{\\text{CI}}_i\\) (denoted by a hat), whereas the true coefficients \\(\\beta_i\\)\nand the selected set \\(m\\) in the case of conditional coverage\n(Eq. (3)) are fixed quantities.\nVariations of these measures focusing on single coefficients are also possible.\nIn practice, in the Assumption Lean framework I just introduced,\nall these coverage measures would not be computed\nunder the selected model’s probability distribution, but rather under a\npre-fixed, more general model for the true probability distribution of \\(Y\\)\nconditional on \\(X\\). We may, for instance, assume that the true error term\n\\(\\varepsilon\\) in Eq. (1) is gaussian with constant\n(\\(X\\)-independent) variance, without making any further assumption on \\(f(X)\\).\nWith enough data, we may even be able to bypass any assumption at all, and\ncompute all relevant quantiles using a bootstrap (Kuchibhotla et al. 2020).\nIn the Model Trusting framework, on the other hand, the conditional coverage\nmeasure would be computed under the selected model… and I’m honestly not\nsure whether it’s possible to make sense of the other two measures in this\nframework.\nSelective vs. Simultaneous Inference\nThe connection between selective and simultaneous inference can now be\nunderstood, through the notion of marginal coverage. In fact, suppose that we\nwere able to provide simultaneous coverage for all parameters\n(selected or not):\n\\[\n\\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i) \\geq 1-\\alpha\n\\tag{5}\n\\]\nThen, it’s easy to see that the same confidence interval would also provide\nmarginal coverage over the selected parameters. In order to see that, simply\nobserve that the simultaneous coverage event can be decomposed as:\n\\[\n(\\beta _i \\in \\widehat {\\text{CI}}_i\\,\\,\\forall i) = (\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\in \\widehat M) \\cap (\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\notin \\widehat M)\n\\]\nwhich implies that:\n\\[\n\\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i \\in \\widehat M) \\geq \\text{Pr}(\\beta _i \\in \\widehat{\\text{CI}}_i\\,\\,\\forall i) \\geq 1-\\alpha,\n\\tag{6}\n\\]\nthat is simultaneous coverage implies marginal coverage over the\nselected parameters. In fact, with a few more set-theory manipulations,\none can arrive to a powerful Lemma (see Kuchibhotla et al. 2020 for details):\ncontrolling the marginal coverage (2) at level \\(\\alpha\\)\nfor any model selection procedure5 is equivalent to controlling\nsimultaneous coverage for all possible model selections.\nThis provides us a first, very simple recipe for selective inference, which can\nbe applied whenever one is able to construct confidence intervals for parameters\nin the absence of selection: use any procedure (e.g. \nBonferroni corrections)\nwhich controls simultaneous coverage for all parameters we may select a priori.\nConclusions\nThis was a long and somewhat abstract post, so perhaps the best way to conclude\nis with some bottom lines:\nWhen performing model-based inference, nothing forces us to make working\nhypotheses about the correctness of the model we arrive at. Not making such\nassumptions corresponds to what I called an Assumption Lean framework.\nIn an Assumption Lean framework, the inferential targets are, in general,\nthe best approximations to the truth allowed by the selected model.\nThere exist many type of coverage guarantees for selective confidence\nintervals.\nBounding the probability of any false coverage statement\n(“marginal coverage over the selected parameters”) allows to turn a problem of\nselective inference into one of simultaneous inference.\nIn particular, it is worth to mention that the last observation lead us to a\nsimple recipe for constructing (somewhat conservative, but valid) selective\nconfidence intervals with marginal coverage. In the posts which follow,\nI will discuss some more advanced methods which produce confidence intervals\nsatisfying the requirements discussed here.\n\n\n\nBenjamini, Yoav, and Daniel Yekutieli. 2005. “False Discovery Rate–Adjusted Multiple Confidence Intervals for Selected Parameters.” Journal of the American Statistical Association 100 (469): 71–81.\n\n\nBerk, Richard, Lawrence Brown, Andreas Buja, Kai Zhang, and Linda Zhao. 2013. “Valid Post-Selection Inference.” The Annals of Statistics, 802–37.\n\n\nKuchibhotla, Arun K, Lawrence D Brown, Andreas Buja, Junhui Cai, Edward I George, and Linda H Zhao. 2020. “Valid Post-Selection Inference in Model-Free Linear Regression.” The Annals of Statistics 48 (5): 2953–81.\n\n\nWhere what’s to be considered best is defined in terms of some\nreasonable metric. For instance, for the conditional mean \\(f(X)\\) of a continuous\nresponse \\(Y\\), a convenient target \\(f^*(X)\\) within a prescribed family of\nfunctions \\(\\mathcal F\\) can be defined by\n\\(f^* =\\arg\\min _{\\phi \\in \\mathcal F} \\mathbb E (\\vert f(X) - \\phi (X)\\vert^2)\\).↩︎\nThere’s also a third advantage, which is that I find much harder to think\nabout selective inference from the Model Trusting point of view, hence to write\nblog posts about it - but that’s likely a limitation of my imagination, rather\nthan of the point of view itself.↩︎\nA cool word for “wrong”.↩︎\nWhy \\(q\\) and not \\(\\alpha\\)? Ask (Benjamini and Yekutieli 2005).↩︎\nIt is assumed that the selection is performed from a from a fixed family\nof models \\(\\mathcal M\\).↩︎\n",
    "preview": {},
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-18-posi/",
    "title": "How to get away with selection. Part I: Introduction",
    "description": "Introducing the problem of Selective Inference, illustrated through a simple simulation in R.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2022-11-14",
    "categories": [
      "Statistics",
      "Selective Inference",
      "R"
    ],
    "contents": "\nPrologue\nA few months back, for undocumented circumstances, my browser’s search\nhistory was full of terms like “parameter estimation with variable selection”,\nor “confidence intervals after cross-validation”, or again\n“linear model uncertainties after staring into the abyss”, …\nSparing you my rock bottom, I eventually stumbled upon the right keywords, and\nstarted digging into the mathematical aspects of Selective Inference, or\nPost-Model Selection Inference. Now, while my hands\nare still full of dirt, I’ve decided it’s the right moment to write some\nnotes about what I’ve learned - whose main recipient is the future me,\nwhich will otherwise inevitably forget what the present me thinks he\nknows. If you’re not the future me:\nWelcome 👋\nIf you have detected some imprecision, or have suggestions for this or the\nnext posts, you are more than welcome to create an issue on the source\nrepository of this blog.\nIntroduction\nBroadly speaking, the problem of Selective Inference is that of\nperforming valid statistical inferences when the actual questions of the data analysis are not fixed in advance, but rather selected through data examination. In model-based inference, this lack of\npre-determination usually stems from the (often unavoidable) practice of\nusing the same data to choose an adequate model for the data generating\nprocess and to perform inference. The intrinsic\nrandomness of the selection process has important consequences on the\nprobability of making different guesses about the selected questions,\nwhich, if not properly taken into account, can completely invalidate the\nanalysis results.\nIf this sounds unfamiliar, think about machine-learning: when training a\npredictive model on a given dataset, you would usually consider the\nerror on the same dataset as a poor (optimistic) estimate of the true\nmodel’s error rate, because the model was tuned to perform well on that\ndata in the first place. There we go, Selective Inference! A selection\nfrom an extended family of models1 is performed through data examination,\nand this event introduces a bias in the error estimate of the final\nmodel from training data.\nThe example from machine-learning also suggests a very simple-minded and\nrelatively a-theoretical approach to Selective Inference: data-splitting2. According to this method, we would use only part of the available data to select the questions to be answered by the analysis, while the remaining part would\nbe reserved to perform the actual inference. For this program to\nsucceed, there are however two important requirements: first, we must have\nenough data to ensure decent statistics for both the selection and inference\ntasks; and second, we must be able to split data in two independent\n(or close to independent) sets. This can suppose problems with, e.g.,\ntime-series data. If, on the other hand, these requirements cannot be met, we\nhave to resort to more sophisticated methods.\nAt this point, I would like to stress that the conceptual problems\nI’ve just pointed out will probably look obvious to any reader with a\ndecent intuition for probability3. What is less obvious, but in fact\na fairly active research field in statistics,\nis how to perform valid selective inferences when the “easy” solution of\ndata-splitting I mentioned above is not available. This is where theory\nre-enters the game, and what I’m going to ramble about in this and the next\nposts.\nIllustrations of Selective Inference\nEnough for the speech, let us see how selection can affect (and invalidate)\nclassical inference with a simple-minded simulation.\nSetting\nTo illustrate why naive classical inference can fail in the presence of\nselection, we consider a very simple regression\nproblem involving a single regressor \\(X\\) and a response \\(Y\\), where all the assumptions of the classical linear model hold. In fact, we will assume the true data generating process to be:\n\\[\nY = mX + \\varepsilon, \\qquad \\varepsilon \\sim \\mathcal N (0, \\sigma),\n\\tag{1}\n\\]\nwhere \\(\\varepsilon \\sim \\mathcal N (0, \\sigma)\\) means “\\(\\varepsilon\\) follows a gaussian distribution with mean \\(0\\) and standard deviation \\(\\sigma\\)”.\nA selective modeling procedure\nNow, suppose we are given a dataset of \\(N\\) independent observations\n\\((y_i, x_i)_{i = 1, \\,2,\\, \\dots,\\,N}\\), and we would like to study the\ndependence of \\(Y\\) from \\(X\\). Of course we don’t know the true law, Eq.\n(1), but by a stroke of luck (or by a Taylor expansion\nargument) we make the correct initial guess that such dependence is\nlinear in \\(X\\). We are, however, unsure whether it would be appropriate\nto also include an intercept term in the fit. We thus establish the\nfollowing selective modeling procedure:\nFit a linear model with intercept term,\n\\(Y = mX + q + \\varepsilon\\).\nStop if the intercept estimate is significantly different from zero (say, at the level of 1-\\(\\sigma\\), \\(p\\text{-value}<32\\%\\)). Otherwise:\nFit a model with no intercept, \\(Y = mX + \\varepsilon\\).\nFinally, we use the last fitted model to construct a “naive 95%”\nconfidence interval \\((\\hat m_-, \\hat m_+)\\) for the slope \\(m\\).\nThis is defined by:\n\\[\n\\hat m_\\pm = \\hat m\\pm t_{0.975, \\,N-d} \\cdot \\hat \\sigma _\\hat m\\qquad (95\\%\\,\\text {C.L.}).\n\\tag{2}\n\\]\nHere \\(t_{0.975,\\, N-d}\\) is the 97.5%-quantile of the \\(t\\)-student\ndistribution with \\(N-d\\) degrees of freedom, and \\(d\\) is the number of\nestimated parameters, (\\(2\\) or \\(1\\), according to where we stopped in the\nmodeling procedure). \\(\\hat m\\) and \\(\\hat \\sigma _{\\hat m}\\) are the\nOrdinary Least Squares (OLS) estimates of the slope and its standard\ndeviation, respectively. These are the classical confidence intervals\nreported by the lm() function in R.\nAt a first glance, this procedure might look reasonable. After all, both\nintervals we may end up constructing do have a genuine 95% coverage probability,\nwhen constructed unconditionally… and by selecting the “best” model we’re\nsupposedly choosing the “best” confidence interval. In spite of this qualitative\nargument, we inquire:\n… does it work?\nNow, the question is: how often do the naive CIs (2)\ncover the true parameter \\(m\\) of Eq. (1)? The answer\nbetter be “at least 95% of the times” for our confidence claim in Eq.\n(2) to be valid!\nWe can check the actual coverage of (2) through a simulation.\nHere I’ll assume \\(m = \\sigma = 1\\), and that the\ndataset consists of \\(N=10\\) independent observations of \\(Y\\) at fixed points\n\\(X = (1, \\,2, \\,\\dots ,\\, 10)\\).\n\n\nm <- sigma <- 1  # True parameters\nx <- 1:10  # x covariate, assumed fixed\n\n\nThe following function generates observations of \\(Y\\) according to the distribution (1):\n\n\ngenerate_y <- function(x, m, sigma) {\n  eps <- rnorm(length(x), mean = 0, sd = sigma)\n  return(m * x + eps)\n  }\n\n\nFor example:\n\n\nset.seed(840)\nplot(x, generate_y(x, m, sigma), xlab = \"X\", ylab = \"Y\")\n\n\n\nBelow we generate \\(B=10^4\\) such \\((X,Y)\\) datasets, for each of which we fit a linear model according to the procedure specified above, and check how many\ntimes the true slope \\(m = 1\\) falls in the confidence interval defined by Eq. (2).\n\n\n# Simulation parameters\nB <- 1e4  # Number of replications\n\n# Preallocate logical vectors to be assigned for each replica - for efficiency. \nq_dropped <- logical(B)  # Was the intercept term 'q' dropped? \nm_covered <- logical(B)  # Was the true parameter 'm' covered?\n\n# Set seed for reproducibility\nset.seed(841)\n\n# Logging\ntime_start <- Sys.time()  \n\n# Start the simulation\nfor (b in 1:B) {\n  y <- generate_y(x, m, sigma)\n  \n  # Fit full model (including intercept 'q')\n  fit <- lm(y ~ x + 1)  \n  q_pval <- summary(fit)$coefficients[1, 4]\n  \n  # Is 'q' term \"significant\"? If not, drop 'q' and fit a simpler model\n  if (q_pval > 0.32)  { \n    q_dropped[[b]] <- TRUE\n    fit <- lm(y ~ x - 1) \n  } else {\n    q_dropped[[b]] <- FALSE\n  }\n  \n  # Construct CI for 'm',  using the selected model's fit\n  m_ci <- confint(fit, 'x', level = 0.95)\n  m_covered[[b]] <- m_ci[[1]] < m && m < m_ci[[2]]\n}\n\ntime_end <- Sys.time()\ncat(\"Done :) Took \", as.numeric(time_end - time_start), \" seconds.\")\n\nDone :) Took  12.12071  seconds.\n\nThe variable m_covered[[b]] is TRUE if the slope \\(m\\) fell in the\nnaive CI \\((m_-, m_+)\\) defined by Eq. (2) in the\nb-th replica of the simulation. Hence, the actual coverage fraction of\nthe CI is given by:\n\n\nmean(m_covered)  # Actual coverage of naive \"95%\" CIs.\n\n[1] 0.9172\n\n92%! If this difference from the nominal 95% coverage guarantee does not\nstrike you as enormous, think about it in these terms: the naive CIs\n(2) fail to cover the true parameter about 8% of the\ntimes; This is a relative +60% of failures with respect to an honest 95%\nCI.\nWhat’s going on\nWe can understand a bit better what’s happening here by decomposing the\ncoverage probability as follows:\n\\[\n\\text {Pr}(m \\in \\text{CI})  = \\text {Pr}(m \\in \\text{CI}_{q \\text{ dropped}}\\,\\vert\\,q \\text{ dropped})\\cdot \\text {Pr}(q \\text{ dropped}) +\\\\ +\\text {Pr}(m \\in \\text{CI}_{q  \\text{ kept}}\\,\\vert\\,q \\text{ kept})\\cdot \\text {Pr}(q \\text{ kept})\n\\tag{3}\n\\]\nThe right hand side of this equation shows how our selective modeling\nprocedure alters the probability \\(\\text{Pr}(m\\in \\text{CI})\\). There are\ntwo contributing factors here: the probability of dropping the intercept\nterm, and the covering probabilities of the CIs constructed in the two\ncases (\\(\\text{CI}_{q \\text{ dropped}}\\) and\n\\(\\text{CI}_{q \\text{ kept}}\\)). We can estimate all these\nprobabilities as:\n\n\nmean(q_dropped)  # Pr(q dropped)\n\n[1] 0.6782\n\nmean(m_covered[q_dropped])  # Pr(m covered | q dropped)\n\n[1] 0.9510469\n\nmean(m_covered[!q_dropped])  # Pr(m covered | q kept)\n\n[1] 0.845867\n\nThe first result directly follows from our procedure, which uses a\nhypothesis test with significance \\(\\alpha = 32\\%\\) to test the (true)\nnull hypothesis \\(q = 0\\). It is a bit harder but in fact possible to\nprove that4\n\\(\\text {Pr}(m \\in \\text{CI}_{q \\text{ dropped}}\\,\\vert\\,q \\text{ dropped}) = 95\\%\\),\nas the second estimate would seem to suggest. The third result is\nfinally what invalidates the naive coverage guarantee in Eq.\n(2).\nConcluding Remarks\nTo summarize:\nWe started with two linear models for \\(Y\\) vs. \\(X\\), which were in fact both well-specified (that is, correct).\nWe stipulated to choose one of the two models by testing the null hypothesis \\(q = 0\\).\nAfter selection, we constructed \\(95\\%\\) confidence intervals for the slope\n\\(\\hat m\\) using the selected model, as if this had been fixed in advance.\nA simulation shows that such intervals have a true coverage probability of\n\\(\\approx 92\\%\\).\nThe mathematical explanation of the last result is provided by Eq. (3), while the (hopefully) plain English one in the introductory part of this post. I will conclude with a few parenthetical remarks.\nFirst, the selective procedure proposed here would likely hardly be applied in practice in such a simple situation5. However, one could easily think of a more complex scenario with multiple covariates, where eliminating redundant ones could turn out to be beneficial for interpretation (if not compulsory, if the number of covariates exceeds the sample size).\nSecond, in order to avoid cluttering the discussion with too much\ntechnicalities, I have deliberately chosen a quite special point in true-model space (\\(q = 0\\)). This implies that both fits with and without intercept estimate the same slope \\(m\\); this is a peculiar property of \\(q = 0\\), which would not be true in the general case \\(q \\in \\mathbb R\\). In general, we would have to carefully define the inferential targets for the \\(q=0\\) and \\(q \\in \\mathbb R\\) cases, in a differential manner.\nConclusion\nThat was all for today. In the next post, I will discuss some mathematical details\nregarding the formulation of the Selective Inference problem in model-building.\nFor those surviving down to the bottom of the funnel, my future plan is to\nreview some (valid) selective inference methods I found interesting, including:\nBenjamini-Yekutieli control of False Coverage Rate (Benjamini and Yekutieli 2005),\nPOSI bounds for marginal coverage (Berk et al. 2013),\nData Fission, an elegant generalization of good old data splitting (Leiner et al. 2021).\n…whatever cool stuff I may discover in the meantime.\nCiao!\n\n\n\nBenjamini, Yoav, and Daniel Yekutieli. 2005. “False Discovery Rate–Adjusted Multiple Confidence Intervals for Selected Parameters.” Journal of the American Statistical Association 100 (469): 71–81.\n\n\nBerk, Richard, Lawrence Brown, Andreas Buja, Kai Zhang, and Linda Zhao. 2013. “Valid Post-Selection Inference.” The Annals of Statistics, 802–37.\n\n\nIsidori, Gino, Davide Lancierini, Patrick Owen, and Nicola Serra. 2021. “On the Significance of New Physics in b→ Sℓ+ ℓ- Decays.” Physics Letters B 822: 136644.\n\n\nLeiner, James, Boyan Duan, Larry Wasserman, and Aaditya Ramdas. 2021. “Data Fission: Splitting a Single Data Point.” arXiv. https://doi.org/10.48550/ARXIV.2112.11079.\n\n\nShalizi, C. R. 2020. “Post-Model-Selection Inference.” 2020. http://bactra.org/notebooks/post-model-selection-inference.html.\n\n\nVrbik, Jan. 2020. “Regression Analysis (Lecture Notes).” 2020. http://spartan.ac.brocku.ca/~jvrbik/MATH3P82/notes.pdf.\n\n\nHere, in the “extended family of models”, I’m also implicitly\naccounting for the multiplicity introduced by continuous model\nparameters and training parameters (also known as hyper-parameters).↩︎\nThe preferential method according to (Shalizi 2020), from which\nI borrowed the “a-theoretical” description, and which I recommend as a starting point for literature review.↩︎\nThis is not to say that correctly accounting for Selective Inference is\nthe default in scientific practice. A relevant example from the field I come\nfrom (Particle Physics), is documented in this stimulating reference:\n(Isidori et al. 2021).↩︎\nI’m always amazed by the great deal of theory one can learn by\nrunning a dumb simulation, and trying to explain a posteriori what\nseems to be a too perfect result. Technically, this follows from the fact that the slope estimate\n\\(\\hat m\\) and residual sum of squares \\(\\text{RSS}\\) of the reduced\nmodel, and the \\(F\\)-statistic used to test \\(q = 0\\), are all\nindependent random variables under the same null hypothesis, here\ntrue by construction. All these facts are in turn consequences of\ngeneral theorems from linear model theory, see for example\n(Vrbik 2020, chap. 4)… and, to be sure, it took me more than a single\nnight without sleep to figure all this out.↩︎\nAnd I’m actually not sure that, after properly taking into account Selective Inference, it would lead to a substantial gain in estimation accuracy, compared to simply fitting the possibly redundant model with intercept.↩︎\n",
    "preview": "posts/2022-10-18-posi/posi_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-11-13-kgrams-v012-released/",
    "title": "kgrams v0.1.2 on CRAN",
    "description": "kgrams: Classical k-gram Language Models in R.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2021-11-13",
    "categories": [
      "Natural Language Processing",
      "R"
    ],
    "contents": "\nSummary\nVersion v0.1.2 of my R package kgrams was just accepted by CRAN. This package provides tools for training and evaluating k-gram language models in R, supporting several probability smoothing techniques, perplexity computations, random text generation and more.\nShort demo\n\n\nlibrary(kgrams)\n# Get k-gram frequency counts from Shakespeare's \"Much Ado About Nothing\"\nfreqs <- kgram_freqs(kgrams::much_ado, N = 4)\n\n# Build modified Kneser-Ney 4-gram model, with discount parameters D1, D2, D3.\nmkn <- language_model(freqs, smoother = \"mkn\", D1 = 0.25, D2 = 0.5, D3 = 0.75)\n\n# Sample sentences from the language model at different temperatures\nset.seed(840)\nsample_sentences(model = mkn, n = 3, max_length = 10, t = 1)\n\n[1] \"i have studied eight or nine truly by your office [...] (truncated output)\"\n[2] \"ere you go : <EOS>\"                                                        \n[3] \"don pedro welcome signior : <EOS>\"                                         \n\nsample_sentences(model = mkn, n = 3, max_length = 10, t = 0.1)\n\n[1] \"i will not be sworn but love may transform me [...] (truncated output)\" \n[2] \"i will not fail . <EOS>\"                                                \n[3] \"i will go to benedick and counsel him to fight [...] (truncated output)\"\n\nsample_sentences(model = mkn, n = 3, max_length = 10, t = 10)\n\n[1] \"july cham's incite start ancientry effect torture tore pains endings [...] (truncated output)\"   \n[2] \"lastly gallants happiness publish margaret what by spots commodity wake [...] (truncated output)\"\n[3] \"born all's 'fool' nest praise hurt messina build afar dancing [...] (truncated output)\"          \n\nNEWS\nOverall Software Improvements\nThe package’s test suite has been greatly extended.\nImproved error/warning conditions for wrong arguments.\nRe-enabled compiler diagnostics as per CRAN policy (#19)\nAPI Changes\nverbose arguments now default to FALSE.\nprobability(), perplexity() and sample_sentences() are restricted to\naccept only language_model class objects as their model argument.\nNew features\nas_dictionary(NULL) now returns an empty dictionary.\nBug Fixes\nFixed bug causing .preprocess and .tknz_sent arguments to be ignored in process_sentences().\nFixed previously wrong defaults for max_lines and batch_size arguments in kgram_freqs.connection().\nAdded print method for class dictionary.\nFixed bug causing invalid results in dictionary() with batch processing and\nnon-trivial size constraints on vocabulary size.\nOther\nMaintainer’s email updated\n\n\n\n",
    "preview": {},
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-25-r-client-for-r-universe-apis/",
    "title": "R Client for R-universe APIs",
    "description": "{runi}, an R package to interact with R-universe repository APIs",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2021-07-25",
    "categories": [
      "R"
    ],
    "contents": "\nIntroduction\nFollowing my previous post on how to use your R-universe API to automatically generate a list of the packages on your R-universe, I started working on a simple R client to interact with such APIs.\nFor those who missed it, R-universe is a new project from rOpenSci that allows you to mantain a personal CRAN-like repository, which automatically syncs with the GitHub repositories hosting your projects.\nAmong other features, each repository has associated a RESTful API with which users can interact for managing and retrieving informations about packages in the repo. Quoting R-universe:\n\nThe package server provides REST APIs for managing package submissions and querying information about individual packages as well as on the repository level. These data can be accessed programmatically or displayed in a front-end dashboard.\n\n{runi}\nI started playing around to implement an R client for R-universe APIs. The package is called runi and the code is here. Up to now, only a small subset of the full API features are available. You can peek at the development version from GitHub, using:\nremotes::install_github(\"vgherard/runi\")\nFor instance, the procedure for obtaining your packages DESCRIPTION outlined in\nmy previous post is performed by:\n\n\ndf <- runi::runi_stats_descriptions(\"vgherard\") # 'vgherard' is my R-universe name.\n\n\ndf is a dataframe containing all the entries of the DESCRIPTION files of my packages:\n\n\ndf[, c(\"Package\", \"Title\")]\n\n# A tibble: 7 × 2\n  Package  Title                                            \n  <chr>    <chr>                                            \n1 kgrams   Classical k-gram Language Models                 \n2 scribblr A Notepad Inside RStudio                         \n3 fcci     Feldman-Cousins Confidence Intervals             \n4 runi     Client for R-universe APIs                       \n5 r2r      R-Object to R-Object Hash Maps                   \n6 gsample  Efficient Weighted Sampling Without Replacement  \n7 sbo      Text Prediction via Stupid Back-Off N-Gram Models\n\ndf[1, \"Description\"] |> strtrim(60) |> paste(\"[...]\")\n\n[1] \"Training and evaluating k-gram language models in R,\\nsupporti [...]\"\n\n\n\n\n",
    "preview": {},
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-21-automatically-resume-your-r-package-portfolio-using-the-r-universe-api/",
    "title": "Automatic resumes of your R-developer portfolio from your R-Universe",
    "description": "Create automatic resumes of your R packages using the R-Universe API.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2021-07-21",
    "categories": [
      "R"
    ],
    "contents": "\nHi R-bloggers 👋\nStarting from today, all posts from this blog in the R category will also appear on R-bloggers. I would like to thank Tal for aggregating my blog, and say “hi!” to all R-bloggers readers. I’m a particle physicist with a passion for R, Statistics and Machine Learning. If you want to find out something more about me, you can take a look at my website, and links therein.\nIntroduction\nR-universe is a cool initiative from rOpenSci, which allows you to create your own CRAN-like repository. The latter is synced with the GitHub repositories (main or specific branches, or releases) associated to your R packages, so that using an R-universe is a very effortless way to organize and share your personal package ecosystem.\nIf you want to setup your own R-universe, follow the instructions in this blog post. In this post, I assume that you have created your own R-universe, and show you how to retrieve metadata on your packages using the R-universe API.\nRetrieving packages descriptions from your R-universe API\nOnce you will have it set up, your R-universe will be available at the URL your-user-name.r-universe.dev. For instance, mine is vgherard.r-universe.dev. From your R-universe home page, you can access the documentation of the API. We will use the command:\nGET /stats/descriptions\n    NDJSON stream with data from package DESCRIPTION files.\nThe JSON stream can be read with jsonlite, as follows:\n\n\ncon <- url(\"https://vgherard.r-universe.dev/stats/descriptions\")\npkgs <- jsonlite::stream_in(con)\n\n\n\n Found 6 records...\n Imported 6 records. Simplifying...\n\nThe result is a dataframe with alll the entries of your packages’ DESCRIPTION file, e.g.:\n\n\npkgs[, c(\"Package\", \"Title\", \"Version\")]\n\n\n   Package                                             Title\n1      r2r                    R-Object to R-Object Hash Maps\n2   kgrams                  Classical k-gram Language Models\n3 scribblr                          A Notepad Inside RStudio\n4  gsample   Efficient Weighted Sampling Without Replacement\n5      sbo Text Prediction via Stupid Back-Off N-Gram Models\n6     fcci              Feldman-Cousins Confidence Intervals\n     Version\n1 0.1.1.9000\n2      0.1.0\n3 0.2.0.9000\n4      0.1.0\n5      0.5.0\n6      1.0.0\n\nI use this query on my personal website to automatically generate a resume of the packages available on my R-universe (this is combined with a GitHub Action scheduled workflow which periodically updates the Code section of my website). More precisely, I define an R string txt containing the Markdown code for my resume, and I inline it in R Markdown using the synthax `r `. This is the code I use on my website:\n\n\ntxt <- \"\"\nfor (i in seq_len(nrow(pkgs))) {\n  txt <- paste0(\n    txt, \n    \"### [`\", pkgs[i, \"Package\"], \"`](\", pkgs[i, \"RemoteUrl\"], \")\", \"\\n\",\n    \"[![CRAN status](https://www.r-pkg.org/badges/version/\", pkgs[i,\"Package\"],\n    \")](https://CRAN.R-project.org/package=\",pkgs[i, \"Package\"], \")\",\n    \"\\n\\n\",\n    \"*\", pkgs[i, \"Title\"], \".* \", pkgs[i, \"Description\"],\n    \"\\n\\n\"\n    )\n}\n\n\n\nand this is the output:\nr2r\n\nR-Object to R-Object Hash Maps. Implementation of hash tables (hash sets and hash maps) in R, featuring arbitrary R objects as keys, arbitrary hash and key-comparison functions, and customizable behaviour upon queries of missing keys.\nkgrams\n\nClassical k-gram Language Models. Tools for training and evaluating k-gram language models in R, supporting several probability smoothing techniques, perplexity computations, random text generation and more.\nscribblr\n\nA Notepad Inside RStudio. A project aware notepad inside RStudio, for taking quick project-related notes without distractions. RStudio addin.\ngsample\n\nEfficient Weighted Sampling Without Replacement. Sample without replacement using the Gumbel-Max trick (c.f. ).\nsbo\n\nText Prediction via Stupid Back-Off N-Gram Models. Utilities for training and evaluating text predictors based on Stupid Back-Off N-gram models (Brants et al., 2007, https://www.aclweb.org/anthology/D07-1090/).\nfcci\n\nFeldman-Cousins Confidence Intervals. Provides support for building Feldman-Cousins confidence intervals [G. J. Feldman and R. D. Cousins (1998) doi:10.1103/PhysRevD.57.3873].\n\n\n\n",
    "preview": {},
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-06-r2r/",
    "title": "{r2r} now on CRAN",
    "description": "Introducing {r2r}, an R implementation of hash tables.",
    "author": [
      {
        "name": "vgherard",
        "url": "https://vgherard.github.io"
      }
    ],
    "date": "2021-07-06",
    "categories": [
      "Data Structures",
      "R"
    ],
    "contents": "\nIntroduction\nMy package {r2r} (v0.1.1) has been accepted by CRAN, and is now available for download from the public repository.\nr2r\n\n\n\n\n\nr2r provides a flexible implementation of hash tables in R, allowing for:\narbitrary R objects as keys and values,\narbitrary key comparison and hash functions,\ncustomizable behaviour (throw or return a default value) on missing key exceptions.\nInstallation\nYou can install the released version of r2r from CRAN with:\ninstall.packages(\"r2r\")\nand the development version from my R-universe repository, with:\ninstall.packages(\"r2r\", repos = \"https://vgherard.r-universe.dev\")\nUsage\n\n\nlibrary(r2r)\nm <- hashmap()\n\n# Insert and query a single key-value pair\nm[[ \"user\" ]] <- \"vgherard\"\nm[[ \"user\" ]]\n\n[1] \"vgherard\"\n\n# Insert and query multiple key-value pairs\nm[ c(1, 2, 3) ] <- c(\"one\", \"two\", \"three\")\nm[ c(1, 3) ]\n\n[[1]]\n[1] \"one\"\n\n[[2]]\n[1] \"three\"\n\n# Keys and values can be arbitrary R objects\nm[[ lm(mpg ~ wt, mtcars) ]] <- c(TRUE, FALSE, TRUE)\nm[[ lm(mpg ~ wt, mtcars) ]]\n\n[1]  TRUE FALSE  TRUE\n\nGetting help\nFor further details, including an introductory vignette illustrating the features of r2r hash maps, you can consult the r2r website. If you encounter a bug, want to suggest a feature or need further help, you can open a GitHub issue.\nComparison with hash\nCRAN package {hash} also offers an implementation of hash tables based on R environments. The two tables below offer a comparison between {r2r} and {hash} (for more details, see the benchmarks Vignette)\n\nTable 1: Features supported by {r2r} and {hash}.\nFeature\nr2r\nhash\nBasic data structure\nR environment\nR environment\nArbitrary type keys\nX\n\nArbitrary type values\nX\nX\nArbitrary hash function\nX\n\nArbitrary key comparison function\nX\n\nThrow or return default on missing keys\nX\n\nHash table inversion\n\nX\n\n\nTable 2: Performances of {r2r} and {hash} for basic hash table operations.\nTask\nComparison\nKey insertion\n{r2r} ~ {hash}\nKey query\n{r2r} < {hash}\nKey deletion\n{r2r} << {hash}\n\n\n\n\n",
    "preview": {},
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-06-test-post/",
    "title": "Test post",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "vgherard",
        "url": {}
      }
    ],
    "date": "2021-07-06",
    "categories": [
      "Other"
    ],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2024-01-08T01:16:11+00:00",
    "input_file": {}
  }
]
