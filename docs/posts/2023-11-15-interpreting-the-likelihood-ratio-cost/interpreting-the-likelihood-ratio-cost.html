<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Valerio Gherardi">
<meta name="dcterms.date" content="2023-11-15">
<meta name="description" content="Analysis of infinite sample properties and comparison with cross-entropy loss.">

<title>Interpreting the Likelihood Ratio cost – vgherard</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BM2Y0430QE"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-BM2Y0430QE', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"express",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  ,
"language":"en"
  });
});
</script> 
  

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">vgherard</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notebooks.html"> 
<span class="menu-text">Notebooks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../software.html"> 
<span class="menu-text">Software</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../scientific_publications.html"> 
<span class="menu-text">Scientific Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../files.html"> 
<span class="menu-text">Files</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/vgherard"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/vgherard/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/ValerioGherardi"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:vgherard840@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://raw.githubusercontent.com/vgherard/cv/master/cv/cv.pdf"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-other-links" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Other links</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-other-links">    
        <li>
    <a class="dropdown-item" href="https://www.r-bloggers.com/author/vgherard/">
 <span class="dropdown-text">vgherard @ r-bloggers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://stackexchange.com/users/2261006/pppqqq">
 <span class="dropdown-text">vgherard @ StackExchange</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://inspirehep.net/authors/1728353?ui-citation-summary=true">
 <span class="dropdown-text">vgherard @ iNSPIRE HEP</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://lichess.org/@/vgherard">
 <span class="dropdown-text">vgherard @ lichess.org</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://ratings.fide.com/profile/94715360">
 <span class="dropdown-text">vgherard @ FIDE</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Interpreting the Likelihood Ratio cost</h1>
                  <div>
        <div class="description">
          <p>Analysis of infinite sample properties and comparison with cross-entropy loss.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Forensic Science</div>
                <div class="quarto-category">Bayesian Methods</div>
                <div class="quarto-category">Information Theory</div>
                <div class="quarto-category">Probability Theory</div>
                <div class="quarto-category">R</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://vgherard.github.io">Valerio Gherardi</a> <a href="https://orcid.org/0000-0002-8215-3013" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 15, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#intro" id="toc-intro" class="nav-link active" data-scroll-target="#intro">Intro</a></li>
  <li><a href="#cross-entropy-with-random-weights" id="toc-cross-entropy-with-random-weights" class="nav-link" data-scroll-target="#cross-entropy-with-random-weights">Cross-entropy with random weights</a></li>
  <li><a href="#a-familiar-case-cross-entropy-loss" id="toc-a-familiar-case-cross-entropy-loss" class="nav-link" data-scroll-target="#a-familiar-case-cross-entropy-loss">A familiar case: cross-entropy loss</a></li>
  <li><a href="#the-likelihood-ratio-cost" id="toc-the-likelihood-ratio-cost" class="nav-link" data-scroll-target="#the-likelihood-ratio-cost">The Likelihood Ratio Cost</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#simulated-example" id="toc-simulated-example" class="nav-link" data-scroll-target="#simulated-example">Simulated example</a></li>
  <li><a href="#final-remarks" id="toc-final-remarks" class="nav-link" data-scroll-target="#final-remarks">Final remarks</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/vgherard/vgherard.github.io/blob/main/posts/2023-11-15-interpreting-the-likelihood-ratio-cost/interpreting-the-likelihood-ratio-cost.Rmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/vgherard/vgherard.github.io/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<h5 class="quarto-listing-category-title">Subscribe</h5>
<form method="post" action="https://blogtrottr.com">
        <p> Enjoy my blog? Get notified of new posts via email: </p>
    <input type="text" name="btr_email" placeholder="your@email.here"><br>
    <input type="hidden" name="btr_url" value="https://vgherard.github.io/index.xml">
    <input type="hidden" name="schedule_type" value="0">
    <input type="submit" value="Subscribe" id="mc-embedded-subscribe" class="button">
</form>
or
<a href="https://vgherard.github.io/index.xml"> subscribe to its RSS feed</a>.
<br>
<br>
R-related posts also available at
<a href="https://www.r-bloggers.com">r-bloggers</a> and <a href="https://rweekly.org/">RWeekly</a>.
<br>
<br>


</div></div>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">Intro</h2>
<p>During the last few months, I’ve been working on a machine learning algorithm with applications in <a href="https://en.wikipedia.org/wiki/Forensic_science">Forensic Science</a>, a.k.a. Criminalistics. In this field, one common task for the data analyst is to present the <em>trier-of-fact</em> (the person or people who determine the facts in a legal proceeding) with a numerical assessment of the strength of the evidence provided by available data towards different hypotheses. In more familiar terms, the forensic expert is responsible of computing the likelihoods (or likelihood ratios) of data under competing hypotheses, which are then used by the trier-of-fact to produce Bayesian posterior probabilities for the hypotheses in question<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>In relation to this, forensic scientists have developed a bunch of techniques to evaluate the performance of a likelihood ratio model in discriminating between two alternative hypothesis. In particular, I have come across the so called <em>Likelihood Ratio Cost</em>, usually defined as:</p>
<p><span id="eq-cllr"><span class="math display">\[
C_{\text{LLR}} = \frac{1}{2N_1} \sum _{Y_i=1} \log(1+r(X_i) ^{-1})+\frac{1}{2N_0} \sum _{Y_i=0} \log(1+r(X_i)),
\tag{1}\]</span></span> where we assume we have data consisting of <span class="math inline">\(N_1+N_0\)</span> independent identically distributed observations <span class="math inline">\((X_i,\,Y_i)\)</span>, with binary <span class="math inline">\(Y\)</span>; <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_0\)</span> stand for the number of positive (<span class="math inline">\(Y=1\)</span>) and negative (<span class="math inline">\(Y=0\)</span>) cases; and <span class="math inline">\(r(X)\)</span> is a model for the likelihood ratio <span class="math inline">\(\Lambda(X) \equiv \frac{\text{Pr}(X\vert Y = 1)}{\text{Pr}(X\vert Y = 0)}\)</span>.</p>
<p>The main reason for writing this note was to understand a bit better what it means to optimize <a href="#eq-cllr" class="quarto-xref">Equation&nbsp;1</a>, which does not look immediately obvious to me from its definition<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. In particular: is the population minimizer of <a href="#eq-cllr" class="quarto-xref">Equation&nbsp;1</a> the actual likelihood ratio? And in what sense is a model with lower <span class="math inline">\(C_\text{LLR}\)</span> better than one with a correspondingly higher value?</p>
<p>The short answers to these questions are: yes; and: <span class="math inline">\(C_\text{LLR}\)</span> optimization seeks for the model with the best predictive performance in a Bayesian inference setting with uninformative prior on <span class="math inline">\(Y\)</span>, assuming that this prior actually reflects reality (<em>i.e.</em> <span class="math inline">\(\text{Pr}(Y=1) = \text{Pr}(Y=0) = \frac{1}{2}\)</span>). The mathematical details are given in the rest of the post.</p>
<!-- Strictly speaking, there are several aspects to what I have simply referred to as the "performance of a likelihood ratio model". First of all, there is the -->
<!-- left-over uncertainty on $Y$ after measuring $X$, which is an intrinsic property of the data and is independent of modeling. Second, $X$ may not correspond to raw data, but rather be the result of some data-processing/summary, which will in general reduce the amount of available information on $Y$. Finally, in the general case, the likelihood ratio $r(X)$ will not be an exact model, but only an approximation estimated from data. All this aspects get captured and mixed by @ref(eq:CLLR), luckily in a way that can be actually decomposed (see below). -->
</section>
<section id="cross-entropy-with-random-weights" class="level2">
<h2 class="anchored" data-anchor-id="cross-entropy-with-random-weights">Cross-entropy with random weights</h2>
<p>We start with a mathematical digression, which will turn out useful for further developments. Let <span class="math inline">\(\{(X_i,\,Y_i)\}_{i=1,\,2,\,\dots,N}\)</span> be independent draws from a joint distribution, with binary <span class="math inline">\(Y_i \in \{0,\,1\}\)</span>. Given a function <span class="math inline">\(w=w(\boldsymbol Y)\)</span> that is symmetric in its arguments<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, we define the random functional:</p>
<p><span id="eq-weighted-loss"><span class="math display">\[
\mathcal L_N^w[f] = -\frac{1}{N}\sum_{i=1} ^N \left[w(\boldsymbol Y)Y_i \log(f(X_i))+ w({\boldsymbol Y}^c)( Y_i^c) \log(f(X_i)^c)\right],
\tag{2}\]</span></span></p>
<p>where <span class="math inline">\(f=f(X)\)</span> is any function satisfying <span class="math inline">\(f(X)\in [0,\,1]\)</span> for all <span class="math inline">\(X\)</span>, and we let <span class="math inline">\(q^c = 1-q\)</span> for any number <span class="math inline">\(q \in [0,\,1]\)</span>. Notice that for <span class="math inline">\(w(\boldsymbol{Y}) \equiv 1\)</span>, this is just the usual cross-entropy loss.</p>
<p>We now look for the population minimizer of <a href="#eq-weighted-loss" class="quarto-xref">Equation&nbsp;2</a>, <em>i.e.</em> the function <span class="math inline">\(f_*\)</span> that minimizes the functional <span class="math inline">\(f \mapsto \mathbb E(\mathcal L _N ^w [f])\)</span><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. Writing the expectation as:</p>
<p><span class="math display">\[
\mathbb E(\mathcal L _N ^w [f]) = -\frac{1}{N}\sum _{i=1} ^N \mathbb E\left[ \mathbb E(Y_i\cdot w(\boldsymbol Y)\vert X_i)\cdot \log(f(X_i))+E(Y_i^c\cdot w(\boldsymbol Y ^c)\vert X_i)\cdot \log(f^c(X_i))\right],
\]</span> we can easily see that <span class="math inline">\(\mathbb E(\mathcal L _N ^w [f])\)</span> is a convex functional with a unique minimum given by:</p>
<p><span id="eq-pop-minimizer"><span class="math display">\[
f_*(X_i) = \frac{1}{1+r(X_i)^{-1}},\quad r_*(X_i) = \dfrac{E(Y_i\cdot w(\boldsymbol Y)\vert X_i)}{E(Y_i^c\cdot w(\boldsymbol Y^c)\vert X_i)}.
\tag{3}\]</span></span></p>
<p>The corresponding expected loss is:</p>
<p><span class="math display">\[
\mathbb E(\mathcal L _N ^w [f_*]) = \mathbb E\left[ \mathbb E(Y_i\cdot w(\boldsymbol Y) + Y_i^c\cdot w(\boldsymbol Y^c)\vert X_i)\cdot \mathcal H(f_*(X_i))\right],
\]</span> where <span class="math inline">\(\mathcal H(p) = -p \log (p) -(1-p) \log(1-p)\)</span> is the entropy of a binary random variable <span class="math inline">\(Z\)</span> with probability <span class="math inline">\(p = \text{Pr}(Z=1)\)</span> (the index <span class="math inline">\(i\)</span> in the previous expression can be any index, since data points are assumed to be identically distributed).</p>
<p>Before looking at values of <span class="math inline">\(f\)</span> other than <span class="math inline">\(f_*\)</span>, we observe that the previous expectation can be succintly expressed as:</p>
<p><span class="math display">\[
\mathbb E(\mathcal L _N ^w [f_*]) = k \cdot H^\prime(Y\vert X),
\]</span></p>
<p>where</p>
<p><span id="eq-def-kappa"><span class="math display">\[
k = \mathbb E(Y_i\cdot w(\boldsymbol Y) + Y_i^c\cdot w(\boldsymbol Y^c))
\tag{4}\]</span></span></p>
<p>and <span class="math inline">\(H'(Y\vert X)\)</span> is the conditional entropy of <span class="math inline">\(Y\vert X\)</span> with respect to a <em>different</em> probability measure <span class="math inline">\(\text{Pr}^\prime\)</span>, defined by:</p>
<p><span id="eq-def-pr-prime"><span class="math display">\[
\text{Pr}^\prime(E) = t \cdot \text {Pr}(E \vert Y = 1) + (1-t)\cdot \text {Pr}(E \vert Y = 0),
\tag{5}\]</span></span></p>
<p>where <span class="math inline">\(t=\text{Pr}^\prime(Y=1)\in [0,\,1]\)</span> is fixed by the requirement<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>:</p>
<p><span id="eq-def-pr-prime-2"><span class="math display">\[
\dfrac{\text {Pr}^\prime (Y=1)}{\text{Pr}^\prime (Y=0)}=\dfrac{\text {Pr} (Y=1)}{\text{Pr} (Y=0)}\cdot\dfrac{\mathbb E(w(\boldsymbol Y)\vert \sum _i Y_i &gt;0)}{\mathbb E(w(\boldsymbol Y^c)\vert \sum _i Y_i^c &gt;0)}.
\tag{6}\]</span></span></p>
<p>In terms of <span class="math inline">\(\text{Pr}^\prime\)</span>, the population minimizers <span class="math inline">\(f_*\)</span> and <span class="math inline">\(r_*\)</span> in <a href="#eq-pop-minimizer" class="quarto-xref">Equation&nbsp;3</a> can be simply expressed as:</p>
<p><span id="eq-pop-minimizer-2"><span class="math display">\[
r_*(X)=\dfrac{\text {Pr}^\prime(Y=1\vert X)}{\text {Pr}^\prime(Y=0\vert X)},\qquad f_*(X)=\text {Pr}^\prime(Y=1\vert X).
\tag{7}\]</span></span></p>
<p>If now <span class="math inline">\(f\)</span> is an arbitrary function, we have:</p>
<p><span class="math display">\[
\begin{split}
\mathbb E(\mathcal L _N ^w [f]) - \mathbb E(\mathcal L _N ^w [f_*]) &amp;= \mathbb E\left[ \mathbb E(Y_i\cdot w(\boldsymbol Y) + Y_i^c\cdot w(\boldsymbol Y^c)\vert X_i)\cdot \mathcal D(f_*(X_i)\vert \vert f(X_i))\right]
&amp;= k\cdot D(\text{Pr}^\prime\vert \vert \text{Pr}^\prime _f)
\end{split}
\]</span> where <span class="math inline">\(\mathcal D(p\vert \vert q) = p \log (\frac{p}{q}) + (1-p) \log (\frac{1-p}{1-q})\)</span>, and <span class="math inline">\(D(\text{Pr}^\prime\vert \vert \text{Pr}^\prime _f)\)</span> is the Kullback-Liebler divergence between the measure <span class="math inline">\(\text{Pr}^\prime\)</span> and the measure <span class="math inline">\(\text{Pr}^\prime _f\)</span> defined by:</p>
<p><span class="math display">\[
\text{Pr}^\prime _f(Y = 1\vert X)=f(X),\qquad \text{Pr}^\prime _f(X)=\text{Pr}^\prime(X)
\]</span> (notice that <span class="math inline">\(\text {Pr} ^{\prime} _{f_*} \equiv \text{Pr} ^{\prime}\)</span> by definition). Finally, suppose that <span class="math inline">\(X = g(\widetilde X)\)</span> for some random variable <span class="math inline">\(\widetilde X\)</span>, and define the corresponding functional:</p>
<p><span class="math display">\[
\widetilde{\mathcal L} _N^w[\widetilde f]  = -\frac{1}{N}\sum_{i=1} ^N \left[w(\boldsymbol Y)Y_i \log(\widetilde f(\widetilde X))+ w({\boldsymbol Y}^c)( Y_i^c) \log(\widetilde f(\widetilde X)^c)\right].
\]</span> Then <span class="math inline">\(\mathcal L _N ^w [f] = \widetilde{\mathcal L} _N^w[f \circ g]\)</span>. If <span class="math inline">\(\widetilde f  _* =\)</span> is the population minimizer of <span class="math inline">\(\widetilde{\mathcal L} _N^w\)</span>, it follows that <span class="math inline">\(\mathbb E (\widetilde{\mathcal L} _N^w[\widetilde f _*]) \leq \mathbb E(\mathcal L _N ^w [f_*])\)</span>.</p>
<p>Putting everything together, we can decompose the expected loss for a function <span class="math inline">\(f=f(X)\)</span>, where <span class="math inline">\(X= g(\widetilde X)\)</span>, in the following suggestive way:</p>
<p><span id="eq-decomposition-weighted-loss"><span class="math display">\[
\begin{split}
\mathbb E(\mathcal L _N ^w [f]) &amp;= (L_N ^w)_\text{min}+(L_N ^w)_\text{proc} +(L_N ^w)_\text{missp},\\
(L_N ^w)_\text{min}&amp;\equiv\mathbb E(\widetilde{\mathcal L} _N^w[{\widetilde f} _*])  \\ &amp;=
\mathbb E\left[ \mathbb E(Y_i\cdot w(\boldsymbol Y) + Y_i^c\cdot w(\boldsymbol Y^c)\vert \widetilde X _i)\cdot \mathcal H({\widetilde f} _*(\widetilde X _i))\right]\\
&amp;=k\cdot H^\prime(Y\vert \widetilde X),\\
(L_N ^w)_\text{proc}&amp;\equiv\mathbb E(\mathcal L _N ^w [f_*]-\widetilde{\mathcal L} _N^w[\phi_*])  \\&amp; =
\mathbb E\left[ \mathbb E(Y_i\cdot w(\boldsymbol Y) + Y_i^c\cdot w(\boldsymbol Y^c)\vert X_i)\cdot  \mathcal H(f_*(X_i))
\right]- (L_N ^w)_\text{min}\\
&amp; = k\cdot I^\prime(Y; \widetilde X\vert X),\\
(L_N ^w)_\text{missp} &amp; \equiv \mathbb E(\mathcal L _N ^w [f]) - \mathbb E(\mathcal L _N ^w [f_*]) \\&amp;= \mathbb E\left[ \mathbb E(Y_i\cdot w(\boldsymbol Y) + Y_i^c\cdot w(\boldsymbol Y^c)\vert X_i)\cdot  \mathcal  D(f_*(X_i)\vert \vert f(X_i))\right]\\ &amp;=k\cdot D(\text {Pr}^\prime\vert \vert \text {Pr}^\prime _f),
\end{split}
\tag{8}\]</span></span></p>
<p>where <span class="math inline">\(k\)</span> is defined in <a href="#eq-def-kappa" class="quarto-xref">Equation&nbsp;4</a>. In the equation for <span class="math inline">\((L^w _N)_\text{proc}\)</span> we introduced the conditional mutual information (with respect to the measure <span class="math inline">\(\text{Pr}^\prime\)</span>), that satisfies [<span class="citation" data-cites="Cover2006">Cover and Thomas (<a href="#ref-Cover2006" role="doc-biblioref">2006</a>)</span>]:</p>
<p><span class="math display">\[
I(\widetilde X;Y\vert X) = I(\widetilde X,Y)-I(X,Y) = H(Y\vert X)-H(Y\vert \widetilde X).
\]</span></p>
<p>The three components in <a href="#eq-decomposition-weighted-loss" class="quarto-xref">Equation&nbsp;8</a> can be interpreted as follows: <span class="math inline">\((L_N ^w)_\text{min}\)</span> represents the minimum expected loss achievable, given the data available <span class="math inline">\(\widetilde X\)</span>; <span class="math inline">\((L_N ^w)_\text{proc}\)</span> accounts for the information lost in the processing transformation <span class="math inline">\(X=g(\widetilde X)\)</span>; finally <span class="math inline">\((L_N ^w)_\text{missp}\)</span> is due to misspecification, <em>i.e.</em> the fact that the model <span class="math inline">\(f(X)\)</span> for the true posterior probability <span class="math inline">\(f_*(X)\)</span> is an approximation.</p>
<p>All the information-theoretic quantities (and their corresponding operative interpretations hinted in the previous paragraph) make reference to the measure <span class="math inline">\(\text{Pr}^\prime\)</span> defined by <a href="#eq-def-pr-prime" class="quarto-xref">Equation&nbsp;5</a> and <a href="#eq-def-pr-prime-2" class="quarto-xref">Equation&nbsp;6</a>. This is merely the result of altering the proportion of positive (<span class="math inline">\(Y=1\)</span>) and negative (<span class="math inline">\(Y=0\)</span>) examples in the <span class="math inline">\(X\)</span>-<span class="math inline">\(Y\)</span> joint distribution by a factor dictated by the weight function <span class="math inline">\(w\)</span> - while keeping conditional distributions such as <span class="math inline">\(X\vert Y\)</span> unchanged.</p>
</section>
<section id="a-familiar-case-cross-entropy-loss" class="level2">
<h2 class="anchored" data-anchor-id="a-familiar-case-cross-entropy-loss">A familiar case: cross-entropy loss</h2>
<p>For <span class="math inline">\(w(\boldsymbol {Y}) = 1\)</span>, the functional <span class="math inline">\(\mathcal {L} _{N} ^{w}[f]\)</span> coincides with the usual cross-entropy loss<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>:</p>
<p><span id="eq-cross-entropy-loss"><span class="math display">\[
\text{CE}[f] = -\frac{1}{N}\sum_{i=1} ^N \left[Y_i \log(f(X_i))+ (1-Y_i) \log(1-f(X_i))\right].
\tag{9}\]</span></span></p>
<p>From <a href="#eq-def-pr-prime-2" class="quarto-xref">Equation&nbsp;6</a> we see that the measure <span class="math inline">\(\text{Pr}^{\prime}\)</span> coincides with the original <span class="math inline">\(\text{Pr}\)</span>, so that by <a href="#eq-pop-minimizer" class="quarto-xref">Equation&nbsp;3</a> the population minimizer of <a href="#eq-cross-entropy-loss" class="quarto-xref">Equation&nbsp;9</a> is <span class="math inline">\(f_{*}(X) = \text{Pr}(Y=1\vert X)\)</span> (independently of sample size). Since <span class="math inline">\(k = 1\)</span> (<em>cf.</em> <a href="#eq-def-kappa" class="quarto-xref">Equation&nbsp;4</a>), the decomposition <a href="#eq-decomposition-weighted-loss" class="quarto-xref">Equation&nbsp;8</a> reads:</p>
<p><span id="eq-decomposition-ce"><span class="math display">\[
\begin{split}
\mathbb E(\text{CE} [f]) &amp;= (\text{CE})_\text{min}+(\text{CE})_\text{proc} +(\text{CE})_\text{missp},\\
(\text{CE})_\text{min}&amp;=H(Y\vert \widetilde X),\\
(\text{CE})_\text{proc}&amp;= I(Y; \widetilde X\vert X),\\
(\text{CE})_\text{missp} &amp;=D(\text {Pr}\vert \vert \text {Pr} _{f}),
\end{split}
\tag{10}\]</span></span></p>
<p>where conditional entropy <span class="math inline">\(H\)</span>, mutual information <span class="math inline">\(I\)</span> and relative entropy <span class="math inline">\(D\)</span> now simply refer to the original measure <span class="math inline">\(\text{Pr}\)</span>.</p>
</section>
<section id="the-likelihood-ratio-cost" class="level2">
<h2 class="anchored" data-anchor-id="the-likelihood-ratio-cost">The Likelihood Ratio Cost</h2>
<p>The quantity <span class="math inline">\(C_{\text{LLR}}\)</span> defined in <a href="#eq-cllr" class="quarto-xref">Equation&nbsp;1</a> can be put in the general form <a href="#eq-weighted-loss" class="quarto-xref">Equation&nbsp;2</a>, if we let <span class="math inline">\(f(X) = (1+r(X)^{-1})^{-1}\)</span> and<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>:</p>
<p><span class="math display">\[
w(\boldsymbol Y) = \left(\dfrac{2}{N}\sum _{i = 1}^{N}Y_j \right)^{-1}
\]</span> In what follows, I will consider a slight modification of the usual <span class="math inline">\(C_\text{LLR}\)</span>, defined by the weight function:</p>
<p><span class="math display">\[
w(\boldsymbol Y) = \dfrac{1}{2(N-1)}\sum _{i = 1}^{N}(1-Y_j).
\]</span> This yields <a href="#eq-cllr" class="quarto-xref">Equation&nbsp;1</a> multiplied by <span class="math inline">\(\dfrac{N_1N_0}{N(N-1)}\)</span>, which I will keep denoting as <span class="math inline">\(C_\text{LLR}\)</span>, with a slight abuse of notation.</p>
<p>We can easily compute<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>:</p>
<p><span id="eq-prior-cllr"><span class="math display">\[
\dfrac{\text {Pr}^\prime (Y=1)}{\text{Pr}^\prime (Y=0)}=1,
\tag{11}\]</span></span></p>
<p>so that, by <a href="#eq-pop-minimizer" class="quarto-xref">Equation&nbsp;3</a>, the population minimizer of <span class="math inline">\(C_\text{LLR}\)</span> is:</p>
<p><span class="math display">\[
r_*(X) = \Lambda (X),\quad f_*(X)=\dfrac{1}{1+\Lambda(X)^{-1}},
\]</span></p>
<p>where <span class="math inline">\(\Lambda(X)\)</span> denotes the <em>likelihood-ratio</em> of <span class="math inline">\(X\)</span>, schematically:</p>
<p><span class="math display">\[
\Lambda(X)\equiv \dfrac{\text{Pr}(X\vert Y = 1)}{\text{Pr}(X\vert Y = 0)}.
\]</span></p>
<p>The constant <span class="math inline">\(k\)</span> in <a href="#eq-def-kappa" class="quarto-xref">Equation&nbsp;4</a> is:</p>
<p><span class="math display">\[
k = \text{Pr}(Y = 1)\text{Pr}(Y = 0)=\text{Var}(Y)
\]</span></p>
<p>The general decomposition <a href="#eq-decomposition-weighted-loss" class="quarto-xref">Equation&nbsp;8</a> becomes: <span id="eq-decomposition-ce"><span class="math display">\[
\begin{split}
\mathbb E(C_\text{LLR} [f]) &amp;= (C_\text{LLR})_\text{min}+(C_\text{LLR})_\text{proc} +(C_\text{LLR})_\text{missp},\\
(C_\text{LLR})_\text{min}&amp;=\text{Var}(Y)\cdot H^{\prime}(Y\vert \widetilde X),\\
(C_\text{LLR})_\text{proc}&amp;= \text{Var}(Y)\cdot I^{\prime}(Y; \widetilde X\vert X),\\
(C_\text{LLR})_\text{missp} &amp;=\text{Var}(Y)\cdot D^{\prime}(\text {Pr}\vert \vert \text {Pr} _{f}),
\end{split}
\tag{12}\]</span></span></p>
<p>where <span class="math inline">\(\text{Pr}^\prime\)</span> is now given by <a href="#eq-prior-cllr" class="quarto-xref">Equation&nbsp;11</a>.</p>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p>The table below provides a comparison between cross-entropy and likelihood-ratio cost, summarizing the results from previous sections.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Cross-entropy</th>
<th>Likelihood Ratio Cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(f_*(X)\)</span></td>
<td><span class="math inline">\(\text{Pr}(Y = 1\vert X)\)</span></td>
<td><span class="math inline">\((1+\Lambda(X)^{-1})^{-1}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(r_*(X)\)</span>`</td>
<td>Posterior odds ratio</td>
<td>Likelihood ratio</td>
</tr>
<tr class="odd">
<td>Minimum Loss</td>
<td><span class="math inline">\(H(Y\vert \widetilde X)\)</span></td>
<td><span class="math inline">\(\text{Var}(Y) \cdot H^\prime(Y\vert \widetilde X)\)</span></td>
</tr>
<tr class="even">
<td>Processing Loss</td>
<td><span class="math inline">\(I(Y; \widetilde X\vert X)\)</span></td>
<td><span class="math inline">\(\text{Var}(Y) \cdot I^\prime(Y; \widetilde X\vert X)\)</span></td>
</tr>
<tr class="odd">
<td>Misspecification Loss</td>
<td><span class="math inline">\(D(f_*\vert\vert f)\)</span></td>
<td><span class="math inline">\(\text{Var}(Y) \cdot D^\prime(f_*\vert\vert f)\)</span></td>
</tr>
<tr class="even">
<td>Reference measure</td>
<td><span class="math inline">\(\text{Pr}\)</span></td>
<td><span class="math inline">\(\text{Pr}^{\prime} = \frac{\text{Pr}(\cdot \vert Y = 1)+\text{Pr}(\cdot \vert Y = 0)}{2}\)</span></td>
</tr>
</tbody>
</table>
<p>The objective of <span class="math inline">\(C_\text{LLR}\)</span> is found to be the likelihood ratio, as terminology suggests. The interpretation of model selection according to <span class="math inline">\(C_\text{LLR}\)</span> minimization turns out to be slightly more involved, compared to cross-entropy, which we first review.</p>
<p>Suppose we are given a set of predictive models <span class="math inline">\(\{\mathcal M_i\}_{i\in I}\)</span>, each of which consists of a processing transformation, <span class="math inline">\(\widetilde X \mapsto X\)</span>, and an estimate of the posterior probability <span class="math inline">\(\text{Pr}(Y = 1\vert X)\)</span>. When the sample size <span class="math inline">\(N \to \infty\)</span>, cross-entropy minimization will almost certainly select the model that minimizes <span class="math inline">\(I(Y; \widetilde X\vert X) + D(f_*\vert \vert f)\)</span>. Following standard Information Theory arguments, we can interpret this model as the statistically optimal compression algorithm for <span class="math inline">\(Y\)</span>, assuming <span class="math inline">\(X\)</span> to be available at both the encoding and decoding ends.</p>
<p>The previous argument carries over <em>mutatis mutandi</em> to <span class="math inline">\(C_\text{LLR}\)</span> minimization, with an important qualification: optimal average compression is now achieved for data distributed according to a different probability measure <span class="math inline">\(\text{Pr}'(\cdot) = \frac{1}{2}\text {Pr}(\cdot\vert Y = 1) + \frac{1}{2}\text {Pr}(\cdot\vert Y = 0)\)</span>. In particular, according to <span class="math inline">\(\text{Pr}'\)</span>, the likelihood ratio coincides with the posterior odds ratio, and <span class="math inline">\((1+\Lambda(X)^{-1})^{-1}\)</span> coincides with posterior probability, which clarifies why we can measure differences from the true likelihood-ratio through the Kullback-Liebler divergence.</p>
<p>The measure <span class="math inline">\(\text{Pr}'\)</span> is not just an abstruse mathematical construct: it is the result of balanced sampling from the original distribution, <em>i.e.</em> taking an equal number of positive and negative cases<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>. If the <span class="math inline">\((X,\,Y)\)</span> distribution is already balanced, either by design or because of some underlying symmetry in the data generating process, our analysis implies that likelihood-ratio cost and cross-entropy minimization are essentially equivalent for <span class="math inline">\(N\to \infty\)</span>. In general, with <span class="math inline">\(\text{Pr} (Y=1) \neq \text{Pr} (Y=0)\)</span>, this is not the case<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>.</p>
<p>The fact that <span class="math inline">\(C_\text{LLR}\)</span> seeks for optimal predictors according to the balanced measure <span class="math inline">\(\text{Pr}'\)</span> is, one could argue, not completely crazy from the point of view of forensic science, where “<span class="math inline">\(Y\in\{0,1\}\)</span>” often stands for a sort verdict (guilty <em>vs.</em> not guilty, say). Indeed, optimizing with respect to <span class="math inline">\(\text{Pr}^\prime\)</span> means that our predictions are designed to be optimal in a world in which the verdict could be <em>a priori</em> <span class="math inline">\(Y=0\)</span> or <span class="math inline">\(Y=1\)</span> with equal probability - which is what an unbiased trier-of-fact should ideally assume. Minimizing <span class="math inline">\(C_\text{LLR}\)</span>, we guard ourselves against any bias that may be implicit in the training dataset, extraneous to the <span class="math inline">\(X\)</span>-<span class="math inline">\(Y\)</span> relation and not explicitly modeled, a feature that may be regarded as desirable from a legal standpoint.</p>
</section>
<section id="simulated-example" class="level2">
<h2 class="anchored" data-anchor-id="simulated-example">Simulated example</h2>
<p>In general, the posterior odd ratio and likelihood ratio differ only by a constant, so it is reasonable to try to fit the same functional form to both of them. Let us illustrate with a simulated example of this type the differences between cross-entropy and <span class="math inline">\(C_{\text{LLR}}\)</span> optimization mentioned in the previous Section.</p>
<p>Suppose that <span class="math inline">\(X \in \mathbb R\)</span> has conditional density: <span class="math display">\[
\phi(X\vert Y) = (2\pi\sigma _Y^2)^{-\frac{1}{2}} \exp(-\frac{(X-\mu_Y)^2}{2\sigma _Y^2})
\]</span> and <span class="math inline">\(Y\)</span> has marginal probability <span class="math inline">\(\text{Pr}(Y = 1) = \pi\)</span>. The true likelihood-ratio and posterior odds ratio are respectively given by:</p>
<p><span class="math display">\[
\begin{split}
\Lambda (X) &amp;
    \equiv \frac{\phi(X\vert Y=1)}{\phi(X\vert Y=0)}
    = e ^ {a X^2 + bX +c},\\
\rho (X) &amp;
    \equiv \frac{\text{Pr}(Y = 1\vert X)}{\text{Pr}(Y = 0\vert X)}
    = e ^ {a X ^ 2 + bX +c+d},
\end{split}
\]</span> where we have defined:</p>
<p><span class="math display">\[
a  \equiv \dfrac{\sigma _1 ^2 -\sigma_0 ^2}{2\sigma _0 ^2\sigma_1 ^2},\quad
b  \equiv \mu _1 - \mu _0, \quad
c  \equiv \dfrac{\mu_0^2}{2\sigma_0^2} -\dfrac{\mu_1 ^2}{2\sigma _1^2}+\ln(\frac{\sigma _0 }{\sigma _1 }),\quad
d  \equiv \ln (\frac {\pi}{1-\pi}) .
\]</span></p>
<p>Suppose that we fit an exponential function <span class="math inline">\(r(X)=e^{mX +q}\)</span> to <span class="math inline">\(\Lambda(X)\)</span> by likelihood-ratio cost minimization, and similarly <span class="math inline">\(r'(X)=e^{m'X+q'}\)</span> to <span class="math inline">\(\rho(X)\)</span> by cross-entropy minimization<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>. Due to the previous discussion, one could reasonably expect the results of the two procedure to differ in some way, which is demonstrated below by simulation.</p>
<p>The chunk of R code below defines the function and data used for the simulation. In particular, I’m considering a heavily unbalanced case (<span class="math inline">\(\text{Pr}(Y = 1) = 0.1\%\)</span>) in which negative cases give rise to a sharply localized <span class="math inline">\(X\)</span> peak around <span class="math inline">\(X=0\)</span> (<span class="math inline">\(\mu _0 = 0\)</span>, <span class="math inline">\(\sigma_0 = .25\)</span>), while the few positive cases give rise to a broader signal centered at <span class="math inline">\(X=1\)</span> (<span class="math inline">\(\mu _1 = 1\)</span>, <span class="math inline">\(\sigma _1 = 1\)</span>).</p>
<div class="cell" data-code_folding="true">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Tidyverse facilities for plotting</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2) </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss functions</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>weighted_loss <span class="ot">&lt;-</span> <span class="cf">function</span>(par, data, w) {</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    m <span class="ot">&lt;-</span> par[[<span class="dv">1</span>]]</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    q <span class="ot">&lt;-</span> par[[<span class="dv">2</span>]]</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> data<span class="sc">$</span>x</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> data<span class="sc">$</span>y</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    z <span class="ot">&lt;-</span> m <span class="sc">*</span> x <span class="sc">+</span> q</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    p <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>z))</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="sc">-</span><span class="fu">mean</span>(y <span class="sc">*</span> <span class="fu">w</span>(y) <span class="sc">*</span> <span class="fu">log</span>(p) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">-</span>y) <span class="sc">*</span> <span class="fu">w</span>(<span class="dv">1</span><span class="sc">-</span>y) <span class="sc">*</span> <span class="fu">log</span>(<span class="dv">1</span><span class="sc">-</span>p))</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>cross_entropy <span class="ot">&lt;-</span> <span class="cf">function</span>(par, data) </span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">weighted_loss</span>(par, data, <span class="at">w =</span> \(y) <span class="dv">1</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>cllr <span class="ot">&lt;-</span> <span class="cf">function</span>(par, data) </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">weighted_loss</span>(par, data, <span class="at">w =</span> \(y) <span class="fu">mean</span>(<span class="dv">1</span><span class="sc">-</span>y))</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Data generating process</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>rxy <span class="ot">&lt;-</span> <span class="cf">function</span>(n, <span class="at">pi =</span> .<span class="dv">001</span>, <span class="at">mu1 =</span> <span class="dv">1</span>, <span class="at">mu0 =</span> <span class="dv">0</span>, <span class="at">sd1 =</span> <span class="dv">1</span>, <span class="at">sd0 =</span> <span class="fl">0.25</span>) { </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> <span class="fu">runif</span>(n) <span class="sc">&lt;</span> pi</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> y <span class="sc">*</span> mu1 <span class="sc">+</span> (<span class="dv">1</span><span class="sc">-</span>y) <span class="sc">*</span> mu0, <span class="at">sd =</span> y <span class="sc">*</span> sd1 <span class="sc">+</span> (<span class="dv">1</span><span class="sc">-</span>y) <span class="sc">*</span> sd0)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>pi <span class="ot">&lt;-</span> <span class="fu">formals</span>(rxy)<span class="sc">$</span>pi</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulation</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">840</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">rxy</span>(<span class="at">n =</span> <span class="fl">1e6</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>par_cllr <span class="ot">&lt;-</span> <span class="fu">optim</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>), cllr, <span class="at">data =</span> data)<span class="sc">$</span>par</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>par_cross_entropy <span class="ot">&lt;-</span> <span class="fu">optim</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>), cross_entropy, <span class="at">data =</span> data)<span class="sc">$</span>par</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>par_cross_entropy[<span class="dv">2</span>] <span class="ot">&lt;-</span> par_cross_entropy[<span class="dv">2</span>] <span class="sc">-</span> <span class="fu">log</span>(pi <span class="sc">/</span> (<span class="dv">1</span><span class="sc">-</span>pi))</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Helpers to extract LLRs from models</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>llr <span class="ot">&lt;-</span> <span class="cf">function</span>(x, par)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    par[<span class="dv">1</span>] <span class="sc">*</span> x <span class="sc">+</span> par[<span class="dv">2</span>] </span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>llr_true <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    mu1 <span class="ot">&lt;-</span> <span class="fu">formals</span>(rxy)<span class="sc">$</span>mu1 </span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    mu0 <span class="ot">&lt;-</span> <span class="fu">formals</span>(rxy)<span class="sc">$</span>mu0 </span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    sd1 <span class="ot">&lt;-</span> <span class="fu">formals</span>(rxy)<span class="sc">$</span>sd1</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    sd0 <span class="ot">&lt;-</span> <span class="fu">formals</span>(rxy)<span class="sc">$</span>sd0</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    a <span class="ot">&lt;-</span> <span class="fl">0.5</span> <span class="sc">*</span> (sd1 <span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> sd0 <span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> (sd1 <span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> sd0 <span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    b <span class="ot">&lt;-</span> mu1 <span class="sc">/</span> (sd1<span class="sc">^</span><span class="dv">2</span>) <span class="sc">-</span> mu0 <span class="sc">/</span> (sd0<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    c <span class="ot">&lt;-</span> <span class="fl">0.5</span> <span class="sc">*</span> (mu0<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> (sd0<span class="sc">^</span><span class="dv">2</span>) <span class="sc">-</span> mu1<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> (sd1<span class="sc">^</span><span class="dv">2</span>)) <span class="sc">+</span> <span class="fu">log</span>(sd0 <span class="sc">/</span> sd1)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    a <span class="sc">*</span> x <span class="sc">*</span> x <span class="sc">+</span> b <span class="sc">*</span> x <span class="sc">+</span> c</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>So, what do our best estimates look like? The plot below shows the best fit lines for the log-likelihood ratio from <span class="math inline">\(C_{\text{LLR}}\)</span> minimization (in solid red) and cross-entropy minimization (in solid blue). The true log-likelihood ratio parabola is the black line. Also shown are the <span class="math inline">\(\text{LLR}=0\)</span> line (in dashed red) and the <span class="math inline">\(\text{LLR}=\ln(\frac{1-\pi}{\pi})\)</span> (in dashed blue), which are the appropriate Bayes thresholds for classifying a data point as positive (<span class="math inline">\(Y=1\)</span>), assuming data comes from a balanced and unbalanced distribution, respectively.</p>
<div class="cell" data-code_folding="true">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_function</span>(<span class="at">fun =</span> \(x) <span class="fu">llr</span>(x, par_cllr), <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span> </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_function</span>(<span class="at">fun =</span> \(x) <span class="fu">llr</span>(x, par_cross_entropy), <span class="at">color =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_function</span>(<span class="at">fun =</span> \(x) <span class="fu">llr_true</span>(x), <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_hline</span>(<span class="fu">aes</span>(<span class="at">yintercept =</span> <span class="dv">0</span>), <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="fu">geom_hline</span>(<span class="fu">aes</span>(<span class="at">yintercept =</span> <span class="sc">-</span><span class="fu">log</span>(pi <span class="sc">/</span> (<span class="dv">1</span><span class="sc">-</span>pi))), </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>                             <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        <span class="fu">ylim</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">10</span>,<span class="dv">10</span>)) <span class="sc">+</span> <span class="fu">xlim</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">2</span>)) <span class="sc">+</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">xlab</span>(<span class="st">"X"</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">"Log-Likelihood Ratio"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="interpreting-the-likelihood-ratio-cost_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The reason why the lines differ is that they are designed to solve a different predictive problem: as we’ve argued above, minimizing <span class="math inline">\(C_\text{LLR}\)</span> looks for the best <span class="math inline">\(Y\vert X\)</span> conditional probability estimate according to the balanced measure <span class="math inline">\(\text{Pr}'\)</span>, whereas cross-entropy minimization does the same for the original measure <span class="math inline">\(\text{Pr}\)</span>. This is how data looks like under the two measures (the histograms are stacked - in the unbalanced case, positive examples are invisible on the linear scale of the plot):</p>
<div class="cell" data-code_folding="true">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>test_data <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">rxy</span>(<span class="at">n =</span> <span class="fl">1e6</span>, <span class="at">pi =</span> <span class="fl">0.5</span>) <span class="sc">|&gt;</span> <span class="fu">mutate</span>(<span class="at">type =</span> <span class="st">"Balanced"</span>, <span class="at">llr_thresh =</span> <span class="dv">0</span>),</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">rxy</span>(<span class="at">n =</span> <span class="fl">1e6</span>) <span class="sc">|&gt;</span> <span class="fu">mutate</span>(<span class="at">type =</span> <span class="st">"Unbalanced"</span>, <span class="at">llr_thresh =</span> <span class="sc">-</span><span class="fu">log</span>(pi <span class="sc">/</span> (<span class="dv">1</span><span class="sc">-</span>pi)))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>test_data <span class="sc">|&gt;</span> </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">fill =</span> y)) <span class="sc">+</span> </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">100</span>) <span class="sc">+</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_grid</span>(type <span class="sc">~</span> ., <span class="at">scales =</span> <span class="st">"free_y"</span>) <span class="sc">+</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">xlim</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">4</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="interpreting-the-likelihood-ratio-cost_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>These differences are reflected in the misclassification rates of the resulting classifiers defined by <span class="math inline">\(\hat Y(X)=I(\text{LLR}(X)&gt;\text{threshold})\)</span>, where the appropriate threshold is zero in the balanced case, and <span class="math inline">\(\ln(\frac{1-\pi}{\pi})\)</span> in the unbalanced case. According to intuition, we see that the <span class="math inline">\(C_\text{LLR}\)</span> optimizer beats the cross-entropy optimizer on the balanced sample, while performing significantly worse on the unbalanced one.</p>
<div class="cell" data-code_folding="true">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>test_data <span class="sc">|&gt;</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="at">llr_cllr =</span> <span class="fu">llr</span>(x, par_cllr),</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">llr_cross_entropy =</span> <span class="fu">llr</span>(x, par_cross_entropy),</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="at">llr_true =</span> <span class="fu">llr_true</span>(x)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        ) <span class="sc">|&gt;</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">group_by</span>(type) <span class="sc">|&gt;</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarise</span>(</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="at">cllr =</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>((llr_cllr <span class="sc">&gt;</span> llr_thresh) <span class="sc">==</span> y),</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="at">cross_entropy =</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>((llr_cross_entropy <span class="sc">&gt;</span> llr_thresh) <span class="sc">==</span> y),</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="at">true_llr =</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>((llr_true <span class="sc">&gt;</span> llr_thresh) <span class="sc">==</span> y)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 4
  type           cllr cross_entropy true_llr
  &lt;chr&gt;         &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;
1 Balanced   0.166         0.185    0.140   
2 Unbalanced 0.000994      0.000637 0.000518</code></pre>
</div>
</div>
</section>
<section id="final-remarks" class="level2">
<h2 class="anchored" data-anchor-id="final-remarks">Final remarks</h2>
<p>Our main conclusion in a nutshell is that <span class="math inline">\(C_\text{LLR}\)</span> minimization is equivalent, <em>in the infinite sample limit</em>, to cross-entropy minimization on a balanced version of the original distribution. We haven’t discussed what happens for finite samples where variance starts to play a role, affecting the <em>efficiency</em> of loss functions as model optimization and selection criteria. For instance, for a well specified model of likelihood ratio, how do the convergence properties of <span class="math inline">\(C_{\text{LLR}}\)</span> and cross-entropy estimators compare to each other? I expect that answering questions like this would require a much more in-depth study than the one performed here (likely, with simulation playing a central role).</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-BRUMMER2006230" class="csl-entry" role="listitem">
Brümmer, Niko, and Johan du Preez. 2006. <span>“Application-Independent Evaluation of Speaker Detection.”</span> <em>Computer Speech &amp; Language</em> 20 (2): 230–75. https://doi.org/<a href="https://doi.org/10.1016/j.csl.2005.08.001">https://doi.org/10.1016/j.csl.2005.08.001</a>.
</div>
<div id="ref-Cover2006" class="csl-entry" role="listitem">
Cover, Thomas M., and Joy A. Thomas. 2006. <em>Elements of Information Theory 2nd Edition (Wiley Series in Telecommunications and Signal Processing)</em>. Hardcover; Wiley-Interscience.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>This is how I understood things should <em>theoretically</em> work, from discussions with friends who are actually working on this field. I have no idea on how much day-to-day practice comes close to this mathematical ideal, and whether there exist alternative frameworks to the one I have just described.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The Likelihood Ratio Cost was introduced in <span class="citation" data-cites="BRUMMER2006230">(<a href="#ref-BRUMMER2006230" role="doc-biblioref">Brümmer and du Preez 2006</a>)</span>. The reference looks very complete, but I find its notation and terminology so unfamiliar that I decided to do my own investigation and leave this reading for a second moment.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>That is to say, <span class="math inline">\(w(Y_{\sigma(1)},\,Y_{\sigma(2)},\dots,\,Y_{\sigma(N)})=w(Y_1,\,Y_2,\dots,\,Y_N)\)</span> for any permutation <span class="math inline">\(\sigma\)</span> of the set <span class="math inline">\(\{1,\,2,\,\dots,\,N\}\)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><em>Nota bene:</em> the function <span class="math inline">\(f\)</span> is here assumed to be fixed, whereas the randomness in the quantity <span class="math inline">\(L _N ^w [f]\)</span> only comes from the paired observations <span class="math inline">\(\{(X_i,\,Y_i)\}_{i=1,\,2,\,\dots,N}\)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Notice that, due to symmetry <span class="math inline">\(\mathbb E(w(\boldsymbol Y)\vert \sum _i Y_i &gt;0) = \mathbb E(w(\boldsymbol Y)\vert Y_1 = 1)\)</span>, which might be easier to compute.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Here and below I relax a bit the notation, as most details should be clear from context.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>The quantity <span class="math inline">\(w(\boldsymbol Y)\)</span> is not defined when all <span class="math inline">\(Y_i\)</span>’s are zero, as the right-hand side of <a href="#eq-cllr" class="quarto-xref">Equation&nbsp;1</a> itself. In this case, we make the convention <span class="math inline">\(w(\boldsymbol Y) = 0\)</span>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>For the original loss in <a href="#eq-cllr" class="quarto-xref">Equation&nbsp;1</a>, without the modification discussed above, the result would have been <span class="math inline">\(\dfrac{\text {Pr}^\prime (Y=1)}{\text{Pr}^\prime (Y=0)}=\dfrac{1-\text {Pr}(Y=0)^N}{1-\text {Pr}(Y=1)^N}.\)</span><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Formally, given an i.i.d. stochastic process <span class="math inline">\(Z_i = (X_i,\,Y_i)\)</span>, we can define a new stochastic process <span class="math inline">\(Z_i ^\prime = (X_i^\prime,\,Y_i^\prime)\)</span> such that <span class="math inline">\(Z_i ^\prime = Z_{2i - 1}\)</span> if <span class="math inline">\(Y_{2i-1}\neq Y_{2i}\)</span>, and <span class="math inline">\(Z_i ^\prime = \perp\)</span> (not defined) otherwise. Discarding <span class="math inline">\(\perp\)</span> values, we obtain an i.i.d. stochastic process whose individual observations are distributed according to <span class="math inline">\(\text{Pr}^\prime\)</span>.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>There is another case in which <span class="math inline">\(C_{\text{LLR}}\)</span> and cross-entropy minimization converge to the same answer as <span class="math inline">\(N\to \infty\)</span>: when used for model selection among a class of models for the likelihood or posterior odds ratio that contains their correct functional form.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>This is just logistic regression. It could be a reasonable approximation if <span class="math inline">\(\sigma_0 ^2\approx \sigma_1 ^2\)</span>, which however I will assume below to be badly violated.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{gherardi2023,
  author = {Gherardi, Valerio},
  title = {Interpreting the {Likelihood} {Ratio} Cost},
  date = {2023-11-15},
  url = {https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost/interpreting-the-likelihood-ratio-cost.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-gherardi2023" class="csl-entry quarto-appendix-citeas" role="listitem">
Gherardi, Valerio. 2023. <span>“Interpreting the Likelihood Ratio
Cost.”</span> November 15, 2023. <a href="https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost/interpreting-the-likelihood-ratio-cost.html">https://vgherard.github.io/posts/2023-11-15-interpreting-the-likelihood-ratio-cost/interpreting-the-likelihood-ratio-cost.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/vgherard\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="vgherard/vgherard.github.io" data-repo-id="MDEwOlJlcG9zaXRvcnkzODI2MzU4NTQ=" data-category="Comments" data-category-id="DIC_kwDOFs6PTs4CbwOw" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">

<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/vgherard/vgherard.github.io/blob/main/posts/2023-11-15-interpreting-the-likelihood-ratio-cost/interpreting-the-likelihood-ratio-cost.Rmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/vgherard/vgherard.github.io/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div><div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>