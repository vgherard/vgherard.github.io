{
  "hash": "e0a6c50c8c479eaf33e314d24bec410f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Influence Functions\"\ndescription: |\n  A short description of the post.\nauthor:\n  - name: Valerio Gherardi\n    url: https://vgherard.github.io\ndate: 2024-04-29\noutput:\n  distill::distill_article:\n    self_contained: false\ndraft: true\n---\n\n\n\n\n\nLet $(\\Omega,\\,\\mathcal E)$ be a measurable space, and let $t\\colon \\mathbb P (\\mathcal E)\\to\\mathbb R$ be a functional defined on the space $\\mathbb P (\\mathcal E)$ of probability measures on $\\mathcal E$.\n\nGiven $P\\in \\mathbb P (\\mathcal E)$, we assume that $t$ is differentiable at $P$, that is to say, the limit:\n\n$$\nDt(P;Q)\\equiv\\lim _{\\varepsilon \\to 0} \\dfrac{t(P + \\varepsilon \\cdot (Q-P))-t(P)}{\\varepsilon}\n$$ exists for all $Q\\in \\mathbb P (\\mathcal E)$, and that the differential is linear and continuous as a function of $Q$. Moreover, denoting by $\\delta _\\omega$ the measure $\\delta _\\omega(A) = I(\\omega\\in A)$, we define the *influence function*:\n\n$$\n\\Phi _t(P;\\omega)=Dt(P;\\delta_\\omega)\n$$\n\nUnder some regularity assumptions, it can be proved that:\n\n$$\n\\intop _\\Omega \\Phi _t(P;\\omega) \\text d P(\\omega) = 0\n$$\n\nTo see this in the simplest case, assume that $P$ is a discrete measure:\n\n$$\nP = \\sum _{i = 1} p_i \\delta _{\\omega _i},\n$$\n\nfor some $\\omega _i \\in \\Omega$, and $p_i>0$ such that $\\sum _{i=1}^n p_i = 1$. Using the fact that $Dt$ is linear[^1] $Dt(P;P)=0$:\n\n[^1]: In the sense that if $Q = (1-s)Q_0+sQ_1$ for some $s \\in [0,\\,1]$, we have $Dt(P;Q)=(1-s)Dt(P;Q_0)+sDt(P;Q_1)$.\n\n$$\n0 = Dt(P;P) = \\sum _{i = 1}^n p_i\\cdot Dt(P;\\delta_{\\omega_i})= \\intop _\\Omega \\Phi _t(P;\\omega) \\text d P(\\omega).\n$$\n\n------------------------------------------------------------------------\n\nLet us compute the influence function for a maximum-likelihood estimator $\\hat \\theta$ of a parameter $\\theta \\in \\mathbb R ^p$. First of all, let us notice that $\\hat \\theta$ is indeed a functional statistic (that is, it depends only on the empirical distribution $\\hat P$), since we can write:\n\n$$\n\\hat \\theta = \\arg \\min _\\theta \\mathcal L (\\theta; X) = \\arg \\min _\\theta \\mathbb E _{\\hat P}(\\ell (\\theta ;X)),\n$$ where we assume $\\mathcal L(\\theta;x_i) = \\sum _i \\ell (\\theta;x_i)$.\n\nNow, for a general functional $T_\\theta$ of $P\\in \\mathbb P (\\mathcal E)$ depending on a parameter $\\theta \\in \\mathbb R ^p$, suppose that $\\theta ^* = \\arg \\min _\\theta T_\\theta(P)$ and $\\theta ^* +  \\delta \\theta ^*= \\arg \\min _\\theta T_\\theta(P + \\delta P)$. By definition we must have:\n\n$$\n0 = \\dfrac{\\partial }{\\partial \\theta} \\vert _{\\theta = \\theta ^* + \\delta \\theta ^*}T_\\theta(P+\\delta P)= \\dfrac{\\partial }{\\partial \\theta} \\vert _{\\theta = \\theta ^* }DT_\\theta(P;\\delta P)+\\delta \\theta ^* \\cdot\\dfrac{\\partial ^2 }{\\partial \\theta ^2} \\vert _{\\theta = \\theta ^* }T_\\theta(P)\n$$ This implies that the differential of $\\theta ^*$ is given by:\n\n$$\nD\\theta ^*(P;\\delta P) = (\\dfrac{\\partial ^2 T_\\theta(P)}{\\partial \\theta ^2})^{-1} \\dfrac{\\partial (DT_\\theta(P;\\delta P))}{\\partial \\theta} \\vert _{\\theta = \\theta ^* }.\n$$ The functional:\n\n$$\nT_\\theta(P)=\\mathbb E _{P}(\\ell (\\theta ;X))\n$$\n\nis linear in $P$, so that its differential is simply given by:\n\n$$\nDT_\\theta (P;\\delta P) = \\intop _\\Omega \\ell (\\theta; x) \\delta P(x),\n$$\n\nif $\\delta P = \\delta _{x} - P$ we get:\n\n$$\nDT_\\theta (P;\\delta _{x} ) = \\ell (\\theta;x) - \\mathbb E_P(\\ell(\\theta;X))\n$$\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}