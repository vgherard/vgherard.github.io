{
  "hash": "8f0c4dd3d603be91ae7f513a37836116",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Maximum Likelihood\"\ndate: 2024-03-14\nbibliography: biblio.bib\ndraft: false\n---\n\n\n\n**Disclaimer.** These are wild notes on Maximum Likelihood that require some deep *labor limae* session. Use at your own risk!\n\nLet $\\mathcal Q \\equiv\\{\\text d Q_{\\theta} = q_\\theta \\,\\text d \\mu\\}_{\\theta \\in \\Theta}$ be a parametric family of probability measures dominated by some common measure $\\mu$. Consider the functional[^1]:\n\n[^1]: The definition does not depend on the representations $q_\\theta = \\frac{\\text d Q_\\theta}{\\text d \\mu}$ chosen for the $\\mu$-density of $Q_\\theta$ if $P$ is also absolutely continuous with respect to $\\mu$, which we tacitly assume. Typically $\\mu$ would be some relative of Lebesgue or counting measures, in continuous and discrete settings respectively.\n\n$$\n\\theta ^* (P) = \\arg \\min_{\\theta \\in \\Theta} \\intop \\text dP\\,\\ln \\left(\\frac{1}{q_\\theta}\\right).\n$$ {#eq-functional-theta-star}. \n\nThis is the parameter of the best (in the cross-entropy sense) approximation of $P$ within $\\mathcal Q$, which we assume to be unique.\n\nIf $P$ represents the true probability distribution of the data under study, $\\theta ^*(P)$ is the target of ML estimation, in the general case in which $P$ is not necessarily in $\\mathcal Q$. The ML estimate $\\hat \\theta _N$ of $\\theta^*$ from an i.i.d. sample of $N$ observations is[^2]:\n\n[^2]: As a random variable, $\\hat \\theta _N$ is also independent (modulo a measure zero set) of the specific $L_1$ representation $q_\\theta$ if $P$ is absolutely continuous with respect to $\\mu$.\n\n$$\n\\hat \\theta _N \\equiv \\theta ^*(\\hat P _N)=\\arg \\max_{\\theta \\in \\Theta} \\sum_{i=1}^N \\ln ({q_\\theta(Y_i)}),\n$$ {#eq-theta-mle}\n\nwhere $\\hat P _N$ is the empirical distribution of the sample.\n\nDenoting:\n\n$$\nc_{P}(\\theta) = \\intop \\text dP\\,\\ln \\left(\\frac{1}{q_\\theta}\\right),\n$$ {#eq-cross-ent-integral}\n\nwe see that $\\theta^*$ is determined by the condition $c_{P}'(\\theta^*)=0$. From this, we can easily derive the first order variation of $\\theta ^*$ under a variation $P \\to P + \\delta P$:\n\n$$\n\\delta \\theta ^* =\\left(\\intop \\text dP\\,I_{\\theta ^*} \\right)^{-1}\\left(\\intop \\text d(\\delta P)u_{\\theta ^*}\\right)\n$$ {#eq-differential-theta-star}\n\nwhere we have defined:\n\n$$\nu_\\theta = \\frac{\\partial }{\\partial \\theta} \\ln q_\\theta,\\quad I_\\theta = -\\frac{\\partial^2 }{\\partial \\theta ^2}  \\ln q_\\theta.\n$$ {#eq-score-fisher-info}\n\nFrom @eq-differential-theta-star we can identify the influence function of the $\\theta ^*$ functional:\n\n$$\n\\psi_P(y)=\\left(\\intop \\text dP\\,I_{\\theta ^*} \\right)^{-1}u_{\\theta ^*}(y)\n$$ {#eq-influence-function}\n \nThen, from the standard theory of influence functions, we have:\n\n$$\n\\hat \\theta _N \\approx \\theta ^*+J ^{-1} U\n$$ {#eq-theta-N-first-order}\n\nwhere we have defined:\n\n$$\nJ\\equiv \\intop \\text dP\\,I_{\\theta ^*},\\quad U\\equiv\\frac{1}{N}\\sum _{i=1}^Nu_{\\theta ^*}(Y_i).\n$$ {#eq-score-fisher-info-2}\nIn particular, we obtain the Central Limit Theorem (CLT)\n\n$$\n\\sqrt N(\\hat \\theta _N - \\theta ^*) \\overset{d}{\\to} \\mathcal N(0, J^{-1}KJ^{-1}),\n$$ {#eq-theta-clt}\n\nwith:\n\n$$\nK = \\mathbb V(u_{\\theta ^*}(Y)). \n$$ {#eq-score-variance}\n\nThe matrices $K$ and $J$ depend on the unknown value $\\theta ^*$, but we can readily construct plugin estimators:\n\n$$\n\\hat J_N = -\\frac{1}{N}\\sum _{i=1}^NI_{\\hat \\theta _N}(Y_i),\\quad\\hat K_N = \\frac{1}{N}\\sum _{i=1}^Nu_{\\hat \\theta _N}(Y_i)u_{\\hat \\theta _N}(Y_i)^T,\n$$ {#eq-plugin-jk}\n\nand estimate the variance of $\\hat \\theta _N$ as:\n\n$$\n\\widehat {\\mathbb V}(\\hat \\theta _N) = \\frac{\\hat J _N ^{-1}\\hat K_N\\hat J_N ^{-1}}{N},\n$$ {#eq-sandwich-estimator}\n\nwhich is the usual Sandwich estimator. Finally, if \n$P = Q_{\\theta^*}$, then $J = K$, and the CLT @eq-theta-clt becomes simply\n\n$$\n\\sqrt N(\\hat \\theta _N - \\theta ^*) \\overset{d}{\\to} \\mathcal N(0, J^{-1}).\n$$\n\nLet us now consider the following expansion of $c_P(\\hat \\theta _N)$ which, we recall, is the cross-entropy of the ML model on the true distribution $P$ (*cf.* @eq-cross-ent-integral):\n\n$$\n\\begin{split}\nc_P(\\hat \\theta _N)\n    &= -\\intop \\text d P(y')\\,\\ln (q_{\\hat \\theta}(y'))\\\\\n    & \\approx -\\mathbb E'(\\ln q_{\\theta^*})+\\frac{1}{2}(\\hat \\theta-\\theta ^*)^TJ (\\hat \\theta-\\theta ^*)\\\\\n    & \\approx -\\mathbb E'(\\ln q_{\\theta^*})+\\frac{1}{2}U^TJ^{-1}U\n\\end{split}\n$$\n\nTaking the expectation with respect to the training dataset, noting that $\\mathbb E(U_{\\theta ^*}U_{\\theta ^*}^T)=K_{\\theta ^*}$, we get:\n\n$$\n\\mathbb E (c_P(\\hat \\theta _N))\\approx -\\mathbb E'(\\ln q_{\\theta^*})+\\frac{1}{2N}\\text {Tr}(J^{-1}K)\n$$ {#eq-cross-ent-exp}\n\nNow consider the in-sample estimate:\n\n$$\n\\begin{split}\nc_{\\hat P _N}(\\hat \\theta _N) &= -\\frac{1}{N}\\sum _{i=1}^N\\ln q_{\\hat \\theta}(Y_i)\\\\\n& \\approx - \\frac{1}{N}\\sum _{i=1} ^N \\ln q_{\\theta^*}(Y_i)- U^T(\\hat \\theta _N-\\theta^*)+ \\frac{1}{2}(\\hat \\theta _N-\\theta^*)^TJ(\\hat \\theta _N-\\theta^*)\\\\\n& \\approx - \\frac{1}{N}\\sum _{i=1} ^N \\ln q_{\\theta^*}(Y_i)- U^TJ ^{-1} U+ \\frac{1}{2}U^TJ ^{-1}\\hat J_N J^{-1}U\\\\\n& \\approx - \\frac{1}{N}\\sum _{i=1} ^N \\ln q_{\\theta^*}(Y_i)- \\frac{1}{2}U^TJ ^{-1} U.\n\\end{split}\n$$ \n\nTaking the expectation:\n\n$$\n\\mathbb E (c_{\\hat P _N}(\\hat \\theta _N)) = -\\mathbb E'(\\ln q_{\\theta^*})-\\frac{1}{2N}\\text{Tr}(J^{-1}K)\n$$ {#eq-in-sample-cross-ent-exp}\n\nComparing @eq-in-sample-cross-ent-exp and @eq-cross-ent-exp we see that:\n\n$$\n\\text{TIC}\\equiv -\\frac{1}{N}\\sum _{i=1}^N\\ln q_{\\hat \\theta}(Y_i)+\\frac{1}{N}\\text{Tr}(J^{-1}K)\n$$ {#eq-tic}\n\nprovides an asymptotically unbiased estimate of $\\mathbb E (c_P(\\hat \\theta _N))$, the expected cross-entropy of a model from family $\\mathcal Q$ estimated on a sample of $N$ observations.\n\nThe previous derivation assumed the $Y_i$ to be i.i.d. and does not apply, strictly speaking, to the case of regression, for which we need some more machinery. Assume that the pairs $(X_i,\\,Y_i)$ are drawn independently from a joint $X-Y$ distribution. Instead of @eq-cross-ent-integral, we consider:\n\nWe define, as in the i.i.d. case:\n\n$$\n\\begin{split}\n\\theta ^*(P;\\mathbf X)&=\\arg\\max _{\\theta} \\frac{1}{N}\\sum_{i=1}^N\\intop \\text dP(y\\vert X_i)\\,\\ln \\left(\\frac{1}{q_{\\theta}(y\\vert X_i)}\\right),\\\\\n\\theta ^*(P)&=\\arg\\max _{\\theta} \\intop \\text dP(y,x)\\,\\ln \\left(\\frac{1}{q_{\\theta}(y\\vert X_i)}\\right),\\\\\n\\hat \\theta _N&=\\arg\\max _{\\theta} \\sum _{i=1}^N\\ln \\left(\\frac{1}{q_{\\theta}(Y_i\\vert X_i)}\\right)\n\\end{split}\n$$ {#eq-theta-conditional}\n\nNoticing that $\\hat \\theta _N$ is a plugin estimate of $\\theta ^*$, we can repeat *mutatis mutandis* the steps leading to the CLT @eq-theta-clt, which is also valid in this case.\n\nRather than doing so, let us consider $\\hat \\theta _N$ as the $\\mathbf X$-conditional plugin estimate of $\\theta ^*(P;\\mathbf X)$, and the latter as a plugin estimate of $\\theta ^*(P)$ interpreted as a functional of the $X$ marginal distribution. Then, a parallel derivation to the one provided above for the i.i.d. case shows the conditional convergence in distribution:\n\n$$\n\\sqrt N(\\hat \\theta _N - \\theta ^*(P;\\mathbf X))\\overset{d \\vert \\mathbf X}{\\to} \\mathcal N(0, J_{N}^{-1}(\\mathbf X)K_{N}(\\mathbf X)J_{N}^{-1}(\\mathbf X)).\n$$ {#eq-clt-conditional}\n\nas well as the unconditional convergence:\n\n$$\n\\sqrt N(\\theta ^*(P;\\mathbf X) - \\theta ^*(P))\\overset{d }{\\to} \\mathcal N(0, J^{-1}\\tilde K J^{-1}).\n$$ {#eq-clt-unconditional}\n\nwhere the various matrices are defined as:\n\n$$\n\\begin{split}\nJ_N(\\mathbf X)&\\equiv \\frac{1}{N}\\sum _{i=1}^N\\mathbb E\\left[I _{\\theta} \\bigg\\vert X=X_i\\right]\\bigg\\vert_{\\theta = \\theta ^*(\\mathbf X)},\\\\\n\\quad K_N(\\mathbf X)&\\equiv\\frac{1}{N}\\sum _{i=1}^N\\mathbb V\\left[u _{\\theta }\\bigg\\vert X=X_i\\right]\\bigg\\vert_{\\theta = \\theta ^*(\\mathbf X)}\n\\end{split}\n$$ {#eq-jk-conditional}\n\nand: \n\n$$\n\\begin{split}\nJ&\\equiv \\mathbb E\\left[I_{\\theta^*} \\right],\\\\\n\\quad \\tilde K&\\equiv\\mathbb V\\left[\\mathbb E\\left(u_{\\theta ^*} \\vert X\\right)\\right]\n\\end{split}\n$$ {#eq-jk-unconditional}\n\nHere $I_\\theta$ and $u_\\theta$ are again defined as in @eq-score-fisher-info, but regarded as functions of the random pair $\\{(X,\\,Y)\\}$, rather than $Y$ alone. Although @eq-jk-conditional is written for $\\theta = \\theta ^*(\\mathbf X)$, to the order of the present approximation we may as well substitute $\\theta ^*(\\mathbf X) \\approx \\theta ^*$. Doing this, we can easily see that $J_N(\\mathbf X) \\to J$, and $K_N(\\mathbf X) \\to \\mathbb E\\left[\\mathbb V\\left(u_{\\theta } \\vert X\\right)\\right]\\bigg\\vert_{\\theta = \\theta ^*}$. This can be used to find the unconditional variance of $\\hat \\theta _N$:\n\n$$\n\\begin{split}\n\\mathbb V(\\hat \\theta _N)\n    &=\\mathbb E (\\mathbb V(\\hat \\theta _N \\vert \\mathbf X))+\\mathbb V (\\mathbb E(\\hat \\theta _N \\vert \\mathbf X))\\\\\n    &=\\mathbb E (\\mathbb V(\\hat \\theta _N \\vert \\mathbf X))+\\mathbb V (\\theta ^*(\\mathbf X))\\\\\n    &=J^{-1}\\left(\\mathbb V\\left[\\mathbb E\\left(u_{\\theta ^*} \\vert X\\right)\\right]+\\mathbb E\\left[\\mathbb V\\left(u_{\\theta ^*} \\vert X\\right)\\right]\\right)J^{-1}\\\\\n    &= J^{-1} KJ^{-1}\n\\end{split}\n$$ with $K = \\mathbb V(u_{\\theta^*})$ as in the i.i.d. case, in agreement with the CLT @eq-theta-clt. Our derivation here shows how the variance of $\\hat \\theta _N$ decomposes into a component due to the variability of $X$, and a component due to the residual variability of $Y$ given $X$.\n\nThe corresponding result for the TIC @eq-tic is slightly less straightforward. Repeating the steps leading to this equation for a fixed sample of regressors $\\mathbf X$, we find that:\n\n$$\n\\mathbb E (\\text{TIC}\\vert \\mathbf X)=\\intop \\prod_{i=1}^N\\text dP(y_i\\vert X_i)\\,\\,\\frac{1}{N}\\sum_{j=1}^N\\intop \\text dP(y^\\prime\\vert X_j)\\ln \\left(\\frac{1}{q_{\\hat \\theta_N}(y^\\prime \\vert X_j)}\\right),\n$$ {#eq-conditional-tic-regressors}\n\nwhere the outer integral is a conditional expectation on the sample responses, while the inner integrals are expectations with respect to a new response associated to a sample regressor $X_i$. If we now average over $\\mathbf X$, we\\\nfind:\n\n$$\n\\mathbb E (\\text{TIC})=\\intop \\prod_{i=1}^N\\text dP(x_i,y_i)\\,\\,\\frac{1}{N}\\sum_{j=1}^N\\intop \\text dP(y^\\prime\\vert x_j)\\ln \\left(\\frac{1}{q_{\\hat \\theta_N}(y^\\prime \\vert x_i)}\\right)=\\mathbb E(\\text{CE}_\\text{in}).\n$$ {#eq-unconditional-tic-regressors}\n\nThe right-hand side is the expected in-sample cross-entropy, which is in general different from the extra-sample cross-entropy:\n\n$$\n\\mathbb E(\\text{CE}) =\\intop \\prod_{i=1}^N\\text dP(x_i,y_i)\\intop \\text dP(x^\\prime,y^\\prime)\\ln \\left(\\frac{1}{q_{\\hat \\theta_N}(y^\\prime \\vert x^\\prime)}\\right).\n$$ {#eq-extra-sample-ce}\n\n## References\n\n-   [@shaliziADA]\n-   [@claeskens2008model]\n-   [@freedman2006so]\n-   [@white1982maximum]\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}