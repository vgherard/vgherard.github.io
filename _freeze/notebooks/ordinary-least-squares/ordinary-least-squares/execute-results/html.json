{
  "hash": "5135d1cf5a934b04542b0c3a63c3f5df",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Ordinary Least Squares\"\ndate: 2024-02-07\nbibliography: biblio.bib\ndraft: false\n---\n\n\n\n# Generalities\n\nOrdinary Least Squares (OLS) is a regression algorithm for estimating the best linear predictor (BLP) $f^*(X) = X\\beta$ of a response $Y \\in \\mathbb R$ in terms of a vector of regressors $X\\in \\mathbb R ^p$, which we will frequently identify with an $1\\times p$ row matrix. Here, \"best\" is understood in terms of the $L_2$ error:\n\n$$\n\\beta = \\arg \\min _{\\beta '}  \\mathbb E[(Y - X\\beta ^\\prime)^2]=\\mathbb E (X^TX)^{-1}\\mathbb E(X^T Y),\n$$ {#eq-blp}\n\nwhere the first equation is the defining one, while the second one follows from elementary calculus.\n\nGiven i.i.d. data $\\{(X_i,\\,Y_i)\\}_{i=1,\\,2,\\,\\dots,\\,N}$, and denoting by $\\mathbf Y$ and $\\mathbf X$ the $N\\times 1$ and $N\\times p$ matrices obtained by vertically stacking independent observations of $Y$ and $X$, respectively, the OLS estimate of @eq-blp is defined by:\n\n$$\n\\hat \\beta = \\arg \\min _{\\beta '}  \\sum _{i=1} ^N \\frac{1}{N}(Y_i - X_i\\beta ^\\prime)^2 = (\\mathbf X ^T \\mathbf X) ^{-1} \\mathbf X ^T \\mathbf Y,\n$$ {#eq-ols}\n\nwhich is readily recognized to be the plugin estimate of $\\beta$. Correspondingly, we define the OLS predictor:\n\n$$\n\\hat Y (x) = x \\hat \\beta. \n$$ {#eq-ols-predictor}\n\nWhat motivates the use of an $L_2$ criterion in @eq-blp?\n\n1.  *Mathematical tractability.* The fact that @eq-blp admits a closed form solution, which is furthermore linear in the response variable $Y$, greatly simplifies the analysis of the properties of $\\beta$ and its estimators such as the OLS one @eq-ols, making it a perfect study case.\n\n2.  *Numerical tractability.* A consequence of the previous point, but worth a separate mention. Computing the plugin estimate in @eq-ols is just a matter of basic linear algebra manipulations, which, with modern software libraries, is a relatively cheap operation.\n\n3.  *Normal theory.* Focusing on the consequence of @eq-blp, namely the plugin estimate @eq-ols, if the conditional distribution of $Y$ given $X$ is normal with constant variance, and if $\\mathbb E(Y\\vert X)$ is truly linear, $\\beta$ coincides with the maximum likelihood estimate of $\\beta$.\n\n## The linear model\n\nThe term generally refers to a model for the conditional distribution of $Y\\vert X$ that requires the conditional mean $\\mathbb E(Y\\vert X)$ to be a linear function of $X$. In its most parsimonious form, this is just:\n\n$$\nY = X \\beta + \\varepsilon, \\quad \\mathbb E(\\varepsilon\\vert X) = 0.\n$$ {#eq-linear-model}\n\nThat said, depending on context, @eq-linear-model is usually supplemented with additional assumptions that further characterise the conditional distribution of the error term[^1], typically (with increasing strength of assumptions):\n\n[^1]: I use the notation $A\\perp B$ to indicate that the random variables $A$ and $B$ are statistically independent.\n\n-   *Constant Variance.* $\\mathbb V(\\varepsilon \\vert X) = \\sigma ^2$, independently of $X$.\n-   $X$-Independent Errors. $\\varepsilon \\perp X$.\n-   *Normal Errors.* $\\varepsilon \\vert X \\sim \\mathcal N (0,\\sigma ^2)$.\n\nWhile the OLS estimator is well-defined irrespective of the validity of any of these models, it is clear that, in order for $\\hat \\beta$ to represent a meaningful summary of the $Y$-$X$ dependence, one should require at least @eq-linear-model to hold in some approximate sense. Correspondingly, while some general features of $\\hat \\beta$ can be discussed independently of linear model assumptions, its most important properties crucially depend on @eq-linear-model.\n\n# Properties\n\n## Algebraic properties of BLP and OLS estimates\n\nConsider the BLP $f^*(X) = X\\beta$, with $\\beta$ defined as in @eq-blp. We usually assume that the covariate vector $X$ contains an intercept term, that is \n\n$$\nX=\\begin{pmatrix}1 & Z\\end{pmatrix},\\quad Z\\in \\mathbb R^{p-1}\n$$ {#eq-x-to-z}\n\nfor some $p-1$ dimensional random vector $Z$. The presence of an intercept term leads to $f^*(X)$ having a bunch of nice properties, such as being unconditionally unbiased ($\\mathbb E(Y-f^*(X))=0$), as we show below.\n\nLet us decompose $\\beta = \\begin{pmatrix}a & b\\end{pmatrix}$, where $a\\in \\mathbb R$ is the intercept term, and $b \\in \\mathbb R^{p-1}$ is the coefficient of $Z$. We can easily prove that:\n\n$$\n\\begin{split}\nb  &=\n\\mathbb V (Z)^{-1}\\mathbb V(Z,Y),\\\\\na  &= \\mathbb E(Y)-\\mathbb E(Z)b, \n\\end{split}\n$$ {#eq-blp-0}\n\nwhere we denote by $\\mathbb V (A,B)$ the covariance matrix of $A$ and $B$, and by $\\mathbb V (A)\\equiv \\mathbb V (A,A)$. These expressions can be used to recast the error term as:\n\n$$\nY-X\\beta = (Y-\\mathbb E(Y))-(Z-\\mathbb E(Z))\\mathbb V (Z)^{-1}\\mathbb V(Z,Y).\n$$ {#eq-blp-error-term}\n\nFrom this expression we can easily find the first two moments:\n\n$$\n\\begin{split}\n\\mathbb E(Y-X\\beta)&=0,\\\\\n\\mathbb E((Y-X\\beta)^2)&=\\mathbb V(Y)-\\mathbb V(Z,Y)^T\\mathbb V(Z)^{-1}\\mathbb V(Z,Y), \n\\end{split}\n$$ {#eq-blp-expected-error}\n\nIn particular, as anticipated the best linear predictor is unconditionally unbiased. More generally, from @eq-blp-error-term it follows one has:\n\n$$\n\\mathbb E(X^T(Y-X\\beta))=0\n$$ {#eq-orthogonality-error-term} \nwhich, since $\\mathbb E(Y-X\\beta) = 0$, can also be interpreted as saying that the error term $Y-X\\beta$ and the covariate vector $X$ are uncorrelated.\n\nThese properties directly translate to corresponding properties of the empirical residuals $\\mathbf Y-\\mathbf X\\hat \\beta$. Notice that, up to now no result depends on the particular probability measure to which expectations refer to. In particular, we can choose this measure to be the empirical distribution realized in a specific sample, which amounts to replace all expectations with sample means. Thus, @eq-orthogonality-error-term translates to:\n\n$$\n\\frac{1}{N}\\mathbf X^T(\\mathbf Y-\\mathbf X\\hat \\beta)=0,\n$$ {#eq-orthogonality-sample-residuals}\n\nwhich implies in particular that sample residuals have vanishing sample means.\n\n## Distribution of coefficient estimates $\\hat \\beta$\n\nAs an estimator of $\\beta$ as defined in @eq-blp, the OLS estimator $\\hat \\beta$ is consistent (converges in probability to $\\beta$) but generally biased (an example is provided [here](https://vgherard.github.io/posts/2023-05-12-consistency-and-bias-of-ols-estimators/)). However, due to the plugin nature of $\\hat \\beta$, the bias is generally of order $\\mathcal O (N^{-1})$, which makes it often negligible in comparison to its $\\mathcal O(N^{-1/2})$ sampling variability (see below).\n\nExplicitly, the bias is given by: \n\n$$\n\\mathbb E(\\hat \\beta) - \\beta = \\mathbb E\\lbrace((\\mathbf X ^T \\mathbf X) ^{-1}-\\mathbb E[(\\mathbf X ^T \\mathbf X) ^{-1}])\\cdot \\mathbf X ^T f (\\mathbf X)\\rbrace,\n$$ {#eq-bias-ols}\n\nwhere $f(X) \\equiv \\mathbb E(Y \\vert X)$ is the true conditional mean function. In general, this vanishes only if $f(X)=X\\beta$, as in the linear expectation model @eq-linear-model.\n\nThe $\\mathbf X$-conditional variance of $\\hat \\beta$ can be derived directly from @eq-ols:\n\n$$\n\\mathbb V (\\hat \\beta \\vert \\mathbf X)=(\\mathbf X ^T \\mathbf X) ^{-1} \\mathbf X ^T  \\mathbb V (\\mathbf Y\\vert \\mathbf X) \\mathbf X (\\mathbf X ^T \\mathbf X), \n$$ {#eq-var-beta-hat-cond}\n\nwhere $\\mathbb V (\\mathbf Y\\vert \\mathbf X)$ is diagonal for i.i.d. observations. For homoskedastic errors we get:\n\n$$\n\\mathbb V (\\hat \\beta \\vert \\mathbf X)=(\\mathbf X ^T \\mathbf X) ^{-1} \\sigma ^2 \\quad (\\mathbb V(Y\\vert X)=\\sigma ^2).\n$$ {#eq-var-beta-hat-cond-homo}\n\nUnder the normal linear model, this allows to obtain finite-sample correct confidence sets for $\\beta$. In the general case, confidence sets can be derived from the Central Limit Theorem satisfied by $\\hat \\beta$ [@buja2019models]:\n\n$$\n\\sqrt N (\\hat \\beta -\\beta) \\to \\mathcal N (0,V )\n$$ {#eq-clt-beta-hat}\n\nwhere the asymptotic variance is given by:\n\n$$\nV = \\mathbb E[X^TX] ^{-1} \\cdot \\mathbb E[X^T(Y-X\\beta)^2X] \\cdot \\mathbb E[X^TX] ^{-1}.\n$$ {#eq-asy-var-beta-hat}\n\nThe plugin estimate of @eq-asy-var-beta-hat leads to the so called Sandwich variance estimator:\n\n$$\nV_\\text{sand} \\equiv  (\\mathbf X^T \\mathbf X)^{-1} \\mathbf X^T D_\\mathbf {r^2}\\mathbf X (\\mathbf X^T \\mathbf X)^{-1},\n$$ {#eq-sandwich-estimate}\n\nwhere $D_{\\mathbf r^2}$ is the diagonal matrix whose $i$-th entry is the squared residual $r_i ^2 = (Y_i-X_i\\beta)^2$.\n\n## Variance estimates\n\nThese can be based on:\n\n$$\ns ^2 = \\frac{1}{N}(\\mathbf Y-\\mathbf X \\hat \\beta)^T(\\mathbf Y-\\mathbf X \\hat \\beta)\n$$ which has expectation\n\n$$\n\\mathbb E\\left[s^2\\right]=\\frac{1}{N}\\text {Tr}\\,\\mathbb E \\left[ (1-\\mathbf H) \\cdot \\left(\\mathbb E(\\mathbf Y \\vert \\mathbf X)\\mathbb E(\\mathbf Y \\vert \\mathbf X)^T + \\mathbb V(\\mathbf Y\\vert\\mathbf X)\\right) \\right] \n$$ {#eq-expected-in-sample-error}\n\nwhere the hat matrix $\\mathbf H$ is defined as usual:\n\n$$\n\\mathbf H \\equiv \\mathbf X(\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T\n$$ {#eq-hat-matrix}\n\nIf the general linear model @eq-linear-model holds, so that $E(\\mathbf Y \\vert \\mathbf X) = \\mathbf X \\beta$, we have $(1-\\mathbf H) \\mathbb E(\\mathbf Y \\vert \\mathbf X) = 0$. If we furthermore assume homoskedasticity, we obtain:\n\n$$\n\\mathbb E\\left[s^2\\right]=\\frac{N-p}{N}\\sigma^2 \\quad(\\text{Homoskedastic linear model}), {#eq-variance-estimate-homo}\n$$ \n\nwhere $p = \\text {Tr}(\\mathbf H)$ is the number of independent covariates. On the other hand, if homoskedasticity holds, but $E(\\mathbf Y \\vert \\mathbf X)$ is not linear, the left-hand side of the previous equation is an overestimate of $\\mathbf V \\vert X$.\n\n# Measurement error\n\nSuppose that, rather than the variables of interest $Y$ and $X$, we can only observe noisy versions:\n\n$$\n\\widetilde Y = Y+\\delta_Y,\\qquad \\widetilde Z=Z+\\delta _Z\n$$ {#eq-noisy-y-and-z}\n\nwhere $X=\\begin{pmatrix}1 & Z\\end{pmatrix}$ as in @eq-x-to-z. We are truly interested in $\\beta$ defined by @eq-blp, but OLS will actually estimate $\\widetilde \\beta =\\begin{pmatrix}\\widetilde a & \\widetilde b\\end{pmatrix}$, the coefficient of the BLP of $\\widetilde Y$ in terms of $\\widetilde X$. Focusing on the slope term $\\widetilde b$, a comparison with @eq-blp-0 yields:\n\n$$\n\\begin{split}\n\\widetilde b -b&= \\Delta_{\\mathbb V(Z)^{-1}}\\cdot\\mathbb V(Z,Y) +\\mathbb V(Z)^{-1} \\cdot \\Delta _{\\mathbb V(Z,Y)}+\\Delta_{\\mathbb V(Z)^{-1}}\\cdot\\Delta _{\\mathbb V(Z,Y)},\\\\\n\\Delta_{\\mathbb V(Z)^{-1}}&=\\mathbb V (\\widetilde Z)^{-1}-V(Z)^{-1}, \\\\ \n\\Delta _{\\mathbb V(Z,Y)} &=\\mathbb V(\\widetilde Z,\\widetilde Y)-\\mathbb V(Z,Y).\n\\end{split}\n$$ {#eq-delta-b}\n\nwhere, explicitly:\n\n$$\n\\begin{split}\n\\mathbb V(\\widetilde Z)&=\\mathbb V(Z)+\\mathbb V(\\delta _Z)+2\\mathbb V(Z,\\delta _Z),\\\\\n\\mathbb V(\\widetilde Z,\\widetilde Y)&=\\mathbb V(Z,Y)+\\mathbb V(\\delta _Z,Y)+\\mathbb V(\\delta _Y,Z)+\\mathbb V(\\delta _Y,\\delta _Z).\n\\end{split}\n$$ {#eq-delta-v-vs-noise}\n\nThe effect on the slope estimate generally depends on the correlation structure of signal and noise. In the special case of \"totally random\" noise, that is if $\\delta _Z$ and $\\delta _Y$ are independent of each other and of $Z$ and $Y$, we see that the only effect is an increase $\\mathbb V(\\widetilde Z) = \\mathbb V(Z)+ \\mathbb V(\\delta _Z)$, which shrinks the slope coefficient $\\widetilde b$ towards zero. In particular, if random noise is only present in the response ($\\delta _Z = 0$), the estimation target is unaffected (although the variance of the $\\hat b$ estimator is, in general).\n\n# Proofs\n\n## Proof of Central Limit Theorem for $\\hat \\beta$\n\nConvergence to the normal distribution @eq-clt-beta-hat with variance @eq-asy-var-beta-hat can be proved using the formalism of influence functions. From @eq-blp, we see that a small variation $P\\to P+\\delta P$ to the joint $XY$ probability measure induces a first order shift:\n\n$$\n\\delta \\beta =  \\intop \\mathbb E(X^TX)^{-1}X^T (Y-\\beta X) \\text d(\\delta P)\n$$ {#eq-beta-differential}\n\nin the best linear predictor. The influence function of $\\beta$ is defined by the measurable representation of $\\delta \\beta$, namely:\n\n$$\n\\phi _\\beta = \\mathbb E(X^TX)^{-1}X^T (Y-\\beta X). \n$$ {#eq-beta-influence-function}\n\nA general result for plugin estimates then tells us that $\\sqrt N (\\hat \\beta -\\beta) \\to \\mathcal N (0, \\mathbb E (\\phi _\\beta ^2))$ in distribution, and using the explicit form of @eq-beta-influence-function we readily obtain @eq-asy-var-beta-hat.\n\n# Related posts\n\n-   [Consistency and bias of OLS estimators](https://vgherard.github.io/posts/2023-05-12-consistency-and-bias-of-ols-estimators/)\n-   [Model misspecification and linear sandwiches](https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches/)\n-   [Linear regression with autocorrelated noise](https://vgherard.github.io/posts/2023-05-20-linear-regression-with-autocorrelated-noise/)\n-   [Testing functional specification in linear regression](https://vgherard.github.io/posts/2023-07-11-testing-functional-specification-in-linear-regression/)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}