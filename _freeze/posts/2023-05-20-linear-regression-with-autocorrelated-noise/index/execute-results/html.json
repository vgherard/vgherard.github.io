{
  "hash": "be81360ee64b74d7eb42dd2dc1f40cf0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear regression with autocorrelated noise\"\ndescription: |\n  Effects of noise autocorrelation on linear regression. Explicit formulae and a simple simulation.\ndate: 2023-05-25\ncategories: \n  - Statistics\n  - Regression\n  - Time Series\n  - Linear Models\n  - Model Misspecification\n  - R\ndraft: false\n---\n\n\n\n\nConsider two time series $Y_t$ and $X_t$ such that:\n\n$$\nY_t =  X_t \\cdot \\beta+\\eta_t\n$$ {#eq-y-vs-x}\n\nwhere $\\eta_t$ is $\\text{AR}(1)$ noise:\n\n$$\n\\eta_{t+1} = \\alpha \\eta_t + \\epsilon_t, \\qquad \\epsilon _t \\sim \\mathcal N(0,\\sigma^2_0)                                                               \n$$ {#eq-ar1}\n\nBy iteration of @eq-ar1, we see that $\\eta_t$ has gaussian *unconditional distribution*:\n\n$$\n\\eta_t \\sim \\mathcal N (0, \\sigma ^2),\\qquad \\sigma^2 \\equiv \\frac{\\sigma^2_0}{1-\\alpha ^2}                             \n$$ {#eq-ar1-unconditional}\nso that individual observations of $(X_t,\\,Y_t)$ are distributed according to a perfectly specified linear model.\n\nThis does *not* mean that, given observational data $\\{(X_t,\\,Y_t)\\}_{t = 1,\\,2,\\,\\dots,\\,T}$, we are allowed to make standard linear model assumptions to perform valid inference on the parameters $\\beta$ and $\\sigma$ of @eq-y-vs-x and @eq-ar1-unconditional. Since the noise terms $\\eta _t$ are not independent draws from a single distribution, but are rather autocorrelated, the usual OLS variance estimate under linear model assumptions will be biased, as we show below [^1].\n\n[^1]: For the linear model assumptions to hold, the $(X_t,\\,Y_t)$ pairs should come from *independent realizations* of the same time series, which is of course not the type of data we are usually presented with.\n\nIt is fairly easy to work out the consequences of autocorrelation. Suppose, more generally, that the error term $\\eta _t$ is a stationary time series with unconditional mean $\\mathbb E(\\eta_t)=0$ and unconditional variance $\\text{Var}(\\eta _t)=\\sigma ^2$. The OLS estimate of $\\beta$ is[^2]:\n\n[^2]: As usual we stack observations vertically in the $\\mathbf X$ and $\\mathbf Y$ matrices.\n\n$$\n\\hat \\beta =(\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T\\mathbf Y=\\beta + (\\mathbf X^T\\mathbf X)^{-1} \\mathbf X^T \\mathbf{η},\n$$ {#eq-ols-beta}\n\nwhich is unbiased since $\\mathbb E (\\mathbf{η}) = 0$. The estimate of the noise variance $\\sigma ^2$, on the other hand:\n\n$$\n\\begin{split}\n\\hat \\sigma ^2  & = \\frac{(\\mathbf Y - \\mathbf X\\hat \\beta)^T(\\mathbf Y - \\mathbf X\\hat \\beta)}{N-p}= \\frac{\\mathbf{η}^T(\\mathbf 1-\\mathbf H)\\mathbf{η} }{N-p} \\\\\n\\mathbb E (\\hat \\sigma ^2) & = \\dfrac{\\text {Tr}\\left[(\\mathbf 1- \\mathbb E(\\mathbf H))\\cdot  \\text {Cor}(\\mathbf{η})\\right]}{N-p}\\sigma ^2                     \n\\end{split}\n$$ where $\\mathbf H = \\mathbf X(\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T$ as usual, and we have used the fact that $\\mathbb {V}( \\mathbf{η} ) = \\sigma ^2 \\cdot \\text {Cor}(\\mathbf{η})$ (since each $\\eta_t$ has the same unconditional variance $\\sigma ^2$). Hence the $\\hat \\sigma ^2$ OLS estimate is biased if $\\text{Cor}(\\mathbf{η})\\neq \\mathbf 1$.\n\nSimilarly, the variance-covariance matrix of the OLS $\\hat \\beta$ estimator is:\n\n$$\n\\mathbb V (\\hat \\beta) = \\mathbb E\\left[(\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T\\text {Cor}(\\mathbf{η})\\mathbf X (\\mathbf X^T\\mathbf X)^{-1} \\right]\\sigma^2\n$$ whereas its OLS estimate is:\n\n$$\n\\hat {\\mathbb V} (\\hat \\beta) = (\\mathbf X^T\\mathbf X)^{-1} \\hat \\sigma ^2\n$$ which is biased for $\\text{Cor}(\\mathbf{η})\\neq \\mathbf 1$.\n\nEven though the variance estimators are themselves biased, the biases could still vanish in the asymptotic limit. This is the case for $\\hat \\sigma ^2$, as we can see by rewriting:\n\n$$\n\\dfrac{\\mathbb E (\\hat \\sigma ^2)}{\\sigma ^2}-1 = -\\dfrac{1}{{N-p}}\\text {Tr}\\left[\\mathbb E(\\mathbf H)^T\\cdot(\\text {Cor}(\\mathbf{η})-\\mathbf 1)\\cdot \\mathbb E(\\mathbf H)\\right]                      \n$$ where we have used the projector properties of $\\mathbf H$ to recast the trace in terms of a symmetric operator. In principle, nothing prevents the operator above to have $O(N)$ eigenvalues, which would make the $\\hat \\sigma ^2$ estimator asymptotically biased[^3]. In realistic cases, one expects the correlations $\\text{Cor}(\\eta_t,\\eta_{t'})$ to decay exponentially with $\\vert t - t'\\vert$ [^4] , in which case the trace is bounded to be of $O(p)$, and $\\mathbb E(\\hat \\sigma ^2) \\to \\sigma ^2$ as $N\\to \\infty$.\n\n[^3]: For an extreme case, suppose that $\\mathbf X = \\mathbf e$ (no covariate except for an intercept term), and let the noise term be $\\eta _t = Z_0 + Z_t$, where $Z_0$ and $\\{Z_t\\}_{t=1,2,\\dots,T}$ are independent $Z$-scores. One can easily see that, in this setting, $\\text {Cor}(\\eta) = \\frac{1}{2}(\\mathbf 1+\\mathbf e \\mathbf e^T )$ and $\\text{Tr}(\\cdots) \\approx \\frac{N}{2}$.\n\n[^4]: For instance, for the $\\text{AR}(1)$ noise of Eq. @eq-ar1, we have $\\text{Cor}(\\eta_t, \\eta_{t'})= \\alpha ^{\\vert t - t'\\vert}$.\n\nFor $\\hat {\\mathbb V} (\\hat \\beta)$ things are not so favorable. It is enough to consider a special case of a plain intercept term: $X=1$. In this case, we find with some manipulations:\n\n$$\n\\begin{split}\n\\mathbb V (\\hat \\beta) &= \\frac{\\sigma ^2}{N}\\left(1+\\frac{1}{N}\\sum _{t\\neq t'} \\text{Cor}(\\eta_t,\\eta_{t'})\\right),\\\\\n\\mathbb E(\\hat {\\mathbb V} (\\hat \\beta)) & = \\frac{\\sigma ^2}{N}\\left(1-\\frac{1}{N(N-1)}\\sum _{t\\neq t'} \\text{Cor}(\\eta_t,\\eta_{t'})\\right)\n\\end{split}\n$$ Since $\\sum _{t\\neq t'}\\text{Cor}(\\eta_t,\\eta_{t'})=O(N)$, we see that:\n\n$$ \n\\lim _{N\\to \\infty} \\dfrac{\\mathbb E(\\hat {\\mathbb V} (\\hat \\beta))}{\\mathbb V(\\hat \\beta)}\\neq 1\n$$ which amounts to say that $\\hat {\\mathbb V} (\\hat \\beta)$ is asymptotically biased[^5].\n\n[^5]: The difference $\\mathbb E(\\hat {\\mathbb V} (\\hat \\beta))-\\mathbb V(\\hat \\beta)$ decays as $O(N^{-1})$, which is of the same order of the estimation target $\\mathbb V (\\hat \\beta)$. Not sure I'm using standard terminology here.\n\n### Illustration\n\nThe (foldable) block below defines helpers to simulate the results of linear regression on data generated according to $Y_t = f(X_t) + \\eta _t$. These are the same functions used in my previous post on [misspecification and sandwich estimators](https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches/) - slightly adapted to the current case.\n\n\n\n\n::: {.cell code_folding='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(dplyr)\nlibrary(ggplot2)\n\nrxy_fun <- function(rx, f, reps) {\n\tres <- function(n) {\n\t\tx <- rx(n)  # X has marginal distribution 'rx'\n\t\ty <- f(x) + reps(x)  # Y has conditional mean 'f(x)' and noise 'reps(x)'\n\t\treturn(tibble(x = x, y = y))  \n\t}\n\treturn(structure(res, class = \"rxy\"))\n}\n\nplot.rxy <- function(x, N = 1000, seed = 840) {\n\tset.seed(seed)\n\t\n\tggplot(data = x(N), aes(x = x, y = y)) +\n\t\tgeom_point(alpha = 0.3) + \n\t\tgeom_smooth(method = \"lm\", se = FALSE)\n}\n\nlmsim <- function(rxy, N = 100, vcov = stats::vcov, B = 1e3, seed = 840) \n{ \n\tset.seed(seed)\n\t\n\tres <- list(\n\t\tcoef = matrix(nrow = B, ncol = 2), \n\t\tvcov = vector(\"list\", B),\n\t\tsigma2 = numeric(B)\n\t\t)\n\tcolnames(res$coef) <- c(\"(Intercept)\", \"x\")\n\tclass(res) <- \"lmsim\"\n\t\t\t\t\t\t\t\t\n\tfor (b in 1:B) {\n\t\t.fit <- lm(y ~ ., data = rxy(N))\n\t\tres$coef[b, ] <- coef(.fit)  # Store intercept and slope in B x 2 matrix\n\t\tres$vcov[[b]] <- vcov(.fit)  # Store vcov estimates in length B list.\n\t\tres$sigma2[[b]] <- sigma(.fit) ^ 2\n\t}\n\t\n\treturn(res)\n}\n\nprint.lmsim <- function(x) \n{\n\tcat(\"Simulation results:\\n\\n\")\n\tcat(\"* Model-trusting noise variance:\\n \")\n\tprint( mean(x$sigma2) )\n\tcat(\"* Model-trusting vcov of coefficient estimates:\\n\")\n\tprint( avg_est_vcov <- Reduce(\"+\", x$vcov) / length(x$vcov) )\n\tcat(\"\\n* Simulation-based vcov of coefficient estimates:\\n\")\n\tprint( emp_vcov <- cov(x$coef))\n\tcat(\"\\n* Ratio (Model-trusting / Simulation):\\n\")\n\tprint( avg_est_vcov / emp_vcov )\n\treturn(invisible(x))\n}\n```\n:::\n\n\n\n\nWe simulate linear regression on data generated according to:\n\n$$\n\\begin{split}\nY_t &= 1 + X_t+\\eta_t,\\\\\nX_{t+1} &= 0.4 \\cdot X_t+Z^X_t,\\\\\n\\eta _{t+1} &= \\frac{1}{\\sqrt 2}\\eta _t +Z^\\eta_t\\\\\n\\end{split}\n$$ where $Z^{X,\\eta}_t\\sim \\mathcal N(0,1)$. The noise $\\eta_t$ is $\\text{AR}(1)$, and results in the unconditional variance of the corresponding linear model $\\text{Var} (\\eta _t) = 2$, twice the conditional variance $\\text{Var}(\\eta _{t+1}\\vert \\eta _t)=\\mathbb E(Z_t ^2)=1$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrxy_01 <- rxy_fun(\n\trx = \\(n) 1 + arima.sim(list(order = c(1,0,0), ar = 0.4), n = n),\n\tf = \\(x) 1 + x,\n\treps = \\(x) arima.sim(\n\t\tlist(order = c(1,0,0), ar = 1/sqrt(2)), \n\t\tn = length(x) \n\t\t)\n)\n\nplot(rxy_01)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\nFrom the simulation below, we see that with $N=100$ serial observations, $\\mathbb E(\\hat \\sigma ^2)$ is relatively close to $\\sigma ^2 = 2$, but the $\\mathbb E(\\hat {\\mathbb V} (\\hat \\beta))$ grossly underestimates all entries (as can be seen from the last line of the output of `lmsim()` below).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmsim(rxy_01, N = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimulation results:\n\n* Model-trusting noise variance:\n [1] 1.870606\n* Model-trusting vcov of coefficient estimates:\n            (Intercept)           x\n(Intercept)  0.03583159 -0.01663739\nx           -0.01663739  0.01659708\n\n* Simulation-based vcov of coefficient estimates:\n            (Intercept)           x\n(Intercept)  0.15486131 -0.02665435\nx           -0.02665435  0.02978162\n\n* Ratio (Model-trusting / Simulation):\n            (Intercept)         x\n(Intercept)   0.2313786 0.6241905\nx             0.6241905 0.5572928\n```\n\n\n:::\n:::\n\n\n\n\nTo correctly estimate $\\mathbb V (\\hat \\beta)$, we could try using the \"autocorrelation-consistent\" sandwich estimator `sandwich::vcovHAC()` [^6]. It turns out that, even with a relatively simple example like the present one, the sample size required for the HAC estimator's bias to die out is unreasonably large (see below). With such large samples, one can probably obtain much better results by leaving out some data for model building, performing inference on the remaining data with a proper time-series model.\n\n[^6]: Disclaimer: I haven't read any theory about the HAC estimator, so I may be misusing it here, but I would have expected it to work relatively well on such an \"easy\" example. For illustrations on how to use sandwich estimators for first- and second-order linear model misspecification, you can read [this post of mine](https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches/).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmsim(rxy_01, vcov = sandwich::vcovHAC, N = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimulation results:\n\n* Model-trusting noise variance:\n [1] 1.870606\n* Model-trusting vcov of coefficient estimates:\n            (Intercept)           x\n(Intercept)  0.08787795 -0.02323242\nx           -0.02323242  0.02339146\n\n* Simulation-based vcov of coefficient estimates:\n            (Intercept)           x\n(Intercept)  0.15486131 -0.02665435\nx           -0.02665435  0.02978162\n\n* Ratio (Model-trusting / Simulation):\n            (Intercept)         x\n(Intercept)   0.5674623 0.8716182\nx             0.8716182 0.7854329\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlmsim(rxy_01, vcov = sandwich::vcovHAC, N = 500)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimulation results:\n\n* Model-trusting noise variance:\n [1] 1.974131\n* Model-trusting vcov of coefficient estimates:\n             (Intercept)            x\n(Intercept)  0.023032270 -0.005723968\nx           -0.005723968  0.005684149\n\n* Simulation-based vcov of coefficient estimates:\n             (Intercept)            x\n(Intercept)  0.029600757 -0.005993161\nx           -0.005993161  0.006152216\n\n* Ratio (Model-trusting / Simulation):\n            (Intercept)         x\n(Intercept)   0.7780973 0.9550834\nx             0.9550834 0.9239189\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlmsim(rxy_01, vcov = sandwich::vcovHAC, N = 1000)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimulation results:\n\n* Model-trusting noise variance:\n [1] 1.98033\n* Model-trusting vcov of coefficient estimates:\n             (Intercept)            x\n(Intercept)  0.011878079 -0.002771484\nx           -0.002771484  0.002849089\n\n* Simulation-based vcov of coefficient estimates:\n             (Intercept)            x\n(Intercept)  0.015085291 -0.002844716\nx           -0.002844716  0.002855522\n\n* Ratio (Model-trusting / Simulation):\n            (Intercept)         x\n(Intercept)   0.7873948 0.9742566\nx             0.9742566 0.9977471\n```\n\n\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}