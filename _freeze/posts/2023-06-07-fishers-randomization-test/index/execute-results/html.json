{
  "hash": "70af011df7bfd482ac1694f28bf14db8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Fisher's Randomization Test\"\ndescription: |\n  Notes and proofs of basic theorems\ndate: 2023-06-07\ncategories: \n  - Statistics\n  - Frequentist Methods\n  - Causal Inference\ndraft: false\n---\n\n\n\n\nLet $N\\in \\mathbb N$ be fixed, and let:\n\n-   $\\mathbf Y(1),\\,\\mathbf Y(0)\\in \\mathbb R ^N$ be random vectors, with components $Y_i(1),Y_i(0)\\in \\mathbb R$,\n-   $\\mathbf Z$ be a random vector with components $Z_i \\in \\{0,1\\}$, independent from $\\mathbf Y(1)$ and $\\mathbf Y(0)$ above,\n-   $\\mathbf Y = \\mathbf Z\\times \\mathbf Y(1)+(1-\\mathbf Z)\\times \\mathbf Y(0)$ (multiplication is component-wise).\n\nGiven a scalar function $t = t(\\mathbf Z, \\,\\mathbf Y)\\in \\mathbb R$, define:\n\n$$\nP(t,\\mathbf Z, \\mathbf Y)=\\sum _{\\mathbf Z '}\\text{Pr}_\\mathbf Z(\\mathbf Z')\\cdot I(t(\\mathbf Z',\\mathbf Y)\\geq t(\\mathbf Z,\\mathbf Y)),\n$$ where $\\text{Pr}_\\mathbf Z(\\cdot)$ is the marginal distribution of treatment assignments.\n\n**Theorem.** If $\\mathbf Y(0) = \\mathbf Y(1)$ then:\n\n$$\n\\text{Pr}(P(t,\\mathbf Z,\\mathbf Y)\\leq \\alpha) \\leq \\alpha.\n$$\n\n*Proof.* Let $\\mathbf Z'$ be distributed according to $\\text{Pr}_\\mathbf Z(\\cdot)$, and define $\\mathbf Y' = \\mathbf Z'\\times \\mathbf Y(1)+(1-\\mathbf Z')\\times \\mathbf Y(0)$. Given $t_0\\in \\mathbb R$, we observe that:\n\n$$\n\\text {Pr}(t(\\mathbf Z',\\mathbf Y')\\geq t_0 \\,\\vert\\,\\mathbf Y(0),\\,\\mathbf Y(1)) = \\sum _{\\mathbf Z '}\\text{Pr}_\\mathbf Z(\\mathbf Z')\\cdot I(t(\\mathbf Z',\\mathbf Y')\\geq t_0).\n$$ Now, if $\\mathbf Y(0) = \\mathbf Y(1)$, we have $t(\\mathbf Z',\\mathbf Y') = t(\\mathbf Z',\\mathbf Y)$, so that we may replace $\\mathbf Y'$ with $\\mathbf Y$ in the RHS of the previous equation. If, moreover, we choose $t_0= t(\\mathbf Z , \\mathbf Y)$ we obtain:\n\n$$\nP(t, \\mathbf Z, \\mathbf Y) = \\text {Pr}(t(\\mathbf Z',\\mathbf Y')\\geq t(\\mathbf Z,\\mathbf Y) \\,\\vert\\,\\mathbf Y(0),\\mathbf Y(1)).\n$$ In other words, $P(t,\\mathbf Z, \\mathbf Y)$ is a conditional $p$-value. Therefore:\n\n$$\n\\text{Pr}(P(t,\\mathbf Z,\\mathbf Y)\\leq \\alpha \\,\\vert\\,\\mathbf Y(0),\\mathbf Y(1)) \\leq \\alpha.\n$$\n\nSince this is valid for any value of $\\mathbf Y (0)$ and $\\mathbf Y(1)$, the thesis follows.\n\n------------------------------------------------------------------------\n\nIn the usual setting of causal inference, we interpret:\n\n-   $Z_i$ as the treatment assignment for the $i$-th statistical unit, $Z_i = 0,1$ standing for \"treatment\" and \"control\", respectively.\n-   $Y_i(1)$ and $Y_i(0)$ as the potential outcomes for the $i$-th unit under treatment and control, respectively.\n-   $Y_i$ as the observed outcome for the $i$-th unit.\n-   $t(\\cdot)$ as a test statistic used to test the null hypothesis $\\mathbf Y(1)= \\mathbf Y (0)$.\n-   $P(t,\\mathbf Z,\\mathbf Y)$ is the randomization $p$-value of $t(\\mathbf Z, \\mathbf Y)$ in a Fisher Randomization Test.\n\nFisher's \"sharp\" null hypothesis is an equality *between random variables*, the potential outcomes. Typical examples for the distribution $\\text{Pr}_\\mathbf Z(\\cdot)$ are:\n\n-   Completely Randomized Experiment (CRE):\n\n$$\n\\text{Pr}_\\mathbf Z (\\mathbf Z) = \\begin{cases}\n\\binom N {n_1} ^{-1} & \\sum _{i=1}^N Z_i =n_1, \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n$$\n\n-   Bernoulli Randomized Experiment (BRE):\n\n$$\n\\text{Pr}_\\mathbf Z (\\mathbf Z) = \\prod _{i=1} ^N \\pi^{Z_i}(1-\\pi)^{1-Z_i}.\n$$\n\nAn example of test statistic is the difference in means between the treatment and control group, that can be written:\n\n$$\nt(\\mathbf Z , \\mathbf Y) = \\sum_i c_i Y_i,\\qquad c_i=\\frac{Z_i}{\\sum _iZ_i} - \\frac{1-Z_i}{\\sum _i(1-Z_i)}.\n$$\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}