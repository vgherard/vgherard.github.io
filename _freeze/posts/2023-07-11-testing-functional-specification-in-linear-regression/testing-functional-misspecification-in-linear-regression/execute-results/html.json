{
  "hash": "92fba0590c2cf0d6056b274f692b1d59",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Testing functional specification in linear regression\"\ndescription: |\n  Some options in R, using the `{lmtest}` package.\ndate: 2023-07-11\ncategories: \n  - Statistics\n  - Model Misspecification\n  - Regression\n  - Linear Models\n  - R\ndraft: false\n---\n\n\n\nAnother one from the series on \"misspecified regression models\" (started with [Model Misspecification and Linear Sandwiches](https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches/)).\n\n## Intro\n\nLately I've been messing around with the [`{lmtest}`](https://cran.r-project.org/web/packages/lmtest/index.html) R package, a nice collection of hypothesis tests for classical linear model assumptions: *linearity* (of course) and *heteroskedasticity* ($X$-independence of the conditional variance).\n\nJust to clarify, here the relevant \"linearity\" assumption is that the conditional mean $\\mathbb E (Y\\vert X)$ is given by a linear combination of *known functions* $f_i$ of $X$:\n\n$$\n\\mathbb E (Y\\vert X) = \\sum _{i = 1}^p \\alpha_if_i(X),\n$$ Testing \"linearity\" (or, as the title goes, \"functional specification\") refers to testing that the chosen set of functions $\\{f_{i}\\}_{i=1,\\dots,p}$ provide a valid description of the data generating process.\n\n## First attempt: residual autocorrelation\n\nMy initial intuition was that it should be possible to test functional specification through the following procedure:\n\n-   Perform linear regression with the specified functional form.\n-   Order the residuals according to the corresponding values of $X$[^1].\n-   Test for serial correlation (e.g. performing a Durbin-Watson test, `lmtest::dwtest`) on the series of ordered residuals.\n\n[^1]: Here I'm implicitly assuming that we have a single $X$, but a similar logic should also apply to multivariate regression.\n\nThe idea is quite simple: if residuals exhibit some systematic pattern when plotted against $X$, then for close values of $X$, residuals should also tend to be close, leading to a positive correlation. For example:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(840)\nx <- rnorm(1e2)\ny <- x^3 + rnorm(length(x))\nplot(x, y)\nabline(lm(y ~ x))\n```\n\n::: {.cell-output-display}\n![](testing-functional-misspecification-in-linear-regression_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\nThis, I suspect, is the reason why functions such as `lmtest::dwtest()` have an `order.by` argument which precisely allows to sort residuals before performing the test.\n\nUnfortunately, it turns out that such a method is not only sensitive to functional misspecification, but also to heteroskedasticity - as one can quickly verify by running a simulation using `lmtest::dwtest()`.\n\nThe overall idea is interesting, and works for homoskedastic noise, but the limitation to constant variance may be a bit too stringent. For this reason I turned to a second method, which also allows to take into account the possibility of heteroskedastic noise.\n\n## Second attempt: RESET + Heteroskedastic Consistent variance estimates\n\nThe idea of RESET tests (see `?lmtest::resettest()`) is also quite simple: if the linear model is correct, there should be relatively little gain in adding additional non-linear functions of the original covariates to the fit's formula.\n\nThe statistical significance of these model adjustments can be tested through a standard $Z$-test (or $F$-test, for multiple adjustments at once), with an important catch: the covariance matrix of regression coefficients used in these tests can be chosen to be robust to heteroskedasticity (see [Model Misspecification and Linear Sandwiches](https://vgherard.github.io/posts/2023-05-14-model-misspecification-and-linear-sandwiches/)).\n\nThe code that follows illustrates this procedure with an example dataset. The following section contains a more in-depth simulation study of the property of the RESET test.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_cars <- lm(dist ~ speed, data = cars)\nwith(data = cars, plot(speed, dist))\nabline(fit_cars)\n```\n\n::: {.cell-output-display}\n![](testing-functional-misspecification-in-linear-regression_files/figure-html/fit_cars-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlmtest::resettest(fit_cars, \n\t\t\t\t\t\t\t\t\ttype = \"regressor\", \n\t\t\t\t\t\t\t\t\tpower = 2,\n\t\t\t\t\t\t\t\t\tvcov = sandwich::vcovHC\n\t\t\t\t\t\t\t\t\t)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tRESET test\n\ndata:  fit_cars\nRESET = 2.32, df1 = 1, df2 = 48, p-value = 0.1344\n```\n\n\n:::\n:::\n\n\n\nUnfortunately, the output of `lmtest::resettest` does not include the results of the extended fit, which can be useful to understand the *impact* of the omitted covariates on the overall model picture (independently of the RESET $p$-value under the null hypothesis). [^2]\n\n[^2]: With enough data, the RESET test would likely test positive for a variety of misspecifications, but that doesn't mean that such misspecification are necessarily relevant from a modeling perspective. Here, for instance, a large coefficient for $\\text{(speed)}^2$ with a $Z$-score of two $\\sigma$s could be more worrying than a minuscule coefficient with a $Z$-score of five $\\sigma$s.\n\nIn order to get some insight on the effect of misspecification, we need to manually perform the RESET fit and make the relevant comparisons:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_cars_sq <- lm(dist ~ speed + I(speed*speed), data = cars)\nwith(data = cars, plot(speed, dist))\nabline(fit_cars)\nlines(x = cars$speed, y = fitted(fit_cars_sq), col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](testing-functional-misspecification-in-linear-regression_files/figure-html/fit_cars_sq-1.png){width=672}\n:::\n:::\n\n\n\n## RESET + HC vcov: a simulation study\n\nWe consider a univariate regression problem, with a regressor $X \\sim \\mathcal N (0,1)$, a and a response $Y$. We will consider three ground truth distributions for $Y$ given $X$:\n\n$$\n\\begin{split}\n\\text{T1}:& \\qquad Y=\\frac{1}{5}X+Z\\\\\n\\text{T2}:& \\qquad Y=\\frac{1}{5}X + \\vert X \\vert Z\\\\\n\\text{T3}:& \\qquad Y=\\frac{1}{5}X^3 + Z\n\\end{split}\n$$ where $Z\\sim \\mathcal N (0,1)$ is independent from $X$. We will study, through simulation, the $p$-value distribution of the RESET test for linear regression based on the model $Y = q+m X + \\varepsilon$, where $q$ and $m$ are unknown coefficients, and $\\epsilon$ is a noise term with unknown variance. It follows that the model is correctly specified with respect to $\\text{T1}$, has functional misspecification with respect to $\\text{T3}$, and potentially noise misspecification[^3] with respect to $\\text{T2}$, if we model variance as being independent of $X$.\n\n[^3]: Sometimes also referred to as \"second order misspecification\".\n\nData will consist of independent samples $(X_i, Y_i)$ from the joint distribution of $X$ and $Y$. To facilitate simulation, we define some helpers in the code chunk below.\n\n\n\n::: {.cell code_folding='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\n#' Helper to generate data with prescribed: \n#' * Regressor distribution: `x`\n#' * Response conditional mean: `f`\n#' * Response conditional noise: `eps` \ndgp_fun <- function(x, f, eps) {\n\tfunction(n) {\n\t\t.x <- x(n)\n\t\tdata.frame(x = .x, y = f(.x) + eps(.x))\n\t}\n}\n\n#' Helper to simulate results of linear regression, with prescribed:\n#' * Data generating process: `dgp`\n#' * Sample size of simulated datasets: `n`\n#' * Summary function (e.g. p-value of RESET test): `summarize_fun`\nlm_simulate <- function(dgp, n, summarize_fun, nsim, simplify) {\n\treplicate(nsim, {\n\t\tdata <- dgp(n)\n\t\tfit <- lm(y ~ x, data)\n\t\tsummarize_fun(fit)\n\t}, simplify = simplify)\n} \n\n#' Helper to perform RESET test on a `lm` fit object, and plot the p-value\n#' distribution. The estimator for regression coefficients variance-covariance\n#' matrix can be set through the `vcov` argument.\nreset_pvalue <- function(\n\t\tdgp, n,  # Data generating process params\n\t\tpower = 2:3, type = \"regressor\", vcov = sandwich::vcovHC,  # RESET params\n\t\tnsim = 1e3  # Simulation params\n\t\t) \n{\n\tsummarize_fun <- function(fit)\n\t\tlmtest::resettest(fit, power = power, type = type, vcov = vcov)$p.value\n\t\n\tp <- lm_simulate(\n\t\tdgp = dgp, \n\t\tn = n, \n\t\tsummarize_fun = summarize_fun, \n\t\tnsim = nsim,\n\t\tsimplify = TRUE\n\t\t)\n\t\n\treturn(data.frame(\n\t\tp = p,\n\t\tdgp = deparse(substitute(dgp)),\n\t\tn = n,\n\t\tvcov = deparse(substitute(vcov)),\n\t\tnsim = nsim\n\t))\n\t\n}\n```\n:::\n\n\n\nFurthermore, we will use:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\n```\n:::\n\n\n\nfor plotting.\n\n### Data generating processes\n\nThe data generating processes can be defined as follows:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndgp_t1 <- dgp_fun(\n\tx = rnorm,\n\tf = \\(x) 0.2 * x,\n\teps = \\(x) rnorm(length(x))\n)\n\ndgp_t2 <- dgp_fun(\n\tx = rnorm,\n\tf = \\(x) 0.2 * x,\n\teps = \\(x) abs(x) * rnorm(length(x))\n)\n\ndgp_t3 <- dgp_fun(\n\tx = rnorm,\n\tf = \\(x) 0.2 * x^3,\n\teps = \\(x) rnorm(length(x))\n)\n```\n:::\n\n\n\nData generated according to these three distributions looks as follows:\n\n\n\n::: {.cell code_folding='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nbind_rows(\n\ttibble(dgp_t1(100), dgp = \"dgp_t1\"),\n\ttibble(dgp_t2(100), dgp = \"dgp_t2\"),\n\ttibble(dgp_t3(100), dgp = \"dgp_t3\"),\n\t) |>\n\tggplot(aes(x = x, y = y)) +\n\t\tgeom_point() +\n\t\tgeom_smooth(method = \"lm\", formula = y ~ x, se = F) +\n\t\tfacet_grid(~ dgp)\n```\n\n::: {.cell-output-display}\n![](testing-functional-misspecification-in-linear-regression_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n### RESET $p$-value distributions\n\nThe RESET $p$-value cumulative distributions for the three ground truths $\\text{T1}$, $\\text{T2}$ and $\\text{T3}$ are shown below [^4]. The $y$ coordinates of these plots can be interpreted as follows:\n\n[^4]: The code is a bit unelegant 😬 but it works.\n\n-   For the ground truths $\\text{T1}$ and $\\text{T2}$, $y$ represents the false positive rate (or Type I Error Rate) in rejecting the null hypothesis \"no functional misspecification\" at a given size of the test $x$. For a valid $p$-value, these curves should lie on or below the straight line $y = x$.\n\n-   For the ground truth $\\text{T3}$, $y$ represents the Power (or one minus the Type II Error Rate) in detecting functional misspecification at a given size $x$. High values correspond to high sensitivity.\n\n\n\n::: {.cell code_folding='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nsim_data <- dplyr::bind_rows(\n\treset_pvalue(dgp = dgp_t1, n = 10, vcov = sandwich::vcovHC),\n\treset_pvalue(dgp = dgp_t1, n = 100, vcov = sandwich::vcovHC),\n\treset_pvalue(dgp = dgp_t1, n = 1000, vcov = sandwich::vcovHC),\n\treset_pvalue(dgp = dgp_t1, n = 10000, vcov = sandwich::vcovHC),\n\treset_pvalue(dgp = dgp_t1, n = 10, vcov = stats::vcov),\n\treset_pvalue(dgp = dgp_t1, n = 100, vcov = stats::vcov),\n\treset_pvalue(dgp = dgp_t1, n = 1000, vcov = stats::vcov),\n\treset_pvalue(dgp = dgp_t1, n = 10000, vcov = stats::vcov),\n\t\n\treset_pvalue(dgp = dgp_t2, n = 10, vcov = sandwich::vcovHC),\n\treset_pvalue(dgp = dgp_t2, n = 100, vcov = sandwich::vcovHC),\n\treset_pvalue(dgp = dgp_t2, n = 1000, vcov = sandwich::vcovHC),\n\treset_pvalue(dgp = dgp_t2, n = 10000, vcov = sandwich::vcovHC),\n\treset_pvalue(dgp = dgp_t2, n = 10, vcov = stats::vcov),\n\treset_pvalue(dgp = dgp_t2, n = 100, vcov = stats::vcov),\n\treset_pvalue(dgp = dgp_t2, n = 1000, vcov = stats::vcov),\n\treset_pvalue(dgp = dgp_t2, n = 10000, vcov = stats::vcov),\n\t\n\treset_pvalue(dgp = dgp_t3, n = 10, vcov = sandwich::vcovHC),\n\treset_pvalue(dgp = dgp_t3, n = 100, vcov = sandwich::vcovHC),\n\treset_pvalue(dgp = dgp_t3, n = 1000, vcov = sandwich::vcovHC),\n\treset_pvalue(dgp = dgp_t3, n = 10000, vcov = sandwich::vcovHC),\n\treset_pvalue(dgp = dgp_t3, n = 10, vcov = stats::vcov),\n\treset_pvalue(dgp = dgp_t3, n = 100, vcov = stats::vcov),\n\treset_pvalue(dgp = dgp_t3, n = 1000, vcov = stats::vcov),\n\treset_pvalue(dgp = dgp_t3, n = 10000, vcov = stats::vcov)\n)\n\nsim_data |>\n\tmutate(n_label = paste(\"n\", n, sep = \" = \")) |>\n\tggplot(aes(p, color = vcov)) + \n\t\tstat_ecdf() +\n\t\tscale_color_discrete(\"vcov\") + \n\t\tscale_x_continuous(\"p-value\", labels = scales::percent) + \n\t\tscale_y_continuous(\"Empirical CDF\", labels = scales::percent) +\n\t\tgeom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n\t\tfacet_grid(n_label ~ dgp, ) +\n\t\tggtitle(\n\t\t\t\"p-value distribution of RESET test\",\n\t\t\tpaste(\"nsim\", max(sim_data$nsim), sep = \" = \")\n\t\t\t)\n```\n\n::: {.cell-output-display}\n![](testing-functional-misspecification-in-linear-regression_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nThe plots illustrate qualitatively the behavior of the RESET test with and without the `vcov` correction for noise heteroskedasticity. Various remarks:\n\n1.  The test with the standard `stats::vcov` estimator is sensitive not only to pure functional misspecification ($\\text{T3}$), but also to pure heteroskedastic noise ($\\text{T2}$).\n\n2.  The `sandwich::vcovHC` estimator leads to an asymptotically correct Type I Error Rate in the $\\text{T2}$ case, but to a somewhat lower sensitivity (with respect to `stats::vcov`) in the $\\text{T3}$ case.\n\n3.  We need to keep in mind that `sandwich::vcovHC` only provides *asymptotically* correct variance-covariance estimates. Thus, for small $n$, the $p$-value distribution of the RESET test using the `sandwich::vcovHC` can also be distorted (even in the perfectly specified case $\\text{T1}$).\n\n## Conclusions\n\nThis post explained how to perform model validation checks that are sensitive to functional misspecification, but relatively robust to heteroskedasticity.\n\nThe general idea is to extend the original model, allowing for more general functional forms in the conditional mean of the response, and test whether such extension significantly improves the fit. The catch is that, when performing the latter test, we need to somehow keep into account the possibility of heteroskedastic noise.\n\nThis idea is readily implemented with RESET tests for linear models: one can simply use a variance-covariance estimator for regression coefficients that is robust to heteroskedasticity. In R, this can be achieved with a single line of code, using `lmtest::resettest(vcov = sandwich::vcovHC)`.\n\nWith some effort, one may be able to generalize such a procedure to any parametric model fitted by Maximum Likelihood Estimation, since a sandwich estimator is available also in this more general case (see *e.g.* the presentation of sandwich estimators in this [paper by D.A. Freedman](https://www.tandfonline.com/doi/abs/10.1198/000313006X152207)).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}