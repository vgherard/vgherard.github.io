{
  "hash": "03d072acdac0f583208437facc5623cb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"How to get away with selection. Part I: Introduction\"\ndescription: |\n  Introducing the problem of Selective Inference, illustrated through a simple simulation in R.\ndate: 2022-11-14\nbibliography: biblio.bib\ncategories:\n  - Statistics\n  - Selective Inference\n  - R\ndraft: false\n---\n\n\n\n\n\n# Prologue\n\nA few months back, for undocumented circumstances, my browser's search history was full of terms like *\"parameter estimation with variable selection\"*, or *\"confidence intervals after cross-validation\"*, or again *\"linear model uncertainties after staring into the abyss\"*, ...\n\nSparing you my rock bottom, I eventually stumbled upon the right keywords, and started digging into the mathematical aspects of **Selective Inference**, or **Post-Model Selection Inference**. Now, while my hands are still full of dirt, I've decided it's the right moment to write some notes about what I've learned - whose main recipient is the future me, which will otherwise inevitably forget what the present me thinks he knows. If you're *not* the future me:\n\n1.  Welcome ðŸ‘‹\n2.  If you have detected some imprecision, or have suggestions for this or the next posts, you are more than welcome to [create an issue](https://github.com/vgherard/vgherard.github.io/issues/new) on the source repository of this blog.\n\n# Introduction\n\nBroadly speaking, the problem of Selective Inference is that of performing valid statistical inferences when *the actual questions of the data analysis are not fixed in advance*, but rather *selected through data examination*. In model-based inference, this lack of pre-determination usually stems from the (often unavoidable) practice of using the same data to choose an adequate model for the data generating process *and* to perform inference. The intrinsic randomness of the selection process has important consequences on the probability of making different guesses about the selected questions, which, if not properly taken into account, can completely invalidate the analysis results.\n\nIf this sounds unfamiliar, think about machine-learning: when training a predictive model on a given dataset, you would usually consider the error on the same dataset as a poor (optimistic) estimate of the true model's error rate, because the model was tuned to perform well on that data in the first place. There we go, Selective Inference! A selection from an extended family of models[^1] is performed through data examination, and this event introduces a bias in the error estimate of the final model from training data.\n\n[^1]: Here, in the \"extended family of models\", I'm also implicitly accounting for the multiplicity introduced by continuous model parameters and training parameters (also known as hyper-parameters).\n\nThe example from machine-learning also suggests a very simple-minded and relatively a-theoretical approach to Selective Inference: data-splitting[^2]. According to this method, we would use only part of the available data to select the questions to be answered by the analysis, while the remaining part would be reserved to perform the actual inference. For this program to succeed, there are however two important requirements: first, we must have enough data to ensure decent statistics for both the selection and inference tasks; and second, we must be able to split data in two independent (or close to independent) sets. This can suppose problems with, *e.g.*, time-series data. If, on the other hand, these requirements cannot be met, we have to resort to more sophisticated methods.\n\n[^2]: The preferential method according to [@Shalizi:2020pmsi], from which I borrowed the \"a-theoretical\" description, and which I recommend as a starting point for literature review.\n\nAt this point, I would like to stress that the conceptual problems I've just pointed out will probably look obvious to any reader with a decent intuition for probability[^3]. What is less obvious, but in fact a [fairly active research field in statistics](https://arxiv.org/search/?query=%22selective+inference%22&searchtype=all&abstracts=show&order=-announced_date_first&size=50), is how to perform valid selective inferences when the \"easy\" solution of data-splitting I mentioned above is not available. This is where theory re-enters the game, and what I'm going to ramble about in this and the next posts.\n\n[^3]: This is not to say that correctly accounting for Selective Inference is the default in scientific practice. A relevant example from the field I come from (Particle Physics), is documented in this stimulating reference: [@isidori2021significance].\n\n# Illustrations of Selective Inference\n\nEnough for the speech, let us see how selection can affect (and invalidate) classical inference with a simple-minded simulation.\n\n### Setting\n\nTo illustrate why naive classical inference can fail in the presence of selection, we consider a very simple regression problem involving a single regressor $X$ and a response $Y$, where *all the assumptions of the classical linear model hold*. In fact, we will assume the true data generating process to be:\n\n$$\nY = mX + \\varepsilon, \\qquad \\varepsilon \\sim \\mathcal N (0, \\sigma),\n$$ {#eq-y-gaussian}\n\nwhere $\\varepsilon \\sim \\mathcal N (0, \\sigma)$ means \"$\\varepsilon$ follows a gaussian distribution with mean $0$ and standard deviation $\\sigma$\".\n\n### A selective modeling procedure\n\nNow, suppose we are given a dataset of $N$ independent observations $(y_i, x_i)_{i = 1, \\,2,\\, \\dots,\\,N}$, and we would like to study the dependence of $Y$ from $X$. Of course we don't know the true law, @eq-y-gaussian, but by a stroke of luck (or by a Taylor expansion argument) we make the correct initial guess that such dependence is linear in $X$. We are, however, unsure whether it would be appropriate to also include an intercept term in the fit. We thus establish the following selective modeling procedure:\n\n-   Fit a linear model with intercept term, $Y = mX + q + \\varepsilon$.\n-   Stop if the intercept estimate is significantly different from zero (say, at the level of 1-$\\sigma$, $p\\text{-value}<32\\%$). Otherwise:\n-   Fit a model with no intercept, $Y = mX + \\varepsilon$.\n\nFinally, we *use the last fitted model* to construct a \"naive 95%\" confidence interval $(\\hat m_-, \\hat m_+)$ for the slope $m$. This is defined by:\n\n$$\n\\hat m_\\pm = \\hat m\\pm t_{0.975, \\,N-d} \\cdot \\hat \\sigma _\\hat m\\qquad (95\\%\\,\\text {C.L.}).\n$$ {#eq-naive-ci-slope}\n\nHere $t_{0.975,\\, N-d}$ is the 97.5%-quantile of the $t$-student distribution with $N-d$ degrees of freedom, and $d$ is the number of estimated parameters, ($2$ or $1$, according to where we stopped in the modeling procedure). $\\hat m$ and $\\hat \\sigma _{\\hat m}$ are the Ordinary Least Squares (OLS) estimates of the slope and its standard deviation, respectively. These are the classical confidence intervals reported by the `lm()` function in R.\n\nAt a first glance, this procedure might look reasonable. After all, both intervals we may end up constructing do have a genuine 95% coverage probability, when constructed unconditionally... and by selecting the \"best\" model we're supposedly choosing the \"best\" confidence interval. In spite of this qualitative argument, we inquire:\n\n### ... does it work?\n\nNow, the question is: how often do the naive CIs @eq-naive-ci-slope cover the true parameter $m$ of @eq-y-gaussian? The answer better be \"at least 95% of the times\" for our confidence claim in @eq-naive-ci-slope to be valid!\n\nWe can check the actual coverage of @eq-naive-ci-slope through a simulation. Here I'll assume $m = \\sigma = 1$, and that the dataset consists of $N=10$ independent observations of $Y$ at fixed points $X = (1, \\,2, \\,\\dots ,\\, 10)$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- sigma <- 1  # True parameters\nx <- 1:10  # x covariate, assumed fixed\n```\n:::\n\n\n\nThe following function generates observations of $Y$ according to the distribution @eq-y-gaussian:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_y <- function(x, m, sigma) {\n\teps <- rnorm(length(x), mean = 0, sd = sigma)\n\treturn(m * x + eps)\n\t}\n```\n:::\n\n\n\nFor example:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(840)\nplot(x, generate_y(x, m, sigma), xlab = \"X\", ylab = \"Y\")\n```\n\n::: {.cell-output-display}\n![](posi_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nBelow we generate $B=10^4$ such $(X,Y)$ datasets, for each of which we fit a linear model according to the procedure specified above, and check how many times the true slope $m = 1$ falls in the confidence interval defined by @eq-naive-ci-slope.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulation parameters\nB <- 1e4  # Number of replications\n\n# Preallocate logical vectors to be assigned for each replica - for efficiency. \nq_dropped <- logical(B)  # Was the intercept term 'q' dropped? \nm_covered <- logical(B)  # Was the true parameter 'm' covered?\n\n# Set seed for reproducibility\nset.seed(841)\n\n# Logging\ntime_start <- Sys.time()  \n\n# Start the simulation\nfor (b in 1:B) {\n\ty <- generate_y(x, m, sigma)\n\t\n\t# Fit full model (including intercept 'q')\n\tfit <- lm(y ~ x + 1)  \n\tq_pval <- summary(fit)$coefficients[1, 4]\n\t\n\t# Is 'q' term \"significant\"? If not, drop 'q' and fit a simpler model\n\tif (q_pval > 0.32)  { \n\t\tq_dropped[[b]] <- TRUE\n\t\tfit <- lm(y ~ x - 1) \n\t} else {\n\t\tq_dropped[[b]] <- FALSE\n\t}\n\t\n\t# Construct CI for 'm',\tusing the selected model's fit\n\tm_ci <- confint(fit, 'x', level = 0.95)\n\tm_covered[[b]] <- m_ci[[1]] < m && m < m_ci[[2]]\n}\n\ntime_end <- Sys.time()\ncat(\"Done :) Took \", as.numeric(time_end - time_start), \" seconds.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDone :) Took  10.90042  seconds.\n```\n\n\n:::\n:::\n\n\n\nThe variable `m_covered[[b]]` is `TRUE` if the slope $m$ fell in the naive CI $(m_-, m_+)$ defined by @eq-naive-ci-slope in the `b`-th replica of the simulation. Hence, the actual coverage fraction of the CI is given by:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(m_covered)  # Actual coverage of naive \"95%\" CIs.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9172\n```\n\n\n:::\n:::\n\n\n\n92%! If this difference from the nominal 95% coverage guarantee does not strike you as enormous, think about it in these terms: the naive CIs @eq-naive-ci-slope fail to cover the true parameter about 8% of the times; This is a relative +60% of failures with respect to an honest 95% CI.\n\n### What's going on\n\nWe can understand a bit better what's happening here by decomposing the coverage probability as follows:\n\n$$\n\\text {Pr}(m \\in \\text{CI}) = \n\t\\text {Pr}(m \\in \\text{CI}_{q \\text{ dropped}}\\,\\vert\\,q \\text{ dropped})\\cdot \\text {Pr}(q \\text{ dropped}) +\\newline\n\t+\\text {Pr}(m \\in \\text{CI}_{q  \\text{ kept}}\\,\\vert\\,q \\text{ kept})\\cdot \\text {Pr}(q \\text{ kept})\n$$ {#eq-coverage-decomposition}\n\n\nThe right hand side of this equation shows how our selective modeling procedure alters the probability $\\text{Pr}(m\\in \\text{CI})$. There are two contributing factors here: the probability of dropping the intercept term, and the covering probabilities of the CIs constructed in the two cases ($\\text{CI}_{q \\text{ dropped}}$ and $\\text{CI}_{q \\text{ kept}}$). We can estimate all these probabilities as:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(q_dropped)  # Pr(q dropped)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6782\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(m_covered[q_dropped])  # Pr(m covered | q dropped)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9510469\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(m_covered[!q_dropped])  # Pr(m covered | q kept)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.845867\n```\n\n\n:::\n:::\n\n\n\nThe first result directly follows from our procedure, which uses a hypothesis test with significance $\\alpha = 32\\%$ to test the (true) null hypothesis $q = 0$. It is a bit harder but in fact possible to prove that[^4] $\\text {Pr}(m \\in \\text{CI}_{q \\text{ dropped}}\\,\\vert\\,q \\text{ dropped}) = 95\\%$, as the second estimate would seem to suggest. The third result is finally what invalidates the naive coverage guarantee in @eq-naive-ci-slope.\n\n[^4]: I'm always amazed by the great deal of theory one can learn by running a dumb simulation, and trying to explain a posteriori what seems to be a too perfect result. Technically, this follows from the fact that the slope estimate $\\hat m$ and residual sum of squares $\\text{RSS}$ of the reduced model, and the $F$-statistic used to test $q = 0$, are all independent random variables under the same null hypothesis, here true by construction. All these facts are in turn consequences of general theorems from linear model theory, see for example [@vrbik:2020ra Chapter 4]... and, to be sure, it took me more than a single night without sleep to figure all this out.\n\n### Concluding Remarks\n\nTo summarize:\n\n1.  We started with two linear models for $Y$ vs. $X$, which were in fact *both well-specified* (that is, correct).\n2.  We stipulated to choose one of the two models by testing the null hypothesis $q = 0$.\n3.  After selection, we constructed $95\\%$ confidence intervals for the slope $\\hat m$ using the selected model, *as if this had been fixed in advance*.\n4.  A simulation shows that such intervals have a true coverage probability of $\\approx 92\\%$.\n\nThe mathematical explanation of the last result is provided by @eq-coverage-decomposition, while the (hopefully) plain English one in the introductory part of this post. I will conclude with a few parenthetical remarks.\n\nFirst, the selective procedure proposed here would likely hardly be applied in practice in such a simple situation[^5]. However, one could easily think of a more complex scenario with multiple covariates, where eliminating redundant ones could turn out to be beneficial for interpretation (if not compulsory, if the number of covariates exceeds the sample size).\n\n[^5]: And I'm actually not sure that, after properly taking into account Selective Inference, it would lead to a substantial gain in estimation accuracy, compared to simply fitting the possibly redundant model with intercept.\n\nSecond, in order to avoid cluttering the discussion with too much technicalities, I have deliberately chosen a quite special point in true-model space ($q = 0$). This implies that both fits with and without intercept estimate the *same* slope $m$; this is a peculiar property of $q = 0$, which would not be true in the general case $q \\in \\mathbb R$. In general, we would have to carefully define the inferential targets for the $q=0$ and $q \\in \\mathbb R$ cases, in a differential manner.\n\n# Conclusion\n\nThat was all for today. In the next post, I will discuss some mathematical details regarding the formulation of the Selective Inference problem in model-building. For those surviving down to the bottom of the funnel, my future plan is to review some (valid) selective inference methods I found interesting, including:\n\n-   Benjamini-Yekutieli control of False Coverage Rate [@benjamini2005false],\n-   POSI bounds for marginal coverage [@berk2013valid],\n-   Data Fission, an elegant generalization of good old data splitting [@https://doi.org/10.48550/arxiv.2112.11079].\n-   ...whatever cool stuff I may discover in the meantime.\n\nCiao!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}