{
  "hash": "b863d9a4272a67a5bceee8881d5fe937",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"AB tests and repeated checks\"\ndescription: |\n  False Positive Rates under repeated checks - a simulation study using R.\ndate: 2023-07-27\ncategories: \n  - AB testing\n  - Sequential Hypothesis Testing\n  - Frequentist Methods\n  - Statistics\n  - R\ndraft: false\n---\n\n\n\n## Intro\n\n> \"How is the experiment going?\"\n\nAlso:\n\n> \"Do we already see something?\"\n\nAnd my favorite one:\n\n> \"Did we already hit significance, or do we need more data?\"\n\nIf you have dealt with experiments with relatively high outcome expectations, you will likely have received (or perhaps asked yourself) similar questions from time to time.\n\nIn many data analysis contexts, including but not limited to for-profit ones, researchers are always trying to come up with positive results as fast as they can. Therefore, it is not at all surprising to see questions such as the ones above regularly arise during the course of an experiment. This is natural and not a problem *per se*. What I want to highlight and quantify in this post is how, if not done carefully, such \"real-time\" monitoring schedules can seriously invalidate data analysis - by inflating false positive and false negative rates.\n\nGenerally speaking, repeated and ad-hoc checks lead to problems of selective/simultaneous inference (a topic which I have touched in [other places in this blog](https://vgherard.github.io/#category:Selective_Inference)). Avoiding them is not the only valid solution - if you want to learn about some proper method you may give a look into [Sequential Hypothesis Testing](https://en.wikipedia.org/wiki/Sequential_analysis), a topic that I may explore in future posts. Here my goal is to understand the *consequences of naive repeated checking*, which can be easily found out through simulation.\n\n## What's the matter with repeated checks?\n\nTo understand why problems can arise, recall that the classical Frequentist framework [^1] operates by providing *a priori* guarantees (bounds) on the probabilities of [^2]:\n\n[^1]: I move within this framework because it is the only one I'm reasonably comfortable with, and for which I have a decent understanding of the decision dynamics that follow from it. That said, I suspect that also Bayesian hypothesis testing can be affected by the kind of issues discussed here, although perhaps in a less transparent way, due to working with formal a posteriori probabilities.\n\n[^2]: The statistical jargon used to indicate these two types of errors, and the corresponding a priori guarantees on their probabilities, sounds very mysterious to me (Type I/II errors, size and power...). I like to think in terms of \"False Positive\" and \"False Negative\" rates, which is the same thing.\n\n-   A false positive outcome in the absence of any signal: rejecting the null hypothesis when this is actually true.\n-   A false negative outcome in the presence of some (well-defined) signal.\n\nThe *a priori* nature of these guarantees means that they are stipulated before running the experiment and assuming a certain experimental *schedule* [^3]. This implies that any departure from the original schedule can in principle invalidate the claimed False Positive Rate (FPR) and False Negative Rate (FNR).\n\n[^3]: This is generally true, also in the aforementioned sequential settings. In that case, the difference is that the schedule takes into account that continuous and/or interim checks will be performed.\n\nFor instance, the most basic experimental schedule (actually the one implicitly assumed by virtually all [sample size calculators](https://www.google.com/search?q=sample+size+calculator) ) is:\n\n1.  Collect data until reaching a prefixed sample size.\n2.  Run an hypothesis test (with a prefixed significance threshold for claiming a signal).\n\nCommon examples of departures from the original schedule include:\n\n-   Running several tests on partial data (before reaching the established sample size), to look for an early signal.\n-   Stopping the experiment beforehand, because partial data doesn't show any signal.\n-   Prolonging the experiment after reaching the established sample size, because there's a \"hint\" to a signal, but the significance threshold was not reached.\n\nIn what follows, I will focus on the first behavior, whose result is to inflate the FPR. Again, there are various ways to perform repeated checks while keeping the FPR under control, but that's not the focus of this post. Instead, I want to understand how FPR is affected when *the same test is repeated several times* on partial data.\n\n## Example\n\nLet me illustrate the idea with an imaginary marketing experiment. Suppose you are optimizing an advertising campaign, say you want to test whether a new ad design performs better than the existing one in terms of click through rates. You start sending batches of two thousands ads[^4] to randomized users, half using the new design and half using the old one.\n\n[^4]: The actual numbers in this example may be totally unrealistic, but that's beside the point.\n\nIf the new design does actually perform better, you want to fully switch to it as soon as possible, so that after each batch send, you compare the click through rates of all ads sent so far, with the idea of switching *as soon as a statistically significant improvement is observed*.\n\nConcretely, you propose to do the following:\n\n1.  At each step, calculate the click through rates for the new and old designs.\n2.  Compute a $p$-value for the hypothesis test[^5] that tests whether the new design leads to an higher click through rate than the old one.\n3.  If the $p$-value is smaller than a certain fixed threshold $\\alpha$, stop the experiment and declare the new design as the winner.\n4.  If no $p$-value smaller than $\\alpha$ is observed after a certain number $n$ of iterations, stop the experiment and declare the old design as the winner.\n\n[^5]: Technically, this would be a two-sample, one-sided binomial test.\n\nNow, the question is: how often would the above procedure declare the new design as the winner, if it doesn't truly perform better than the old one? (*i.e.* what is the FPR of the whole procedure?)\n\n## Simulation\n\nTo compute the FPR, we assume that both the new and old designs have in fact the *same* click through rate $p = 10 \\%$. The following function generates a sequence of $n$ consecutive $p$-values, computed as described above, that one could observe under these circumstances:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_p_values <- function(n = 28,      # maximum number of iterations\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsize = 1e3,  # ad sends per batch\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tp = 0.1      # true common click through rate\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t) \n\t{\n\tsuccesses_a <- cumsum( rbinom(n = n, size = size, prob = p) )  # clicks old ad\n\tsuccesses_b <- cumsum( rbinom(n = n, size = size, prob = p) )  # clicks new ad\n\t\n\tsapply(1:n, \\(k) {\n\t\tprop.test(\n\t\t\tx = c(successes_a[k], successes_b[k]), \n\t\t\tn = k * size * c(1, 1), \n\t\t\talternative = \"greater\",\n\t\t\t)$p.value\n\t})\n}\n```\n:::\n\n\n\nFor instance:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(999)\n( p_example <- generate_p_values(n = 5) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4704229 0.3932333 0.1669308 0.2219066 0.2592812\n```\n\n\n:::\n:::\n\n\n\nThe function below evaluates such a sequence of $p$-values with a fixed threshold $\\alpha$:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nevaluate_p_values <- function(p, alpha = 0.05, checkpoints = seq_along(p)) {\n\tp <- p[checkpoints]\n\tas.logical(cumsum(p < alpha))\n}\n```\n:::\n\n\n\nFor instance, with $\\alpha = 20\\%$, the sequence above would lead to a (false) positive result, which would be claimed at the third check. Output looks as follows:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nevaluate_p_values(p_example, alpha = 0.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE FALSE  TRUE  TRUE  TRUE\n```\n\n\n:::\n:::\n\n\n\nLet me now simulate a large number of such \"experiments\". I will fix $\\alpha = 5\\%$, a popular choice:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(840)\nsim_data <- replicate(1e4, generate_p_values(n = 100) |> evaluate_p_values())\n```\n:::\n\n\n\nThe result is a matrix whose columns are logical vectors such as the one above:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_data[,1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE\n```\n\n\n:::\n:::\n\n\n\n(a true negative result). Hence, the averages of this matrix rows provide the false positive rates after $n$ checks:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfpr <- rowMeans(sim_data)\nplot(fpr, type = \"l\", xlab = \"Checks\", ylab = \"False Positive Rate\")\nabline(h = 0.05, col = \"red\", lty = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](ab-tests-and-repeated-checks_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nThe curve above shows how the FPR depends on the number of checks performed, according to the procedure described in the previous section. For a single check, this coincides with FPR of an individual binomial test[^6]. However, allowing for repeated checks, we see that the overall FPR steadily increases with number of checks. With $n = 3$ checks, the FPR is already close to $10 \\%$, almost twice the nominal FPR of each individual test:\n\n[^6]: The fact that this is not exactly equal to $\\alpha$, but in fact slightly smaller, is due to the discreteness of the underlying binomial distributions. The $p$-value of the binomial test is defined in such a way to satisfy $\\text{Pr}(p < \\alpha)\\leq \\alpha$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfpr[3]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0929\n```\n\n\n:::\n:::\n\n\n\nWith $n \\approx 40$ checks, the FPR is about $25 \\%$, the same FPR of an experiment that involves tossing a coin twice, declaring it biased if the result is two consecutive \"tails\".\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfpr[40]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2471\n```\n\n\n:::\n:::\n\n\n\nHere we are assuming that data is re-checked after the arrival of every single batch, but there are of course infinite alternative possibilities. For instance, the plot below shows what happens when checks are performed after the collection of $n = 1, \\,4, \\,16, \\,64$ batches of data (at each checkpoint, the expected size of statistical fluctuations is reduced by a factor of $2$).\n\n\n\n::: {.cell code_folding='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\ncheckpoints <- c(1, 4, 16, 64)\n\nset.seed(840)\nfpr2 <- replicate(1e4, \n\t\t\t\t\tgenerate_p_values(n = 64) |> \n\t\t\t\t\t\tevaluate_p_values(checkpoints = checkpoints)\n\t\t\t\t\t) |>\n\trowMeans()\n\nplot(fpr2, \n\t\t type = \"b\", \n\t\t xlab = \"Checks\", \n\t\t ylab = \"False Positive Rate\", \n\t\t xaxt = \"n\"\n\t\t )\n\nabline(h = 0.05, col = \"red\", lty = \"dashed\")\naxis(1, at = seq_along(checkpoints))\naxis(3, at = seq_along(checkpoints), labels = paste(checkpoints, \"K\"))\nmtext(\"Sample Size\", side = 3, line = 2)\n```\n\n::: {.cell-output-display}\n![](ab-tests-and-repeated-checks_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nAs a third possible variation, we may think of applying different $p$-value thresholds at different checks (a scheme that can be actually made to work in practice, see for instance the Wikipedia article on the [Haybittleâ€“Peto boundary](https://en.wikipedia.org/wiki/Haybittle%E2%80%93Peto_boundary)). The following plot illustrates this, assuming three (equally spaced) checks after the collection of $n = 1,\\,2,\\,3$ data batches, using the significance thresholds $\\alpha = 0.01, \\,0.025, \\,0.05$, respectively.\n\n\n\n::: {.cell code_folding='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(840)\n\nalpha <- c(0.01, 0.025, 0.05)\n\nfpr3 <- replicate(1e5, \n\t\t\t\t\tgenerate_p_values(n = 3) |> \n\t\t\t\t\t\tevaluate_p_values(alpha = alpha)\n\t\t\t\t\t) |>\n\trowMeans()\n\nplot(fpr3, \n\t\t type = \"b\", \n\t\t xlab = \"Checks\", \n\t\t ylab = \"False Positive Rate\", \n\t\t xaxt = \"n\"\n\t\t )\n\nabline(h = alpha[3], col = \"red\", lty = \"dashed\")\nabline(h = alpha[2], col = \"blue\", lty = \"dashed\")\naxis(1, at = seq_along(fpr3))\naxis(3, at = seq_along(fpr3), labels = alpha)\nmtext(\"p-value threshold\", side = 3, line = 2)\n```\n\n::: {.cell-output-display}\n![](ab-tests-and-repeated-checks_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n## Conclusions\n\nThis post illustrated quantitatively how the performance of repeated checks during the process of data collection can affect the overall False Positive Rate of an experimental analysis. The code provided above can be easily adapted to simulate other types of experiments and schemes for interim checks.\n\nA question that may possibly arise is: *should I really care?* You could argue that what I've shown here represents a simple trade-off between FPR on one side, FNR and efficiency (speed) in detection of a signal on the other.\n\nMy answer is a resounding *yes*, irrespective of whether you are running experiments for purely scientific or utilitaristic purposes. If you are unable to characterize (at least approximately) the FPR and FNR of your analysis, the whole point of running a formal test looks very dubious to me. You may as well simply collect some data and draw an educated guess.\n\nOther story is if you are able to tell *in advance* how interim checks affect FPR/FNR, and use this knowledge to optimize your analysis strategy. This note provides some clues on how to do so.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}