{
  "hash": "f8e73239b50e163c8673e46f68d32f4c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Grammar as a biometric for Authorship Verification\"\ndescription: |\n  Notes on preprint 2403.08462 by A. Nini, O. Halvani, L. Graner, S. Ishihara \n  and myself.\ndate: 2024-04-25\ndate-modified: 2024-08-15\nbibliography: biblio.bib\ncategories: \n  - Authorship Verification\n  - Natural Language Processing\n  - Forensic Science\n  - Machine Learning\n  - Statistics\n  - R\ndraft: false\n---\n\n\n\n\nAbout a month ago we finally managed to drop [@nini2024authorship], *\"Authorship Verification based on the Likelihood Ratio of Grammar Models\"*, on the arXiv. Delving into topics such as authorship verification, grammar and forensics, was quite a detour for me, and I'd like to summarize here some of the ideas and learnings I got from working with all this new and interesting material.\n\nThe main qualitative idea put forward by Ref. [@nini2024authorship] is that *grammar is a fundamentally personal and unique trait of an individual*, therefore providing a sort of \"behavioural biometric\". One first goal of this work was to put this general principle under test, by applying it to the problem of Authorship Verification (AV): the process of validating whether a certain document was written by a claimed author. Concretely, we built an algorithm for AV that relies entirely on the grammatical features of the examined textual data, and compared it with the state-of-the-art methods for AV.\n\nThe results were very encouraging. In fact, our method actually turned out to be generally superior to the previous state-of-the-art on the benchmarks we examined. This is a notable result, keeping also into account that our method uses *less* textual information (only the grammar part) than other methods to perform its inferences.\n\n## The algorithm\n\nI sketch here a pseudo-implementation of our method in R. For the fit of $k$-gram models and perplexity computations, I use my package [`{kgrams}`](https://vgherard.github.io/kgrams/), which can be installed from CRAN. Model (hyper)parameters such as number of impostors, order of the $k$-gram models, etc. are hardcoded, see [@nini2024authorship] for details.\n\nThis is just for illustrating the essence of the method. For practical reasons, in the code chunk below I'm not reproducing the definition of the function `extract_grammar()`, which in our work is embodied by the POS-noise algorithm. This function should transform a regular sentence, such as \"He wrote a sentence\", to its underlying grammatical structure, say \"[Pronoun] [verb] a [noun]\".\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' @param q_doc character. Text document whose authorship is questioned.\n#' @param auth_corpus character. Text corpus of claimed author.\n#' @param imp_corpus character. Text corpus of impostors.\n#' @param n_imp a positive number. Number of \"impostor\" simulations.\n\nscore <- function(q_doc, auth_corpus, imp_corpus, n_imp = 100) \n{\n\tq_doc <- extract_grammar(q_doc)\n\tauth_corpus <- extract_grammar(auth_corpus)\n\timp_corpus <- extract_grammar(imp_corpus)\n\n\t# Compute perplexity based on claimed author's language model.\n\tauth_mod <- train_language_model(auth_corpus)\n  auth_perp <- kgrams::perplexity(q_doc, model = auth_mod)\n  \n  # Compute perplexity based on impostor language models.\n  #\n  # Each impostor is trained on a synthetic corpus obtained by sampling from\n  # the impostor corpus the same number of sentences as the corpus of the \n  # claimed author.\n  n_sents_auth <- length(kgrams::tknz_sent(auth_corpus))\n  imp_corpus_sentences <- kgrams::tknz_sent(imp_corpus)\n  imp_mod <- replicate(n_imp, {\n    sample(imp_corpus_sentences, n_sents_auth) |> train_language_model()\n  })\n  imp_perp <- sapply(imp_mod, \\(m) kgrams::perplexity(q_doc, model = m))\n  \n  # Score is the fraction of impostor models that perform worse (higher \n  # perplexity) than the proposed authors language model\n  score <- mean(auth_perp < imp_perp)\n  \n  return(score)\n}\n\ntrain_language_model <- function(text)\n{\n  text |> \n    kgrams::kgram_freqs(N = 10, .tknz_sent = kgrams::tknz_sent) |>\n    kgrams::language_model(smoother = \"kn\", D = 0.75)\n}\n\nextract_grammar <- identity  # Just a placeholder - see above.\n```\n:::\n\n\n\n\nTo be used as follows:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq_doc <- \"a a b a. b a. c b a. b a b. a.\" \nauth_corpus <- \"a a b a b. b c b. a b c a. b a. b c a.\" \nimp_corpus <- \"a a. b. a. b a. b a. c. a b a. d. a b. a d. a b a b c b a.\"\n\nset.seed(840)\nscore(q_doc, auth_corpus, imp_corpus)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.89\n```\n\n\n:::\n:::\n\n\n\n\nThe \"score\" computed by this algorithm turns out to be a good truthfulness predictor for the claimed authorship, higher scores being correlated with true attributions. *If* the impostor corpus is fixed once and for all, and *if* the pairs `q_doc` and `auth_corpus` are randomly sampled from a fixed joint distribution, we can set a threshold for score in such a way that the attribution criterion `score > threshold` maximizes some objective such as accuracy. This is, more or less, what we studied quantitatively in our paper.\n\n## Reflections on *in silico* Authorship Verification\n\nThe various *ifs* at the end of the previous sections led me to reflexionate on the applicability of machine-learning approaches, such as the one we discussed in our work, to real-life contexts.\n\nAs implied above, in order for a metric such as accuracy to represent a sensible measure of predictive performance, we should be able to regard the AV problems encountered in our favorite practical use case as random samples from some *fixed* population. In other words, we consider a stationary source of random authorship claims, and assume that our trained model is routinely used to verify claims from this source.\n\nNow, while there are many circumstances in which the above assumptions make total sense, I think there are also interesting AV applications in which one is not interested in the long-run properties of the method but, rather, in a single inference. The real case of the poem \"Shall I die?\", controversially attributed to Shakespeare in 1985, is an example of this kind. An approach to this case based on empirical Bayes is discussed in [@efron2021computer, ยง6.2]. Although we may be able to build a reasonable impostor corpus to be used with this problem, it is not clear how one should come up with a relevant testing dataset of AV problems to empirically quantify uncertainty.\n\nFor cases such as the \"Shall I die?\" controversy, the machine-learning setting considered in our study is just an *in silico* model of real AV. As such, I believe it still provides useful indications on what could be good authorship indicators and work in general, but we must acknowledge the practical limitations in our way to quantify uncertainty. Other approaches, such as classical null hypothesis testing, may be more suited to this specific kind of AV problems.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}