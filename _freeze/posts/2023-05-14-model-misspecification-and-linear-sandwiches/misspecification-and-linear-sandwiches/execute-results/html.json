{
  "hash": "3075fd9613644cca536316fd385c7d84",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model Misspecification and Linear Sandwiches\"\ndescription: |\n  Being wrong in the right way. With R excerpts.\ndate: 2023-05-14\nbibliography: biblio.bib\ncategories: \n  - Statistics\n  - Regression\n  - Linear Models\n  - Model Misspecification\n  - R\ndraft: FALSE\n---\n\n\n\n\n\n## Introduction\n\nTraditional linear models, such as the output of the R function `lm()`, are often loaded with a set of strong assumptions. Take univariate regression:\n\n$$\nY = q+mX+\\varepsilon.\n$$ {#eq-lm} \n\nThis equation assumes that:\n\n1.  The conditional mean $\\mathbb E(Y\\vert X) = q + mX$, a linear function of $X$.\n2.  The conditional variance $\\mathbb {V}(Y \\vert X)=\\mathbb{V}(\\varepsilon\\vert X)$ is independent of $X$.\n3.  The conditional distribution $Y\\vert X$ is gaussian.\n4.  In a set of measurements $\\left\\{\\left(X_i,Y_i\\right)\\right\\}_{i = 1,\\, \\dots, \\,N}$, $Y_i$ and the set $\\left\\{ X_j, Y_j\\right\\} _{j\\neq i}$ are conditionally independent of each other, given the value of the corresponding regressor $X_i$.[^1]\n\n[^1]: This is already somewhat implicit in @eq-lm, that models $Y$ and $X$ as single random variables. The reason for stating this condition in an apparently convoluted way, rather than a simpler *\"data points* $(X_i,Y_i)$ are independent draws from the same joint distribution\", is that this formulation includes cases where the $X_i$'s are *not* independent, *cf.* the following note.\n\nThe last assumption is satisfied in many practical situations, and we will take it here for granted[^2]. What happens when the first three assumptions are violated (that is \"frequently\" to \"almost always\", depending on context)?\n\n[^2]: There are of course important exceptions, like time series or spatial data. Noteworthy, our formulation of strict linear model assumptions can also cover some cases of temporal or spatial dependence in the regressors $X_i$, provided that such dependence is not reflected on $Y_i \\vert X_i$.\n\nA comprehensive discussion is provided by [@buja2019models]. These authors show that:\n\n-   If the conditional mean $\\mathbb E (Y \\vert X)$ is not linear (\"first order misspecification\"), then the Ordinary Least Squares (OLS) regression coefficients $\\hat \\beta$ consistently estimate: \n$$\n\\beta \\equiv \\text{arg } \\min _{\\beta^\\prime} \\mathbb E((Y-X\\beta^\\prime)^2)\n$$ {#eq-target}\n\nwhich can be thought as the \"best linear approximation of the response\"[^3].\n\n-   Both non-linearity in the sense of the previous point, and $X$-dependence in $\\mathbb{V}(Y \\vert X)$ (\"second order misspecification\") affect the sampling distribution of $\\hat \\beta$ and, in particular, $\\mathbb{V}(\\hat \\beta)$, which is the relevant quantity for inference in the large-sample limit.\n\n-   Both problems can be efficiently addressed through the so-called \"sandwich\" estimators for the covariance matrix of $\\hat \\beta$ [@white1980heteroskedasticity], whose consistency is robust to both type of misspecification.\n\n[^3]: According to an $L_2$ loss criterion.\n\nDetails can be found in the mentioned reference. The rest of the post illustrates with examples how to compute \"sandwich\" estimates in `R`, and why you may want to do so.\n\n## Fitting misspecified linear models in R\n\nThe [`{sandwich}`](https://CRAN.R-project.org/package=sandwich) package (available on CRAN) provides estimators for the regression coefficients' variance-covariance matrix $\\mathbb V (\\hat \\beta)$ that are robust to first and second order misspecification. These can be readily used with `lm` objects, as in the example below:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(mpg ~ wt, data = mtcars)\n\nstats::vcov(fit)  # standard vcov (linear model trusting estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            (Intercept)        wt\n(Intercept)    3.525484 -1.005693\nwt            -1.005693  0.312594\n```\n\n\n:::\n\n```{.r .cell-code}\nsandwich::vcovHC(fit)  # sandwich vcov (model-robust estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            (Intercept)         wt\n(Intercept)    5.889249 -1.7418581\nwt            -1.741858  0.5448011\n```\n\n\n:::\n:::\n\n\n\nIt is important to note that both functions `stats::vcov()` and `sandwich::vcovHC()` employ the same point estimates of regression coefficients\\\nto compute $\\mathbb V (\\hat \\beta)$:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n```\n\n\n:::\n:::\n\n\n\nThe difference between these functions lies in the different assumptions they make on the linear model residuals, which leads to different estimates for $\\mathbb{V}(\\hat \\beta)$.\n\n## Effects of misspecification\n\nThis section illustrates some consequences of model misspecification through simulation. The examples use:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\n```\n:::\n\n\n\nFor convenience, we define some helpers to be used in the following examples. The function below returns random generators for the generic additive error model $Y = f(X) + \\varepsilon$, where the distribution of the noise term $\\varepsilon$ may in general depend on $X$. Both $X$ and $Y$ are assumed here and below to be 1-dimensional.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrxy_fun <- function(rx, f, reps) {\n\tres <- function(n) {\n\t\tx <- rx(n)  # X has marginal distribution 'rx'\n\t\ty <- f(x) + reps(x)  # Y has conditional mean 'f(x)' and noise 'reps(x)'\n\t\treturn(tibble(x = x, y = y))  \n\t}\n\treturn(structure(res, class = \"rxy\"))\n}\n\nplot.rxy <- function(x, N = 1000, seed = 840) {\n\tset.seed(seed)\n\t\n\tggplot(data = x(N), aes(x = x, y = y)) +\n\t\tgeom_point(alpha = 0.3) + \n\t\tgeom_smooth(method = \"lm\", se = FALSE)\n}\n```\n:::\n\n\n\nThe following function simulates fitting the linear model `y ~ x` over multiple datasets generated according to a function `rxy()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmsim <- function(rxy, N = 100, vcov = stats::vcov, B = 1e3, seed = 840) \n{ \n\tset.seed(seed)\n\t\n\tres <- list(coef = matrix(nrow = B, ncol = 2), vcov = vector(\"list\", B))\n\tcolnames(res$coef) <- c(\"(Intercept)\", \"x\")\n\tclass(res) <- \"lmsim\"\n\t\t\t\t\t\t\t\t\n\tfor (b in 1:B) {\n\t\t.fit <- lm(y ~ ., data = rxy(N))\n\t\tres$coef[b, ] <- coef(.fit)  # Store intercept and slope in B x 2 matrix\n\t\tres$vcov[[b]] <- vcov(.fit)  # Store vcov estimates in length B list. \n\t}\n\t\n\treturn(res)\n}\n\nprint.lmsim <- function(x) \n{\n\tcat(\"Simulation results:\\n\\n\")\n\tcat(\"* Model-trusting vcov (average of vcov estimates):\\n\")\n\tprint( avg_est_vcov <- Reduce(\"+\", x$vcov) / length(x$vcov) )\n\tcat(\"\\n* Simulation-based vcov (vcov of coefficient estimates):\\n\")\n\tprint( emp_vcov <- cov(x$coef))\n\tcat(\"\\n* Ratio (1st / 2nd):\\n\")\n\tprint( avg_est_vcov / emp_vcov )\n\treturn(invisible(x))\n}\n```\n:::\n\n\n\nThe print method defined above shows a comparison of the covariance matrices obtained by:\n\nA)  Averaging variance-covariance estimates from the various simulations, and\nB)  Taking the variance-covariance matrix of regression coefficients obtained in the simulations.\n\nThe first one can be considered a \"model-trusting\" estimate (where the actual \"model\" is specified by the `vcov` argument of `lmsim()`, i.e. `stats::vcov` and `sandwich::vcovHC` for the traditional and sandwich estimates, respectively). The second one is a model-free simulation-based estimate of the true $\\mathbb{V}(\\hat \\beta)$. The comparison between the two[^4] provides a measure of the asymptotic bias of the model-trusting estimate.\n\n[^4]: I use an element-wise ratio, in order to avoid confusion from the different scales involved in the various entries of $\\mathbb V (\\hat \\beta)$.\n\n### Example 1: First order misspecification\n\n$$\nY = X ^ 2 + \\varepsilon,\\quad X \\sim \\text{Unif} (0,1),\\qquad \\varepsilon \\sim \\mathcal N (0,0.01) \n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrxy_01 <- rxy_fun(\n\trx = runif,\n\tf = \\(x) x^2,\n\treps = \\(x) rnorm(length(x), sd = .01)\n\t)\n```\n:::\n\n\n\nIn this model, $\\mathbb E (Y \\vert X)$ is not linear in $X$ (first order misspecification), but the remaining assumptions of the linear model hold. This is how a typical linear fit of data generated from this model looks like:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(rxy_01, N = 300)\n```\n\n::: {.cell-output-display}\n![](misspecification-and-linear-sandwiches_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nHere the effect of misspecification on the variance-covariance model trusting estimates is to underestimate true covariance values (by a factor as large as 40%!):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmsim(rxy_01)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n              (Intercept)             x\n(Intercept)  0.0002277348 -0.0003417356\nx           -0.0003417356  0.0006833282\n\n* Simulation-based vcov (vcov of coefficient estimates):\n              (Intercept)             x\n(Intercept)  0.0003367876 -0.0005662584\nx           -0.0005662584  0.0011488351\n\n* Ratio (1st / 2nd):\n            (Intercept)         x\n(Intercept)   0.6761971 0.6034976\nx             0.6034976 0.5948009\n```\n\n\n:::\n:::\n\n\n\nThis is fixed by the `sandwich::vcovHC()` estimators:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmsim(rxy_01, vcov = sandwich::vcovHC)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n              (Intercept)             x\n(Intercept)  0.0003475834 -0.0005732957\nx           -0.0005732957  0.0011443449\n\n* Simulation-based vcov (vcov of coefficient estimates):\n              (Intercept)             x\n(Intercept)  0.0003367876 -0.0005662584\nx           -0.0005662584  0.0011488351\n\n* Ratio (1st / 2nd):\n            (Intercept)         x\n(Intercept)    1.032055 1.0124276\nx              1.012428 0.9960916\n```\n\n\n:::\n:::\n\n\n\n### Example 2: Second order misspecification\n\n$$\nY = X + \\varepsilon,\\quad X \\sim \\text{Unif} (0,1),\\qquad \\varepsilon \\sim \\mathcal N (0,X) \n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrxy_02 <- rxy_fun(\n\trx = runif,\n\tf = \\(x) x,\n\treps = \\(x) rnorm(length(x), sd = x)\n\t)\n\nplot(rxy_02, N = 300)\n```\n\n::: {.cell-output-display}\n![](misspecification-and-linear-sandwiches_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nThis model is first-order consistent, but second-order misspecified (variance is not independent of $X$). The effects on vcov model-trusting estimates is mixed: some covariances are underestimated, some are overestimated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmsim(rxy_02)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n            (Intercept)           x\n(Intercept)  0.01344466 -0.02014604\nx           -0.02014604  0.04008595\n\n* Simulation-based vcov (vcov of coefficient estimates):\n             (Intercept)           x\n(Intercept)  0.005456494 -0.01417346\nx           -0.014173461  0.04834196\n\n* Ratio (1st / 2nd):\n            (Intercept)         x\n(Intercept)    2.463974 1.4213920\nx              1.421392 0.8292164\n```\n\n\n:::\n:::\n\n\n\nAgain, this large bias is corrected by the sandwich estimator:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmsim(rxy_02, vcov = sandwich::vcovHC)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n             (Intercept)           x\n(Intercept)  0.005637138 -0.01451506\nx           -0.014515056  0.04909868\n\n* Simulation-based vcov (vcov of coefficient estimates):\n             (Intercept)           x\n(Intercept)  0.005456494 -0.01417346\nx           -0.014173461  0.04834196\n\n* Ratio (1st / 2nd):\n            (Intercept)        x\n(Intercept)    1.033106 1.024101\nx              1.024101 1.015653\n```\n\n\n:::\n:::\n\n\n\n### Example 3: sample size effects\n\nThe sandwich estimators only become unbiased in the large sample limit. For instance, in our previous Example 1, the sandwich covariance estimates require sample sizes of $N \\approx 50$ or larger, in order for their bias to be relatively contained ($\\lesssim 10\\%$). With a small sample size:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmsim(rxy_01, N = 10, vcov = sandwich::vcovHC)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n             (Intercept)           x\n(Intercept)  0.008253143 -0.01356350\nx           -0.013563503  0.02691423\n\n* Simulation-based vcov (vcov of coefficient estimates):\n             (Intercept)            x\n(Intercept)  0.005084963 -0.008573385\nx           -0.008573385  0.017136158\n\n* Ratio (1st / 2nd):\n            (Intercept)        x\n(Intercept)    1.623049 1.582048\nx              1.582048 1.570611\n```\n\n\n:::\n:::\n\n\n\nFor such small sample sizes, however, one should probably also keep into account the [bias in the point estimate $\\hat \\beta$ itself](https://vgherard.github.io/posts/2023-05-12-consistency-and-bias-of-ols-estimators/), so that the bias in the variance $\\mathbb V (\\hat \\beta)$ becomes a kinda second-order problem.\n\n### Example 4: variance underestimation and overestimation\n\nAccording to the heuristics of [@buja2019models], the linear model trusting variances $\\mathbb V (\\hat \\beta)_{ii}$ tend to underestimate (overestimate) the true variances:\n\n-   In the presence of non-linearity, when the strong deviations from linearity are far away from (close to) the center of the regressor distribution.\n-   In the presence of heteroskedasticity, when the regions of high variance are far away from the (close to) the center of the regressor distribution.\n\nWe illustrate the second case. Consider the following two models:\n\n$$\nY = X + \\varepsilon,\\quad X \\sim \\text{Unif} (0,1),\\qquad \\varepsilon \\sim \\mathcal N (0,\\vert X-\\frac{1}{2}\\vert ) \n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrxy_04a <- rxy_fun(\n\trx = runif,\n\tf = \\(x) x,\n\treps = \\(x) rnorm(length(x), sd = abs(0.5 - x))\n\t)\n\nplot(rxy_04a)\n```\n\n::: {.cell-output-display}\n![](misspecification-and-linear-sandwiches_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n$$\nY = X + \\varepsilon,\\quad X \\sim \\text{Unif} (0,1),\\qquad \\varepsilon \\sim \\mathcal N (0,\\frac{1}{2}-\\vert X-\\frac{1}{2}\\vert ) \n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrxy_04b <- rxy_fun(\n\trx = runif,\n\tf = \\(x) x,\n\treps = \\(x) rnorm(length(x), sd = 0.5 - abs(0.5 - x))\n\t)\n\nplot(rxy_04b)\n```\n\n::: {.cell-output-display}\n![](misspecification-and-linear-sandwiches_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\nIn agreement with the heuristics, we have, for the first model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmsim(rxy_04a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n             (Intercept)            x\n(Intercept)  0.003326042 -0.004989057\nx           -0.004989057  0.009977552\n\n* Simulation-based vcov (vcov of coefficient estimates):\n             (Intercept)            x\n(Intercept)  0.005390525 -0.009154439\nx           -0.009154439  0.018296535\n\n* Ratio (1st / 2nd):\n            (Intercept)         x\n(Intercept)   0.6170162 0.5449878\nx             0.5449878 0.5453247\n```\n\n\n:::\n:::\n\n\n\nand, for the second model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmsim(rxy_04b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n             (Intercept)            x\n(Intercept)  0.003420946 -0.005150512\nx           -0.005150512  0.010300847\n\n* Simulation-based vcov (vcov of coefficient estimates):\n             (Intercept)            x\n(Intercept)  0.001590907 -0.001503471\nx           -0.001503471  0.003131620\n\n* Ratio (1st / 2nd):\n            (Intercept)        x\n(Intercept)    2.150312 3.425748\nx              3.425748 3.289303\n```\n\n\n:::\n:::\n\n\n\nIt is interesting to notice that, far away from the large-sample limit, the sandwich estimates also have a bias (as discussed in the previous example), but the bias leads to an overestimate of $\\mathbb V (\\hat \\beta)$ *in both cases*[^5]:\n\n[^5]: I don't know whether this result (that sandwich estimates are, at worst, overestimates) is a general one.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmsim(rxy_04a, N = 10, vcov = sandwich::vcovHC)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n            (Intercept)          x\n(Intercept)  0.07714254 -0.1302820\nx           -0.13028198  0.2595908\n\n* Simulation-based vcov (vcov of coefficient estimates):\n            (Intercept)          x\n(Intercept)  0.05560994 -0.0957307\nx           -0.09573070  0.1947398\n\n* Ratio (1st / 2nd):\n            (Intercept)        x\n(Intercept)    1.387208 1.360922\nx              1.360922 1.333013\n```\n\n\n:::\n\n```{.r .cell-code}\nlmsim(rxy_04b, N = 10, vcov = sandwich::vcovHC)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimulation results:\n\n* Model-trusting vcov (average of vcov estimates):\n            (Intercept)           x\n(Intercept)  0.05301354 -0.07223407\nx           -0.07223407  0.13959714\n\n* Simulation-based vcov (vcov of coefficient estimates):\n            (Intercept)           x\n(Intercept)  0.02725563 -0.03408101\nx           -0.03408101  0.06735272\n\n* Ratio (1st / 2nd):\n            (Intercept)        x\n(Intercept)    1.945049 2.119481\nx              2.119481 2.072628\n```\n\n\n:::\n:::\n\n\n\n## Conclusions\n\nSandwich estimators provide valid inference for parameter covariances and standard errors in misspecified linear regression settings. These model-robust tools are available in R through [`{sandwich}`](https://CRAN.R-project.org/package=sandwich) (which also provides\\\nmethods for more general `glm` objects).\n\nFor fairly large datasets, this model-robust approach can be coupled with data splitting, leading to a modeling procedure which I'm finding to be quite solid and versatile in practice:\n\n1.  Perform data exploration and model selection on a separate portion of data. This is to avoid [biasing inferential results with random selective procedures](https://vgherard.github.io/posts/2022-10-18-posi/).\n2.  Once a reasonable model is found, fit the model on the remaining data, adopting robust covariance estimates for model parameters.\n\nThis works very well with independent data for which a (generalized) linear model can provide a useful parametric description. Generalizations may be discussed in a separate post.\n",
    "supporting": [
      "misspecification-and-linear-sandwiches_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}