{
  "hash": "6e7b1ec1064222e24a15f47788a3dc4c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exponential Dispersion Models\"\ndate: 2024-03-07\nbibliography: biblio.bib\ndraft: false\n---\n\n\n\n\n## Intro\n\nExponential Dispersion Models (EDMs) provide a natural generalization of the normal distribution, in which the modeled variable $Y$ is assumed to follow a probability density:\n\n$$\n\\text d P _{\\lambda,\\,\\mu}(y)=e^{-\\frac{\\lambda}{2}d(y,\\,\\mu)}\\text d \\nu _{\\lambda}(y)\n$$ \n\nwith respect to a certain dominating measure $\\nu _{\\lambda}$. Here $\\mu = \\intop y\\,\\text d P_{\\lambda,\\,\\mu}(y)$ and $d(y,\\,\\mu) \\geq 0$, with equality only for $y = \\mu$. The function $d(y,\\,\\mu)$ is called the *unit deviance*, and plays for EDMs the same role of squared distance $(y-\\mu)^2$ for the normal model. Not surprisingly, EDMs provide a sound framework for the maximum-likelihood based formulation of generalized linear models, additive models, and similar beasts.\n\n## Exponential Dispersion Models\n\nWe start with a probability measure on $\\mathbb R ^n$ in the form of an *additive EDM*:\n\n$$\n\\text d P ^* _{\\lambda, \\theta} (z) = e^{\\theta ^T z-\\lambda\\kappa(\\theta)}\\text dQ^*_\\lambda (z)\n$$ {#eq-edstar}\n\nwhere $\\lambda > 0$, $\\text Q^*_\\lambda$ is a Borelian probability measure on $\\mathbb R$, and $\\kappa(\\theta)$ is a differentiable strictly convex function, with $\\kappa''(\\theta) > 0$ and $\\kappa(0) =0$. For a random variable $Z$ distributed according to @eq-ed we write $Z\\sim \\text{ED}^*(\\lambda, \\,\\theta,\\,\\kappa)$.\n\nFor any given $\\lambda$, normalization of @eq-edstar requires: \n\n$$\ne^{\\lambda \\kappa(\\theta)}=\\intop e^{\\theta ^Ty}\\text dQ^*_\\lambda(z)\n$$ {#eq-norm-cond}\n\nto hold for all $\\theta$ and $\\lambda$. In other words, $M_\\lambda(\\theta) \\equiv e^{\\lambda \\kappa(\\theta)}$ must be the moment generating function of the measure $Q^* _\\lambda(y)$ for a given $\\theta$, which we assume to be uniquely determined by its moments[^1], so that we can omit the mention of the measure $Q^*_\\lambda$ in the notation $\\text{ED}^*(\\lambda, \\,\\theta,\\,\\kappa)$\\^. This requires, in particular $\\kappa (0) = 0$.\n\n[^1]: Whether a set of moments determines a unique probability measure is called the [Hamburger moment problem](https://en.wikipedia.org/wiki/Hamburger_moment_problem).\n\nA closely related parametrization is the so-called *reproductive EDM*:\n\n$$\n\\text d P _{\\lambda, \\theta} (y) = e^{\\lambda(\\theta ^T y-\\kappa(\\theta))}\\text dQ_\\lambda (y)\n$$ {#eq-ed}\n\nFor a random variable $Y$ distributed according to @eq-ed we write $Y\\sim \\text{ED}(\\lambda, \\,\\theta,\\,\\kappa)$. The link between @eq-ed and @eq-edstar is that $Y\\sim \\text{ED}(\\lambda, \\,\\theta,\\,\\kappa)$ if and only if $Z=\\lambda Y\\sim \\text{ED}^*(\\lambda, \\,\\theta,\\,\\kappa)$, so that reproductive and additive EDMs can be interchanged whenever convenient, at least for theoretical considerations. The probability measures $\\text d Q_\\lambda$ and $\\text d Q ^*_\\lambda$, which are uniquely determined by normalization, are related by push-forward:\n\n$$\nQ_\\lambda ^* = (m_\\lambda )_*(Q_\\lambda),\n$$ {#eq-push-forward}\n\nwhere $m_\\lambda$ denotes multiplication by $\\lambda$, *i.e.* $m_\\lambda(y)=\\lambda y$.\n\nIn cases of practical interest (see the examples below), $Q_\\lambda$ and $Q_\\lambda^*$ are absolutely continuous either with respect to the Lebesgue measure, or with respect some measure concentrated on $c \\cdot \\mathbb N$ for some $c>0$. The two cases are referred to as the \"continuous\" and \"discrete\" case, respectively, for obvious reasons.\n\n## General Properties\n\n### Moment generating function\n\nConsider first the reproductive EDM @eq-ed. If $Y\\sim \\text {ED}(\\lambda,\\,\\theta,\\,\\kappa)$, its moment generating function is:\n\n$$\nM_Y(s)=\\mathbb E(e^{sY})=\\exp\\left[\\lambda\\left(\\kappa(\\theta +\\frac{s}{\\lambda})-\\kappa(\\theta)\\right)\\right],\n$$ {#eq-mgf}\n\nfrom which we can derive, in particular:\n\n$$\n\\begin{split}\n\\mathbb E(Y) &= \\frac{\\text d}{\\text ds}\\vert_{s=0}\\log M(s) =\\kappa'(\\theta),\\\\\n\\mathbb V(Y) &= \\frac{\\text d^2}{\\text ds ^2}\\vert_{s=0}\\log M(s) =\\frac{\\kappa''(\\theta)}{\\lambda}.\\\\\n\\end{split}\n$$ {#eq-exp-var}\n\nFor the additive EDM @eq-edstar, the corresponding results for $Z\\sim \\text{ED}^*(\\lambda,\\,\\theta,\\,\\kappa)$ are:\n\n$$\n\\begin{split}\nM_Z(s)&=\\exp\\left[\\lambda\\left(\\kappa(\\theta + s)-\\kappa(\\theta)\\right)\\right],\\\\\n\\mathbb E(Z) &= \\lambda\\kappa'(\\theta),\\\\\n\\mathbb V(Z) &= \\lambda \\kappa''(\\theta).\n\\end{split}\n$$ {#eq-mgf-results-edstar}\n\n### Legendre Transform of $\\kappa (\\theta)$\n\nSince $\\kappa$ is strictly convex, the mapping:\n\n$$\n\\mu = \\frac{\\partial\\kappa}{\\partial\\theta}\n$$ {#eq-mu-theta-mapping}\n\nis invertible, and we may equivalently parametrize the reproductive EDM in terms of $\\mu$ and $\\lambda$ as follows:\n\n$$\n\\text d P _{\\lambda, \\mu} (y) = e^{\\lambda(\\theta(\\mu) ^T (y-\\mu)+\\tau(\\mu))}\\text dQ_\\lambda (y).\n$$ {#eq-ed-mu}\n\nwhere:\n\n$$\n\\tau(\\mu) = \\theta(\\mu)^T\\mu - \\kappa(\\theta(\\mu)).\n$$ {#eq-legendre-transform}\n\nis the Legendre transform of $\\kappa$.\n\n## Deviance\n\nConsider two reproductive EDMs $P _{\\lambda,\\mu _1}$ and $P _{\\lambda,\\mu _2}$ with the same dispersion parameter $\\lambda$ (the function $\\kappa$ is assumed to be fixed throughout). The likelihood ratio at a given $Y=y$ is:\n\n$$\n\\ln (\\frac{\\text d P_{\\lambda,\\mu_1}}{\\text d P_{\\lambda,\\mu_2}}(y))=\\lambda\\cdot\\left[(\\theta_1-\\theta_2)y-(\\kappa_1-\\kappa_2)\\right],\n$$ {#eq-log-likelihood}\n\nwhere $\\theta _1 = \\theta(\\mu_1)$, $\\kappa_1= \\kappa(\\theta(\\mu _1))$, *etc.*. Setting $\\mu _1 = y$ and $\\mu _2 = \\mu$ in this expression and multiplying by a convenient factor, we obtain the so called *unit scaled deviance*:\n\n$$\n\\begin{split}\nd_\\lambda(y,\\mu) &\\equiv 2\\left.\\ln (\\frac{\\text d P_{\\lambda,\\mu_0}}{\\text d P_{\\lambda,\\mu}}(y)) \\right \\vert _{\\mu_0=y}\\\\&=2\\lambda\\cdot\\left[(\\theta(y)-\\theta(\\mu))y-\\kappa(\\theta(y))+\\kappa(\\theta(\\mu))\\right].\n\\end{split}\n$$ {#eq-unit-scaled-deviance}\n\nThe *unit deviance* is defined as:\n\n$$\n\\begin{split}\nd(y,\\mu) &\\equiv d_1(y,\\mu)\\\\&=2\\cdot\\left[(\\theta(y)-\\theta(\\mu))y-\\kappa(\\theta(y))+\\kappa(\\theta(\\mu))\\right]\n\\end{split}\n$$ {#eq-unit-deviance}\n\nIt is also useful to express $d_\\lambda$ in terms of the Legendre transform $\\tau$ of $\\kappa$, as defined in @eq-legendre-transform:\n\n$$\nd_\\lambda(y,\\mu) =2\\lambda\\cdot\\left[-\\theta(\\mu)^T(y-\\mu)+\\tau(y)-\\tau(\\mu)\\right]\n$$ {#eq-unit-deviance-legendre}\n\nUsing the convexity of $\\kappa$, it is easy to show that $d_\\lambda(y,\\mu) \\geq 0$ for all $y$ and $\\mu$, and that $d_\\lambda(y,\\mu) = 0$ requires $\\mu = y$. The probability measure can be expressed in terms of the unit deviance as:\n\n$$\n\\text d P _{\\lambda, \\mu} (y) = e^{-\\frac{\\lambda}{2}d(y,\\,\\mu)}e^{\\lambda \\tau(y)}\\text dQ_\\lambda (y).\n$$ {#eq-ed-vs-deviance}\n\n## Maximum Likelihood Estimation\n\nLet $Y_i\\sim \\text{ED}(\\lambda,\\,\\mu^{(0)}_ i ,\\,\\kappa)$ be independent for $i=1,\\,2,\\,\\dots,\\,N$ and let $M\\subseteq \\mathbb R ^N$ be a family of models for the mean $\\boldsymbol \\mu ^{(0)} = (\\mu _1^{(0)},\\,\\mu_2^{(0)},\\dots,\\,\\mu_N^{(0)})^T$ (in a GLM context, $M$ would be the linear subspace spanned by the covariates, $\\boldsymbol \\mu _\\beta = \\mathbf X \\beta$). From @eq-ed-vs-deviance, we see that the likelihood of a model $\\boldsymbol \\mu \\in M$ is, modulo a $\\boldsymbol \\mu$-independent term, equal to its total deviance:\n\n$$\n\\log \\mathcal L (\\boldsymbol \\mu,\\,\\lambda;\\mathbf Y) = -\\frac{\\lambda}{2}\\mathcal D(\\mathbf Y,\\boldsymbol \\mu)+g_\\lambda(\\mathbf Y),\n$$ {#eq-likelihood-vs-deviance}\nwith:\n\n$$\n\\mathcal D (\\mathbf Y,\\boldsymbol \\mu)\\equiv\\sum_{i=1}^Nd(Y_i,\\mu_i) \n$$ {#eq-total-deviance}\n\nHence, the Maximum Likelihood Estimate (MLE) of $\\boldsymbol \\mu$ corresponds to the minimum deviance estimate:\n\n$$\n\\hat {\\boldsymbol \\mu}\\equiv \\arg \\max _{\\boldsymbol \\mu \\in M} \\mathcal L (\\boldsymbol \\mu,\\,\\lambda;\\,\\mathbf Y)=\\arg \\min _{\\boldsymbol \\mu \\in M} \\mathcal D (\\mathbf Y;\\boldsymbol \\mu).\n$$ {#eq-mle-is-mde}\n\nIn particular, the MLE $\\hat {\\boldsymbol \\mu}$ is obtained by minimizing a function of $\\boldsymbol \\mu$ only, and is independent on whether the dispersion parameter $\\lambda$ is being estimated itself or not.\n\nThese results are sometimes formulated in terms of a \"saturated\" model $\\boldsymbol \\mu _\\text{s} = \\mathbf Y$. From @eq-likelihood-vs-deviance we see that such a model has likelihood equal to $g_{\\lambda}(\\boldsymbol \\mu)$, implying that:\n\n$$\n\\lambda\\mathcal D(\\mathbf Y,\\boldsymbol \\mu) = -2\\log \\left(\\frac{\\mathcal L (\\boldsymbol \\mu,\\lambda;\\mathbf Y)}{\\mathcal L (\\mathbf Y,\\lambda;\\mathbf Y)}\\right)\n$$ {#eq-deviance-log-lik}.\n\nWe note the asymptotic results [@jorgensen1992theory, ยง3.6]:\n\n$$\n\\lambda \\mathcal D(\\mathbf Y,\\,\\hat {\\boldsymbol \\mu})\\overset{d}{\\to}  \\chi ^2 _{N-p} \\qquad (\\lambda \\to \\infty),\\\\\n$$ {#eq-deviance-asymptotics}\n\nfor a correctly specified model family $M$ with $\\dim (M) = p$, and:\n\n$$\n\\lambda \\mathcal D(\\mathbf Y,\\,\\hat {\\boldsymbol \\mu}_1)-\\lambda D(\\mathbf Y,\\,\\hat {\\boldsymbol \\mu}_2)\\overset{d}{\\to} \\chi ^2 _{p_2-p_1} \\qquad (\\lambda \\to \\infty \\text { or } N\\to \\infty),\n$$ {#eq-deviance-diff-asymptotics}\n\nfor a correctly specified model family $M_1$ and $M_2 \\supseteq M_1$, with $p_i =\\dim (M_i)$. @eq-deviance-asymptotics can be seen as the limiting case of @eq-deviance-diff-asymptotics when $M_2 = \\mathbb R ^N$, as in the saturated model. The manifolds $M$ and $M_i$ are not strictly required to be linear subspaces of $\\mathbb R^N$, because in the limits and under the null hypotheses implied by Eqs. @eq-deviance-asymptotics and @eq-deviance-diff-asymptotics the distributions of MLEs are concentrated around the true value $\\boldsymbol \\mu ^{(0)}$, so that the manifolds $M$ and $M_i$ can be effectivley approximated by their tangent spaces.\n\nNoteworthy, limit @eq-deviance-asymptotics holds in the small dispersion limit $\\lambda \\to \\infty$ only, whereas limit @eq-deviance-diff-asymptotics is also valid in the large sample limit, essentially due to Wilks' theorem.\n\n## Examples of EDMs\n\n#### Univariate Gaussian\n\nThe univariate gaussian family $N (\\mu, \\sigma^2)$, with probability density function (PDF):\n\n$$\nf_{\\mu,\\sigma}(y)=\\frac{1}{\\sqrt {2 \\pi \\sigma ^2}}\\exp\\left[-\\frac{(y-\\mu)^2}{2\\sigma ^2}\\right] \n$$ {#eq-gaussian-pdf}\n\ncorresponds to the reproductive EDM $\\text{ED}(\\lambda, \\,\\theta,\\,\\kappa)$ with:\n\n$$\n\\kappa (\\theta) = \\frac{\\theta ^2}{2},\\quad \\theta \\in \\mathbb R,\\quad \\lambda \\in \\mathbb R^+.\n$$ {#eq-gaussian-edm} \n\nThe correspondence is given by:\n\n$$\n\\theta = \\mu,\\quad \\lambda = \\frac{1}{\\sigma^2}.\n$$ {#eq-gaussian-identification}\n\nThe base probability measure is given by:\n\n$$\n\\text d Q_\\lambda (y)=\\sqrt{\\frac \\lambda {2\\pi}}e^{-\\lambda y^2/2} \\text dy\n$$ {#eq-gaussian-base-measure}\n\nThe Legendre transform of $\\kappa$ is:\n\n$$\n\\tau(\\mu) = \\frac{\\mu ^2}{2} \n$$ {#eq-gaussian-tau}\n\nand the unit deviance reads:\n\n$$\nd(y, \\hat \\mu) =(y-\\hat \\mu)^2.\n$$ {#eq-gaussian-deviance}\n\n#### Binomial\n\nThe binomial family $\\mathcal B(p,N)$ with probability mass function (PMF):\n\n$$\nf_{p,N}(z) = \\binom{N}{z} p^z(1-p)^{N-z}\n$$ {#eq-binomial-pmf}\n\ncorresponds to the additive EDM $\\text{ED}^*(\\lambda, \\,\\theta;\\,\\kappa)$ with:\n\n$$\n\\kappa(\\theta) = \\ln (\\dfrac{1+e^\\theta}{2}),\\quad \\theta\\in \\mathbb R ,\\quad \\lambda \\in \\mathbb N.\n$$ {#eq-binomial-edm}\n\nThe correspondence is given by:\n\n$$\n\\theta = \\ln\\frac{p}{1-p},\\quad\\lambda =N.\n$$ {#eq-binomial-identification}\n\nThe base probability measure reads:\n\n$$\n\\frac{\\text d Q_\\lambda^*(z)}{\\text d z} =2^{-\\lambda}\\sum_{i=0} ^\\lambda \\binom{\\lambda}{i} \\delta (z-i).\n$$ {#eq-binomial-base-measure}\n\nThe Legendre transform of $\\kappa$ (using $p$ for the mean parameter) is:\n\n$$\n\\tau(p)= \\ln2+p\\ln p+(1-p)\\ln(1-p)\n$$ {#eq-binomial-tau}\n\nand the unit deviance for the *reproductive* EDM:\n\n$$\nd(y,\\hat p)=-2y\\ln (\\dfrac{\\hat p}{y})-2(1-y)\\ln (\\dfrac{1-\\hat p}{1-y}).\n$$ {#eq-binomial-deviance}\n\nFor the additive EDM, appropriate to an integer valued binomial variable, this is given by:\n\n$$\nd(z,\\hat p)=-2\\frac{z}{N}\\ln (\\dfrac{N\\hat p}{z})-2(1-\\frac{z}{N})\\ln (\\dfrac{N-N\\hat p}{N-z}).\n$$ {#eq-binomial-deviance-bis}\n\n#### Multinomial\n\nThe multinomial family $\\text{Mult} _{K+1}(p_1,\\,p_2,\\dots ,p_{K+1},\\,N)$ for $K+1$ categories is given by the PMF:\n\n$$\nf_{\\boldsymbol p ,N}(z) = \\binom{N}{z_1,\\,z_2,\\,\\dots,\\,z_{K+1}}\\prod _{k=1}^{K+1}p_k^{z_k},\n$$ {#eq-mult-pmf}\n\nIn order to identify this with an EDM, we use the constraints $\\sum _{i=1}^{M+1}z_i =1$ and $\\sum _{i=1}^{M+1}p_i =1$ to eliminate one dependent variable and parameter, say $z_{M+1}$ and $p_{M+1}$, respectively. The family of densities for the resulting $M$-dimensional vector $\\boldsymbol z=(z_1\\,z_2\\,\\dots\\,z_M)^T$ corresponds to the additive EDM $\\text{ED}^*(\\lambda, \\,\\theta;\\,\\kappa)$ with:\n\n$$\n\\quad \\kappa(\\theta) = \\ln(\\dfrac{1+\\sum_{i=1}^K e^{\\theta _k}}{K+1}), \\quad \\theta \\in \\mathbb R^K,\\quad \\lambda \\in \\mathbb N,\n$$ {#eq-mult-edm}\n\nthe correspondence being given by:\n\n$$\n\\theta _i = \\ln \\frac{p_i}{p_{K+1}},\\quad \\lambda = N.\n$$ {#eq-mult-identification}\n\nThe base measure:\n\n$$\n\\frac{\\text d Q_\\lambda^*(z)}{\\text d \\boldsymbol z} =(K+1)^{-\\lambda}\\sum_{\\boldsymbol i\\in \\mathbb N ^{K}\\,\\colon \\,\\sum _{k=1}^{K}i_k\\leq\\lambda} \\binom{\\lambda}{i_1,\\,i_2,\\dots,i_{K+1}} \\delta (\\boldsymbol z-\\boldsymbol i), \n$$ {#eq-mult-base-measure}\n\nwhere $i_{K+1} = N - \\sum _{k=1} ^{K} i_k$. The Legendre transform of $\\kappa$ is:\n\n$$\n\\tau(\\boldsymbol p)= \\ln (K+1)+\\sum _{k=1}^{K+1}p_k\\ln p_k\n$$ {#eq-mult-tau}\n\nand the deviance is:\n\n$$\nd(y,\\hat {\\boldsymbol p}) = -2\\sum _{k=1}^{K+1}y_k\\ln (\\frac{\\hat p_k}{y_k}).\n$$ {#eq-mult-deviance}\n\nFor the additive EDM, appropriate to an integer valued multinomial variable, this is given by:\n\n$$\nd(z,\\hat p)=-2\\sum _{k=1}^{K+1}\\frac{z_k}{N}\\ln (\\dfrac{N\\hat p}{z_k}).\n$$ {#eq-mult-deviance-bis}\n\n#### Poisson\n\nThe Poisson PMF is:\n\n$$\nf(y) = \\frac {\\nu ^z} {z!} e^{-\\nu}\n$$ {#eq-poisson-pmf}\n\nThis can be interpreted as coming from an additive EDM with:\n\n$$\n\\kappa(\\theta)=e^\\theta-1,\\quad \\theta \\in \\mathbb R,\\quad \\lambda \\in \\mathbb R^+.\n$$ {#eq-poisson-edm}\n\nHowever, the correspondence is not unique, being given by the single relation:\n\n$$\n\\lambda e^\\theta = \\nu \n$$ {#eq-poisson-identification}\n\nwhich describes a curve in the $\\Theta \\times \\Lambda$ space. The corresponding base measure is:\n\n$$\n\\dfrac{\\text d Q _\\lambda (z)}{\\text d z} = e^{-\\lambda}\\sum _{k=0}^{\\infty}\\frac{\\lambda ^k\\delta(z-k)}{k!}\n$$ {#eq-poisson-base-measure}\n\nwhich is nothing but the Poisson measure itself.\n\n## References\n\nI have mostly followed [@jorgensen1987exponential]. References [@jorgensen1992theory] [@jorgensen1997theory] from the same author provide more extensive expositions. A good reference for GLMs is [@mccullagh2019generalized].\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}