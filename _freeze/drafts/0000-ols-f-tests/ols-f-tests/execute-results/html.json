{
  "hash": "4b191cfccf2c2003b2677cac788f8995",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"OLS F-tests\"\ndescription: |\n  A short description of the post.\nauthor:\n  - name: vgherard\n    url: https://vgherard.github.io\ndate: 2023-10-06\noutput:\n  distill::distill_article:\n    self_contained: false\ndraft: true\n---\n\n\n\n\nLet $Y\\in\\mathbb{R}^{n\\times1}$ and $X\\in\\mathbb{R}^{n\\times p}$ be random \nvariables and denote $H=X(X^{T}X)^{-1}X^{T}$. The usual OLS estimator is:\n\n$$\n\\hat{Y}=HY=X\\hat{\\beta}\n\\\\\n\\hat{\\beta}=(X^{T}X)^{-1}X^{T}Y.\n$$\nWe assume that data are i.i.d. draws from a joint distribution \n$\\mathcal{F}(X,Y)$, and we denote:\n\n$$\n\\beta\t=\\arg\\min_{\\beta^{\\prime}}\\mathbb{E}((Y-X\\beta^{\\prime})^{2}),\\\\\n\\epsilon\t=Y-X\\beta.\n$$\nThe OLS residuals are:\n\n$$\n\\hat r = Y-\\hat Y=(1-H)Y = (1-H)\\epsilon\n$$\nand the Residuals Squared of Sum is:\n\n$$\n\\text{RSS}=\\hat r^T\\hat r=\\epsilon^T(1-H)\\epsilon\n$$\n\nFurthermore, we have:\n\n$$\n\\hat \\beta =\\beta + (X^TX)^{-1}X^T\\epsilon \n$$\nFor the moment, we have not assumed anything on the “error” term $\\epsilon$ \n(in particular, we may have $\\mathbb{E}(\\epsilon\\vert X)\\neq0$ and \n$\\mathbb{V}(\\epsilon\\vert X)=v(X)$ for some non-constant positive function \n$v(\\cdot)$).\nIf the error term is normal with constant variance, these equations imply that \n$\\hat \\beta$ and $\\hat r$ are conditionally independent. \nFurthermore both $\\hat r$ and  $\\hat \\beta$ have Gaussian $X$-conditional \ndistribution, and:\n\n$$\n\\mathbb E(\\hat r ^T\\hat r\\vert X) = (n-p)\\sigma ^2,\\\\\n\\mathbb V(\\hat \\beta\\vert X)=(X^T X) ^{-1} \\sigma^2\n$$\nThese information can be used to construct confidence sets for the parameter \nvector $\\beta$.\n\n—\n\nLet us now decompose $X$ as follows:\n\n$$\nX=\\begin{pmatrix}X_{0} & X_{1}\\end{pmatrix},\n$$ \n\nwhere $X_{0}\\in\\mathbb{R}^{n\\times k}$ and $X_{1}\\in\\mathbb{R}^{n\\times(p-k)}$,\nand denote by $H_0$, $\\hat \\beta _0$, etc. the corresponding quantities for the\nOLS with this reduced number of regressors.\n\n\n*Theorem.* $H H_0 = H_0$.\n\nThe proof is immediate if we recall the interpretation of $H$ as the orthogonal\nprojection on the subspace of $\\mathbb R ^n$ generated by the regressors \nvectors. The second projection clearly has no effect.\n\nAs a consequence the projectors $1-H$ and $H-H_0$ are orthogonal.\n\nFor a similar reason, we have the following independence relations (denote $\\perp$):\n\n$$\n\\hat \\beta _0 \\perp \\hat r_0,\\\\\n\\hat \\beta _0 \\perp \\hat r,\\\\\n\\hat \\beta \\perp \\hat \\beta _0, \n$$\nSince $\\hat r = (1-H) \\epsilon$, this implies that:\n\n$$\n\\hat r_0 ^T \\hat r_0 = \\hat r_1^T\\hat r_1+(\\hat r_0^T\\hat r_0-\\hat r_1^T\\hat r_1)\n$$\nwhere the two terms are *independent* $\\chi ^2$ variables. Since the sum and\nratios of these are still independent, we find independence of:\n\n$$\n\n$$\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}