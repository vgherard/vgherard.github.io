{
  "hash": "a90a357ab753f1bbd0d2c12ff5f5d7f5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Finite sample bias of ARMA estimates\"\ndescription: |\n  A short description of the post.\nauthor:\n  - name: vgherard\n    url: https://vgherard.github.io\ndate: 2023-06-08\noutput:\n  distill::distill_article:\n    self_contained: false\ndraft: true\n---\n\n\n\n\n\n## Intro\n\nFitting ARMA models on short time series datasets can be challenging from various points of view. A purely technical difficulty is the fact that, even for well specified models, coefficient estimates obtained from maximum likelihood estimation are biased. More generally, the sampling distribution of estimators can deviate substantially from their asymptotic Gaussian distribution, invalidating naive constructions like normal confidence intervals (*i.e.* $\\text{Estimate} \\pm k \\cdot \\text{SE}$)\n\nConsider, for example, a stationary $\\text{AR}(1)$ process:\n\n$$\nY_{t+1} =  \\mu + \\alpha \\cdot (Y_t-\\mu) +\\epsilon _t,\\quad \\epsilon _t\\sim \\mathcal N(0,\\sigma ^2), (\\#eq:AR1)\n$$ and denote by $\\hat \\alpha _n$ the maximum likelihood estimate of $\\alpha$ from a series of $n$ data points. It is only in the $n\\to \\infty$ limit that $\\mathbb E (\\hat \\alpha _n) \\to \\alpha$, but for small $n$, the bias can be severe. Furthermore, as I show below, the distribution of $\\hat \\alpha _n$ has a heavy left tail, which results in a decent probability of estimating a negative correlation between $Y_{t+1}$ and $Y_t$. Altogether, even with a simple model like \\@ref(eq:AR1), parameter estimation in the low sample size regime is not trivial.\n\nWith these considerations in mind, I set out myself to construct an (approximately) unbiased estimator for the $\\alpha$ parameter in Eq. \\@ref(eq:AR1), and a corresponding confidence interval. Eq. \\@ref(eq:AR1) is, of course, just a toy model, but the method I discuss below - based on a parametric bootstrap - is rather general and can be applied in realistic circumstances.\n\n## Distribution of $\\hat alpha _n$\n\nTo begin with, we study the distribution of the maximum likelihood estimate $\\hat \\alpha _n$ for small $n$. For later convenience, I define a couple of wrappers:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate 'n' observations from a given AR(1) model (with unit noise variance).\nar_sim <- function(alpha, mu, n)  \n\tmu + arima.sim(list(ar = alpha), n = n)\n\n# Fit an AR(1) model to a given time series 'y'.\nar_fit <- function(y, method = \"ML\")\n\tarima(y, c(1, 0, 0), method = method)\n```\n:::\n\n\n\nThe plot below shows the distribution of $\\hat \\alpha _n$, for $\\mu = 1$, $\\alpha = 0.7$ and $n = 20$, which presents the features anticipated in the introduction.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- 1\nalpha <- 0.7\nn <- 20\n\nset.seed(840)\nnsim <- 1e2\nfits <- replicate(nsim, \n\t\t\t\t\t\t\t\t\tar_sim(alpha = alpha, mu = mu, n = n) |> ar_fit(), \n\t\t\t\t\t\t\t\t\tsimplify = F\n\t\t\t\t\t\t\t\t\t)\n\nestimates <- sapply(fits, coef)[\"ar1\", ]\n\nhist(estimates, breaks = 100,\n\t\t xlab = \"Estimate\", \n\t\t main = \"AR(1) coefficient estimates\",  \n\t\t )\nabline(v = alpha, col = \"blue\", lwd = 3, lty = 1)\nabline(v = mean(estimates), col = \"red\", lwd = 3, lty = 1)\n```\n\n::: {.cell-output-display}\n![AR(1) coefficient estimates. The vertical blue line represents true parameter value, while the red line is the expected value of the estimator.](finite-sample-bias-of-arma-estimates_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nThe true coverage rate of the asymptotic $95 \\%$ CIs is calculated as follows (interestingly, for $\\mu = 0$ the function `ar1_confint_naive()` constructs valid confidence intervals)[^1]:\n\n[^1]:  R does not have an S3 `confint.Arima` method, and the default method treats the $t$ statistic $\\frac{\\hat \\alpha - \\alpha}{\\widehat{\\text{SE}}(\\hat \\alpha)}$ as a $Z$-score. A better approximation is obtained if this is treated as a $t$ statistic with the correct number of degrees of freedom, as in the `ar1_confint_naive()` function below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nar1_confint_naive <- function(fit, conf.level = 0.95) {\n\test <- coef(fit)[\"ar1\"]\n\tse <- sqrt(vcov(fit)[\"ar1\", \"ar1\"])\n\t\n\tdf <- nobs(fit) - length(coef(fit))\n\t\n\ta <- (1 - conf.level) / 2\n\tk <- qt(a, df, lower.tail = FALSE)\n\t\n\tres <- est + k * se * c(-1, 1)\n\tunname(res)\n\t\n\treturn(res)\n}\n\ncis <- sapply(fits, ar1_confint_naive)\n\nmean( cis[1,] < alpha & alpha < cis[2,], na.rm = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.88\n```\n\n\n:::\n:::\n\n\n\n## Bootstrap confidence intervals\n\nThe bias can be corrected by a bootstrap:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nar_t <- function(fit, alpha) {\n\talpha_hat <- coef(fit)[\"ar1\"]\n\talpha_hat_se <- vcov(fit)[\"ar1\", \"ar1\"]\n\tt <- (alpha_hat - alpha) / alpha_hat_se\n\tunname(t)\n}\n\nconf.level <- .95\n\nnsim_max <- 1e2\nB <- 1e2\n\ncl <- parallel::makeCluster(8)\nparallel::clusterExport(cl, c(\"ar_sim\", \"ar_fit\", \"ar_t\", \"n\", \"B\", \"conf.level\"))\nboot <- pbapply::pblapply(fits[1:nsim_max], function(fit) {\n\talpha_hat <- coef(fit)[\"ar1\"]\n\talpha_hat_se <- unname(sqrt(vcov(fit)[\"ar1\", \"ar1\"]))\n\ta <- (1 - conf.level) / 2\n\t\n\tboot_fits <- replicate(B,\n\t\t\t\t\t\t\t\t\t\t\t\t tryCatch(\n\t\t\t\t\t\t\t\t\t\t\t\t \tar_sim(alpha = alpha_hat, mu = 1, n = n) |> ar_fit(),\n\t\t\t\t\t\t\t\t\t\t\t\t \terror = function(cnd) NULL\n\t\t\t\t\t\t\t\t\t\t\t\t \t),\n\t\t\t\t\t\t\t\t\t\t\t\t simplify = F\n\t\t\t\t\t\t\t\t\t\t\t\t )\n\t\n\tboot_fits <- boot_fits[!sapply(boot_fits, is.null)]\n\t\n\talpha_hat_boot <- sapply(boot_fits, coef)[\"ar1\", ]\n\talpha_hat_t_boot <- sapply(boot_fits, ar_t, alpha = alpha_hat)\n\t\n\tmask <- is.na(alpha_hat_t_boot) | is.na(alpha_hat_t_boot)\n\t\n\talpha_hat_boot <- alpha_hat_boot[!mask]\n\talpha_hat_t_boot <- alpha_hat_t_boot[!mask]\n\t\n\tres <- data.frame(\n\t\talpha_hat = alpha_hat, \n\t\tbias = mean(alpha_hat_boot) - alpha_hat,\n\t\talpha_lo = 2 * alpha_hat - quantile(alpha_hat_boot, 1 - a), \n\t\talpha_up = 2 * alpha_hat - quantile(alpha_hat_boot, a), \n\t\talpha_lo_stud = alpha_hat - quantile(alpha_hat_t_boot, 1 - a) * alpha_hat_se, \n\t\talpha_up_stud = alpha_hat - quantile(alpha_hat_t_boot, a) * alpha_hat_se\n\t\t)\n\trow.names(res) <- NULL\n\tres\n}, cl = cl) |>\n\tReduce(rbind, x = _)\n\nboot\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      alpha_hat        bias    alpha_lo  alpha_up alpha_lo_stud alpha_up_stud\n1    0.85262414 -0.21370381  0.79973628 1.5330880   -0.06786336      2.248862\n2    0.66628947 -0.16358239  0.55126488 1.2516046   -0.38421867      2.637598\n3    0.57436849 -0.11243704  0.38930746 1.0820911   -0.87040826      2.332403\n4    0.62906071 -0.16639624  0.48782933 1.1628651   -0.48096943      2.353772\n5    0.60360711 -0.14103211  0.39680192 1.2483159   -1.68643256      2.714766\n6    0.67062183 -0.13138837  0.53541599 1.2224299   -0.84761088      2.490482\n7    0.75210110 -0.16305599  0.63779258 1.5086017   -0.81074672      2.588553\n8    0.16046215 -0.09296242 -0.12806263 0.6116353   -1.43235920      2.365224\n9    0.81233199 -0.18902375  0.75942747 1.4647122    0.29096507      2.233586\n10   0.36355095 -0.07476591  0.05194016 0.8553122   -2.04607969      2.133092\n11   0.29713893 -0.11856927  0.11881571 0.7768664   -0.80508440      2.402030\n12   0.26425288 -0.07701423 -0.08137667 0.7651990   -2.29266137      2.353534\n13   0.50789145 -0.10721095  0.30407172 1.1866496   -1.42300298      3.270568\n14   0.85142735 -0.20698491  0.80569873 1.4641624    0.21955108      2.077997\n15   0.59190852 -0.17705840  0.48875320 1.1266197   -0.12366032      2.486286\n16   0.39342496 -0.10226860  0.19423231 0.9042563   -0.94223567      2.639166\n17   0.54744081 -0.14652187  0.28773109 1.1365294   -2.22967302      2.868310\n18   0.10752399 -0.05171095 -0.19604570 0.5810965   -1.61134070      2.701097\n19   0.34979837 -0.11775091  0.12152056 0.8613640   -1.18065011      2.285865\n20   0.46325963 -0.14329455  0.23963983 1.0806181   -0.97289426      2.730974\n21   0.70433077 -0.15483175  0.58292311 1.3191636   -0.82676905      2.668950\n22   0.41496964 -0.11033371  0.15472327 1.0714396   -1.64614714      3.140966\n23   0.40987784 -0.14216843  0.14901281 1.0446333   -1.39219311      2.470782\n24   0.55714393 -0.13810007  0.35395538 1.1813317   -1.31997119      2.563271\n25   0.45314423 -0.11759096  0.22012865 1.1379260   -1.23191180      3.058506\n26   0.51005789 -0.14403789  0.28748974 1.0465831   -1.70513596      2.623366\n27   0.38553312 -0.09012367  0.08655444 0.8741905   -1.83991926      2.316589\n28   0.73782363 -0.16383621  0.62181164 1.5073353   -0.31282733      2.798067\n29   0.52267279 -0.14537328  0.37153125 1.1991660   -0.57259585      3.161280\n30   0.48848169 -0.11985704  0.22338842 1.0828777   -1.83355435      2.797930\n31   0.51379162 -0.10397354  0.30910440 1.0855455   -1.58232940      2.733725\n32   0.56608776 -0.10441495  0.31764588 1.0897657   -2.96633708      2.452287\n33  -0.01753534 -0.02511439 -0.46162173 0.3766199   -2.50818996      2.473629\n34   0.71791990 -0.16413914  0.58276623 1.2889101   -0.88855981      2.405954\n35   0.61045493 -0.14806704  0.42062583 1.2210491   -1.07838572      2.641056\n36   0.44740531 -0.08813014  0.22111976 0.8479975   -1.01931248      1.969333\n37   0.68617268 -0.15950251  0.50275981 1.3682548   -2.09787643      2.591288\n38   0.71803962 -0.19067253  0.58175261 1.4221191   -1.24368546      2.809407\n39   0.31808418 -0.10403550  0.06707897 0.9263030   -1.53755143      3.381199\n40   0.79310447 -0.17400223  0.71273756 1.4446260   -0.51824658      2.554380\n41   0.65867195 -0.14511373  0.47718115 1.1898774   -1.67696939      2.579242\n42   0.67945356 -0.18061965  0.47176400 1.4517207   -2.15377230      3.003304\n43   0.19549629 -0.06186647 -0.10367595 0.6020538   -1.78144287      2.269729\n44   0.07830885 -0.07207912 -0.33913153 0.6458178   -2.44903545      3.084126\n45   0.77519597 -0.19465505  0.68388334 1.3842710   -0.29164589      2.520201\n46   0.80175979 -0.16214650  0.68444814 1.4183638   -1.82560697      2.218338\n47   0.63916175 -0.13531974  0.48052071 1.1894672   -1.08601876      2.387421\n48   0.06779585 -0.08021173 -0.22473014 0.5010738   -1.50345099      2.473342\n49   0.59938189 -0.14916406  0.43003278 1.1652901   -1.24870922      2.757007\n50   0.63782228 -0.15048139  0.49873525 1.1878828   -0.74943633      2.920804\n51   0.83334116 -0.22892425  0.78734938 1.5588074    0.37878007      2.159398\n52   0.74327722 -0.17907454  0.65706431 1.2755835   -0.09045131      2.253712\n53   0.68939115 -0.20233710  0.58257498 1.3261896   -0.12300438      2.600321\n54   0.39312221 -0.15253002  0.19817161 0.9804376   -0.81809485      2.684391\n55   0.40948845 -0.12063318  0.17821796 0.9726573   -1.18520867      2.739950\n56   0.65288471 -0.13113939  0.48125451 1.2003444   -1.12613398      2.497952\n57   0.42414153 -0.10252522  0.19193184 0.9578500   -1.33320374      2.692285\n58   0.30344529 -0.08175445  0.04974496 0.7771460   -1.26138457      2.330205\n59   0.13624832 -0.05353784 -0.21057038 0.6015902   -1.73166012      2.369165\n60   0.22754856 -0.07049305 -0.07412494 0.7616020   -1.49298037      2.634165\n61   0.49708780 -0.13864881  0.27513701 1.0789393   -1.12558326      2.619007\n62   0.65771333 -0.13344594  0.50300112 1.4345131   -1.17421926      2.894699\n63   0.31430301 -0.08718899  0.02296955 0.8835146   -1.68729596      2.806561\n64   0.43749528 -0.12049123  0.22537670 0.9910000   -0.82453273      2.595737\n65   0.63626199 -0.14993846  0.47079467 1.2677090   -1.12844590      2.707079\n66   0.74264520 -0.12406168  0.60961375 1.3058810   -1.05649831      2.353727\n67   0.29031800 -0.12520325 -0.07053104 0.8132847   -1.80945997      2.370075\n68   0.45178130 -0.09107598  0.22116828 0.9957196   -1.26978309      2.400876\n69   0.71887968 -0.12815552  0.54218113 1.1581700   -2.09926633      2.158336\n70   0.23160918 -0.08660471 -0.03933225 0.7847937   -1.36630485      2.934219\n71   0.43019010 -0.10940647  0.17940582 1.0160913   -1.20516673      2.599120\n72   0.38783777 -0.08499682  0.01899511 0.9265594   -2.94415986      2.542609\n73   0.71021675 -0.18985669  0.59476921 1.3408887   -0.81926608      2.770826\n74   0.51693247 -0.16360601  0.31934329 1.2464934   -1.20174405      3.566576\n75   0.22051175 -0.04989151 -0.07753006 0.7907294   -1.50162961      2.930031\n76   0.57205860 -0.14802442  0.40302107 1.2516942   -0.85621538      2.901214\n77   0.33530914 -0.09315608  0.11017861 0.8390205   -0.91156486      2.465676\n78   0.61309541 -0.14687694  0.45041831 1.1771197   -0.94152590      2.729418\n79   0.76787237 -0.18321184  0.64688665 1.3714591   -0.86090631      2.330043\n80   0.78058262 -0.18306932  0.70120089 1.3678282    0.01165264      2.301951\n81   0.59210294 -0.15037156  0.43443404 1.2192283   -0.55229915      2.752526\n82   0.75507632 -0.12958772  0.60645456 1.3266024   -1.81022620      2.392618\n83   0.27567597 -0.08892300  0.04940554 0.7425945   -0.89686633      2.112619\n84   0.59677406 -0.14064644  0.40706152 1.1867099   -1.73555045      2.640451\n85   0.36208119 -0.06079044  0.06845230 0.8217980   -1.41579953      2.114869\n86   0.66060440 -0.12483660  0.54269264 1.1633602   -0.82266789      2.687089\n87   0.46530689 -0.11018570  0.22666786 0.9784540   -1.10651353      2.339292\n88   0.69680152 -0.19464608  0.55752349 1.4786555   -1.49936126      3.553793\n89   0.54511798 -0.10108840  0.32019117 1.1418278   -1.55135801      2.740540\n90   0.36206390 -0.10264502  0.14196008 0.9223322   -1.52585672      3.058105\n91   0.86058557 -0.18871139  0.78600691 1.4597496   -0.32777654      2.129622\n92   0.63211010 -0.14673121  0.41769125 1.1714889   -2.40151323      2.302230\n93   0.46105065 -0.10359134  0.18184897 0.9820593   -2.13694679      2.516794\n94   0.24476797 -0.07099094 -0.12422993 0.7075350   -2.50725330      2.269475\n95   0.36880897 -0.12964498  0.14886848 0.9351509   -1.12646773      2.619609\n96   0.44798597 -0.15218718  0.22613241 1.0437099   -1.12167043      2.591011\n97  -0.02740938 -0.03918067 -0.39008337 0.4947322   -1.80724883      3.224323\n98   0.68126994 -0.16736038  0.54173145 1.2398646   -0.61724451      2.313473\n99   0.53455586 -0.16088815  0.37787859 1.1538872   -0.59645760      2.489353\n100  0.62976701 -0.14091732  0.49500012 1.3008210   -0.59723243      2.632046\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(boot$bias)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.1298048\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(boot$bias)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04198601\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(boot$alpha_lo < alpha & alpha < boot$alpha_up)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.85\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(boot$alpha_lo_stud < alpha & alpha < boot$alpha_up_stud)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n",
    "supporting": [
      "finite-sample-bias-of-arma-estimates_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}